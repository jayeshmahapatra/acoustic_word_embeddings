{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Core Python, Pandas, and kaldi_io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter,OrderedDict \n",
    "import kaldi_io\n",
    "from datetime import datetime\n",
    "\n",
    "#ngrams\n",
    "import nltk,re\n",
    "import nltk.corpus\n",
    "from nltk.corpus import switchboard\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#Scikit\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances,average_precision_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels,paired_distances\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#BigPhoney\n",
    "from big_phoney import BigPhoney\n",
    "\n",
    "\n",
    "#Torch and utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader,random_split,ConcatDataset\n",
    "\n",
    "#Import User defined classes\n",
    "from data_helpers import DataHelper\n",
    "from sfba4.utils import alignSequences\n",
    "from models import SimpleNet, SiameseNet, OrthographicNet\n",
    "from siamese_dataset import SiameseTriplets\n",
    "from ami_dataset import AMI_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load source model\n",
    "source_net = SimpleNet(9974)\n",
    "source_net = source_net.to(dev)\n",
    "source_net_save_path = \"./Models/awe_best_model.pth\"\n",
    "source_net.load_state_dict(torch.load(source_net_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 125006\n",
      "Finished Loading the Data, 125006 examples\n",
      "Number of Unique words  9974\n",
      "torch.Size([59844, 3, 40, 100])\n",
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n"
     ]
    }
   ],
   "source": [
    "num_examples = np.Inf\n",
    "frequency_bounds = (0,155)\n",
    "train_sm_dataset = SiameseTriplets(num_examples = num_examples, split_set = \"train\", frequency_bounds = frequency_bounds)\n",
    "val_sm_dataset = SiameseTriplets(num_examples = num_examples, split_set = \"val\", frequency_bounds = frequency_bounds)\n",
    "test_sm_dataset = SiameseTriplets(num_examples = num_examples, split_set = \"test\", frequency_bounds = frequency_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_sm_dataset, shuffle = True, batch_size = 64, pin_memory = True)\n",
    "val_dl = torch.utils.data.DataLoader(val_sm_dataset, shuffle = True, batch_size = 64, pin_memory = True)\n",
    "test_dl = torch.utils.data.DataLoader(test_sm_dataset, shuffle = True, batch_size = 64, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num,num_to_word = train_sm_dataset.word_to_num,train_sm_dataset.num_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(word):\n",
    "    #Remove punctuation\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    return \"[\"+word.lower()+\"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_common_ngrams(num = 50000):\n",
    "    switchboard.ensure_loaded()\n",
    "    words = switchboard.words()\n",
    "    #Add start and end of word markers and make words lower case\n",
    "    words = list(map(process_words,words))\n",
    "    #Filter empty words\n",
    "    words = list(filter(lambda x: x!=\"[]\", words))\n",
    "\n",
    "    #get all n_grams up to n=10\n",
    "    n = 8\n",
    "    ngrams_list = []\n",
    "\n",
    "    for word in words:\n",
    "        ngrams_list.append(list(filter(lambda x: x!=tuple('[') and x!= tuple(']'),list(ngrams(list(word),1)))))\n",
    "        for i in range(2,n+1):\n",
    "            ngrams_list.append(list(ngrams(list(word),i)))\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #Unroll the list\n",
    "    ngrams_list = flatten(ngrams_list)\n",
    "\n",
    "    ngrams_counter = Counter(ngrams_list)\n",
    "    print(len(ngrams_counter.keys()))\n",
    "\n",
    "    common_ngrams = []\n",
    "    for index,(key,value) in enumerate(ngrams_counter.most_common(num)):\n",
    "        common_ngrams.append(key)\n",
    "    \n",
    "    return common_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51794\n"
     ]
    }
   ],
   "source": [
    "num_ngrams = 100\n",
    "common_ngrams = give_common_ngrams(num_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e',),\n",
       " ('t',),\n",
       " ('o',),\n",
       " ('a',),\n",
       " ('h',),\n",
       " ('i',),\n",
       " ('n',),\n",
       " ('s',),\n",
       " ('u',),\n",
       " ('r',),\n",
       " ('e', ']'),\n",
       " ('[', 't'),\n",
       " ('t', ']'),\n",
       " ('l',),\n",
       " ('d',),\n",
       " ('y',),\n",
       " ('t', 'h'),\n",
       " ('[', 't', 'h'),\n",
       " ('[', 'i'),\n",
       " ('w',),\n",
       " ('[', 'a'),\n",
       " ('s', ']'),\n",
       " ('m',),\n",
       " ('h', 'e'),\n",
       " ('d', ']'),\n",
       " ('g',),\n",
       " ('[', 's'),\n",
       " ('c',),\n",
       " ('h', ']'),\n",
       " ('i', 'n'),\n",
       " ('t', 'h', 'e'),\n",
       " ('[', 'w'),\n",
       " ('y', ']'),\n",
       " ('f',),\n",
       " ('a', 'n'),\n",
       " ('[', 't', 'h', 'e'),\n",
       " ('o', 'u'),\n",
       " ('n', ']'),\n",
       " ('h', 'a'),\n",
       " ('o', ']'),\n",
       " ('b',),\n",
       " ('[', 'o'),\n",
       " ('k',),\n",
       " ('r', 'e'),\n",
       " ('[', 'y'),\n",
       " ('p',),\n",
       " ('a', 't'),\n",
       " ('i', ']'),\n",
       " ('e', 'r'),\n",
       " ('[', 'i', ']'),\n",
       " ('n', 'd'),\n",
       " ('u', 'h'),\n",
       " ('[', 'u'),\n",
       " ('i', 't'),\n",
       " ('n', 'd', ']'),\n",
       " ('v',),\n",
       " ('r', ']'),\n",
       " ('[', 'a', 'n'),\n",
       " ('e', 'a'),\n",
       " ('a', 't', ']'),\n",
       " ('[', 'b'),\n",
       " ('a', 'n', 'd'),\n",
       " ('[', 'h'),\n",
       " ('a', 'n', 'd', ']'),\n",
       " ('h', 'e', ']'),\n",
       " ('h', 'a', 't'),\n",
       " ('[', 'm'),\n",
       " ('h', 'a', 't', ']'),\n",
       " ('[', 'a', 'n', 'd'),\n",
       " ('[', 'a', 'n', 'd', ']'),\n",
       " ('v', 'e'),\n",
       " ('u', 'h', ']'),\n",
       " ('y', 'o'),\n",
       " ('n', 'g'),\n",
       " ('[', 'u', 'h'),\n",
       " ('[', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('[', 'y', 'o', 'u'),\n",
       " ('n', 'o'),\n",
       " ('e', 'n'),\n",
       " ('t', 'o'),\n",
       " ('t', 'h', 'a'),\n",
       " ('[', 'n'),\n",
       " ('[', 't', 'h', 'a'),\n",
       " ('[', 'd'),\n",
       " ('[', 'c'),\n",
       " ('u', ']'),\n",
       " ('n', 't'),\n",
       " ('r', 'e', ']'),\n",
       " ('t', 'h', 'e', ']'),\n",
       " ('[', 't', 'h', 'e', ']'),\n",
       " ('i', 'n', 'g'),\n",
       " ('o', 'u', ']'),\n",
       " ('y', 'o', 'u', ']'),\n",
       " ('[', 'y', 'o', 'u', ']'),\n",
       " ('g', ']'),\n",
       " ('a', ']'),\n",
       " ('o', 'n'),\n",
       " ('t', 'h', 'a', 't'),\n",
       " ('[', 't', 'h', 'a', 't')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map common ngrams to index values for one hot encoding\n",
    "ngram_to_index = {}\n",
    "#ngram_to_index\n",
    "for index,ngram in enumerate(common_ngrams):\n",
    "    ngram_to_index[ngram] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_letter_ngram(word):\n",
    "    \n",
    "    n=10\n",
    "    word_list = list(word)\n",
    "    letter_ngram = np.zeros(len(common_ngrams))\n",
    "    \n",
    "    #Extract ngrams from the word\n",
    "    ngrams_list = []\n",
    "    \n",
    "    ngrams_list.append(list(filter(lambda x: x!=tuple('[') and x!= tuple(']'),list(ngrams(list(word),1)))))\n",
    "    for i in range(2,n+1):\n",
    "        ngrams_list.append(list(ngrams(list(word),i)))\n",
    "    \n",
    "    #Flatten\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #Unroll the list\n",
    "    ngrams_list = flatten(ngrams_list)\n",
    "    \n",
    "    for ngram in ngrams_list:\n",
    "        if ngram in ngram_to_index.keys():\n",
    "            letter_ngram[ngram_to_index[ngram]] += 1\n",
    "        \n",
    "    return letter_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_letter_ngrams(words):\n",
    "    letter_ngrams = []\n",
    "    for word in words:\n",
    "        letter_ngrams.append(give_letter_ngram(word))\n",
    "    \n",
    "    return np.stack(letter_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_cos(x1,x2,cos):\n",
    "    \n",
    "    return (1-cos(x1,x2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(word_embedding,same_word_embedding,diff_word_embedding):\n",
    "    m = torch.tensor(1.0, dtype = torch.float).to(dev, non_blocking = True)\n",
    "    lower_bound = torch.tensor(0.0, dtype = torch.float).to(dev, non_blocking = True)\n",
    "    a = torch.max(lower_bound,m - cos(word_embedding ,same_word_embedding) + cos(word_embedding ,diff_word_embedding))\n",
    "\n",
    "    \n",
    "    return torch.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthographicNet(nn.Module):\n",
    "    def __init__(self,num_input,num_output):\n",
    "        super(OrthographicNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def give_embeddings(self,x,dev):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #print(x.shape)\n",
    "        #print(\"Done\")\n",
    "        return x.cpu().detach().numpy() if dev.type == 'cuda' else x.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input,num_output = 50000,9974\n",
    "orthographic_net = OrthographicNet(num_input,num_output)\n",
    "orthographic_net = orthographic_net.float()\n",
    "orthographic_net.to(dev)\n",
    "optimizer = optim.SGD(orthographic_net.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = torch.optim.Adadelta(orthographic_net.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "verbose = True\n",
    "model_save_path = \"./Models/best_orthographic_model2.pth\"\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "batch_limit = 10\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "    if verbose:\n",
    "            print('epoch %d '%(epoch))\n",
    "\n",
    "    train_loss = 0\n",
    "    orthographic_net.train()\n",
    "    for batch_idx, (train_data,train_labels) in enumerate(train_dl):\n",
    "\n",
    "        #print(train_data.shape)\n",
    "        #Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        train_data = train_data.to(dev, non_blocking=True)\n",
    "        #Get word mfcc features\n",
    "        word = train_data[:,0,:]\n",
    "        #Get labels\n",
    "        word_labels = [num_to_word[int(train_labels[i,0])] for i in range(train_labels.shape[0])]\n",
    "        diff_word_labels = [num_to_word[int(train_labels[i,1])] for i in range(train_labels.shape[0])]\n",
    "            \n",
    "            \n",
    "        #Get letter_ngrams\n",
    "        word_letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "        diff_letter_ngrams = torch.tensor(batch_letter_ngrams(diff_word_labels), dtype =torch.float,device = dev)\n",
    "\n",
    "        #Get the word embedding and letter_ngram embeddings\n",
    "        with torch.no_grad():\n",
    "            word_embedding = source_net(word)\n",
    "        \n",
    "        #word_embedding = np.stack([saved_word_embedding_dict.item().get(word).squeeze() for word in word_labels ])\n",
    "        #word_embedding = torch.tensor(word_embedding,dtype =torch.float, device = dev)\n",
    "        \n",
    "        word_ngram_embedding = orthographic_net(word_letter_ngrams)\n",
    "        diff_word_ngram_embedding = orthographic_net(diff_letter_ngrams)\n",
    "        \n",
    "        \n",
    "        #Calculate the triplet loss\n",
    "        \n",
    "        loss = triplet_loss(word_embedding,word_ngram_embedding,diff_word_ngram_embedding)\n",
    "        pdb.set_trace()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if batch_idx == batch_limit:\n",
    "            break\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    '''\n",
    "    orthographic_net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (val_data,val_labels) in enumerate(val_dl):\n",
    "\n",
    "            val_data = val_data.to(dev, non_blocking=True)\n",
    "            #Get word mfcc features\n",
    "            word = train_data[:,0,:]\n",
    "            #Get labels\n",
    "            word_labels = [num_to_word[int(val_labels[i,0])] for i in range(train_labels.shape[0])]\n",
    "            diff_word_labels = [num_to_word[int(val_labels[i,1])] for i in range(train_labels.shape[0])]\n",
    "            \n",
    "            #Get letter_ngrams\n",
    "            word_letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "            diff_letter_ngrams = torch.tensor(batch_letter_ngrams(diff_word_labels), dtype =torch.float,device = dev)\n",
    "\n",
    "            #Get the word embedding and letter_ngram embeddings\n",
    "            word_embedding = source_net(word)\n",
    "            \n",
    "            #word_embedding = np.stack([saved_word_embedding_dict.item().get(word).squeeze() for word in word_labels ])\n",
    "            #word_embedding = torch.tensor(word_embedding,dtype =torch.float, device = dev)\n",
    "            \n",
    "            word_ngram_embedding = orthographic_net(word_letter_ngrams)\n",
    "            diff_word_ngram_embedding = orthographic_net(diff_letter_ngrams)\n",
    "        \n",
    "            \n",
    "\n",
    "            #Calculate the triplet loss\n",
    "            val_loss += triplet_loss(word_embedding,word_ngram_embedding,diff_word_ngram_embedding)\n",
    "\n",
    "            if batch_idx == batch_limit:\n",
    "                break\n",
    "            \n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Best val loss %.3f Saving Model...\"%(val_loss/len(val_dl)))\n",
    "            torch.save(orthographic_net.state_dict(),model_save_path)\n",
    "        \n",
    "        '''\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"train loss: %.8f\"%(train_loss/len(train_dl)))\n",
    "        #print(\"val loss: %.5f\"%(val_loss/len(val_dl)))\n",
    "        \n",
    "    train_loss_list.append(train_loss/len(train_dl))\n",
    "    #val_loss_list.append(val_loss/len(val_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the learning curves\n",
    "\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(len(train_loss_list)),train_loss_list, label = 'train')\n",
    "plt.plot(range(len(train_loss_list)), val_loss_list, label = 'val')\n",
    "plt.legend()\n",
    "plt.savefig('orthographic_lc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best orthographic model\n",
    "model_save_path = \"./Models/best_orthographic_model2.pth\"\n",
    "orthographic_net.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_list = ['Data/feats_cmvn.ark']\n",
    "num_examples = np.Inf\n",
    "dh = DataHelper(load_list,num_examples)\n",
    "dh.load_data()\n",
    "dh.process_data()\n",
    "c,word_to_num,num_to_word = dh.generate_key_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save orthographic model word embeddings\n",
    "\n",
    "#Load Words\n",
    "words = list(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate word embedding dict\n",
    "word_embedding_dict = {}\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        word_features = give_letter_ngram(word)\n",
    "        #print(word_features)\n",
    "        word_embedding = orthographic_net(torch.tensor(word_features, dtype =torch.float, device = dev))\n",
    "\n",
    "        word_embedding_dict[word] = word_embedding.detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/orthographic_word_embedding_dict2.npy\",word_embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how similar are the word embeddings to the saved one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 173657\n",
      "Finished Loading the Data, 173657 examples\n",
      "Number of Unique words  9974\n"
     ]
    }
   ],
   "source": [
    "load_list = ['Data/feats_cmvn.ark']\n",
    "num_examples = np.Inf\n",
    "dh = DataHelper(load_list,num_examples)\n",
    "dh.load_data()\n",
    "dh.process_data()\n",
    "c,word_to_num_cl,num_to_word_cl = dh.generate_key_dicts()\n",
    "inputs,labels = dh.give_inputs_and_labels()\n",
    "del dh\n",
    "words = list(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_embedding_dict(words,net):\n",
    "    word_embedding_dict = OrderedDict()\n",
    "    #Calculate embeddings\n",
    "    for word in words:\n",
    "        #Find the mfcc features of the acoustic representation of the word in the data\n",
    "        word_features = inputs[np.where(np.isin(labels,word_to_num[word]))]\n",
    "        \n",
    "        #Calculate embeddings for the feature\n",
    "        word_embedding = net.give_embeddings(torch.tensor(word_features, device = dev, dtype=torch.float),dev)\n",
    "        \n",
    "        #If the number of representation is more than one, take the average embedding\n",
    "        word_embedding_dict[word] = np.mean(word_embedding, axis = 0).reshape(1,-1)\n",
    "    \n",
    "    return word_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = generate_word_embedding_dict(words,source_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the word embeddings\n",
    "saved_word_embedding_dict = np.load('Data/word_embedding_dict.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95097256\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for word in words:\n",
    "    calc_em = word_embedding_dict[word].reshape(1,-1)\n",
    "    saved_em = saved_word_embedding_dict.item().get(word).squeeze().reshape(1,-1)\n",
    "    a = pairwise_kernels(calc_em,saved_em, metric = \"cosine\")\n",
    "    #print(word,a)\n",
    "    d.append(a)\n",
    "print(np.mean(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the siamese dataset is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    }
   ],
   "source": [
    "#Check if the words are mapped to same mfcc or not\n",
    "matches = 0\n",
    "total_data = 0\n",
    "word_index = 1\n",
    "#Loop through training examples\n",
    "for batch_idx, (train_data,train_labels) in enumerate(train_dl):\n",
    "\n",
    "    #Loop through individual examples\n",
    "    for i in range(train_data.shape[0]):\n",
    "        \n",
    "        #Extract a word and it's mfcc\n",
    "        mfcc_index = 0 if word_index ==0 else 2\n",
    "        mfcc = train_data[i,mfcc_index]\n",
    "        label = train_labels[i,word_index]\n",
    "        word = num_to_word[int(label.numpy())]\n",
    "        \n",
    "        \n",
    "        #Get all mfccs for this word from the classic dataloader\n",
    "        label_cl_num = word_to_num_cl[word]\n",
    "        #print(label_cl_num)\n",
    "        ids = np.where(np.isin(labels,label_cl_num))\n",
    "        \n",
    "        #mfccs\n",
    "        mfccs = inputs[ids]\n",
    "        \n",
    "        for j in range(mfccs.shape[0]):\n",
    "            if np.array_equal(mfcc,mfccs[j]):\n",
    "                matches+=1\n",
    "    \n",
    "    total_data += train_data.shape[0]\n",
    "    break\n",
    "    \n",
    "print(total_data,matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if n-gram vector is rich enough to predict words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_one_hot(words):\n",
    "    \n",
    "    one_hot = np.zeros((len(words),len(word_to_num.keys())))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        one_hot[i,word_to_num[word]] = 1\n",
    "    \n",
    "    return one_hot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramNet(nn.Module):\n",
    "    def __init__(self,num_input,num_output):\n",
    "        super(NgramNet, self).__init__()\n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(num_input, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #print(x.shape)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_helpers import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 173657\n",
      "Finished Loading the Data, 173657 examples\n",
      "Number of Unique words  9974\n",
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 173657\n",
      "Finished Loading the Data, 173657 examples\n",
      "Number of Unique words  9974\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "num_examples = np.Inf\n",
    "train_ds = AMI_dataset(num_examples = num_examples, split_set = \"train\", data_filepath = \"Data/feats_cmvn.ark\", char_threshold = 5, frequency_bounds = (0,np.Inf))\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, pin_memory = True, shuffle = True, drop_last = True)\n",
    "\n",
    "val_ds = AMI_dataset(num_examples = num_examples, split_set = \"val\", data_filepath = \"Data/feats_cmvn.ark\", char_threshold = 5, frequency_bounds = (0,np.Inf))\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, pin_memory = True, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_word,word_to_num = train_ds.num_to_word,train_ds.word_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_to_num.keys())\n",
    "ngram_net = NgramNet(num_ngrams,num_words)\n",
    "ngram_net = ngram_net.float()\n",
    "ngram_net = ngram_net.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining training criterion\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngram_net.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \n"
     ]
    }
   ],
   "source": [
    "#Loop through words\n",
    "num_epochs = 100\n",
    "verbose = True\n",
    "model_save_path = \"./Models/best_ngram_model.pth\"\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "batch_limit = np.Inf\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "    if verbose:\n",
    "            print('epoch %d '%(epoch))\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    ngram_net.train()\n",
    "    for batch_idx, (train_data,train_labels) in enumerate(train_dl):\n",
    "\n",
    "        #print(train_data.shape)\n",
    "        #Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        #Get labels\n",
    "        word_labels = [num_to_word[int(train_labels[i])] for i in range(train_labels.shape[0])]\n",
    "        \n",
    "        \n",
    "        #Get letter_ngrams\n",
    "        letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "\n",
    "        \n",
    "        #Get labels as one hot\n",
    "        labels = train_labels.to(dev)\n",
    "        \n",
    "        #print(letter_ngrams.shape)\n",
    "        #print(labels)\n",
    "        \n",
    "        #Predict words using the model\n",
    "        predicted_labels = ngram_net(letter_ngrams)\n",
    "        \n",
    "        #Calculate loss\n",
    "        loss = criterion(predicted_labels,labels.long())\n",
    "        loss.backward()\n",
    "        plot_grad_flow(ngram_net.named_parameters())\n",
    "        \n",
    "        #for n,p in ngram_net.named_parameters():\n",
    "        #    print(n)\n",
    "        #    #print(p)\n",
    "        #    print(p.grad.abs().mean())\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(predicted_labels,labels)\n",
    "\n",
    "        if batch_idx == batch_limit:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    ngram_net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        for batch_idx, (val_data,val_labels) in enumerate(val_dl):\n",
    "            \n",
    "\n",
    "\n",
    "            #Get labels\n",
    "            word_labels = [num_to_word[int(val_labels[i])] for i in range(train_labels.shape[0])]\n",
    "            \n",
    "            \n",
    "            #Get letter_ngrams\n",
    "            letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "\n",
    "            \n",
    "            #Get labels as one hot\n",
    "            labels = val_labels.to(dev)\n",
    "            \n",
    "            #Predict words using the model\n",
    "            predicted_labels = ngram_net(letter_ngrams)\n",
    "            \n",
    "            #Calculate loss\n",
    "            batch_val_loss = criterion(predicted_labels,labels.long())\n",
    "        \n",
    "            \n",
    "\n",
    "            #Calculate the triplet loss\n",
    "            val_loss += batch_val_loss.item()\n",
    "            val_acc += accuracy(predicted_labels,labels)\n",
    "\n",
    "            \n",
    "            if batch_idx == batch_limit:\n",
    "                break\n",
    "            \n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Best val loss %.3f Saving Model...\"%(val_loss/len(val_dl)))\n",
    "            torch.save(ngram_net.state_dict(),model_save_path)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"train loss: %.3f train acc: %.3f\"%(train_loss/len(train_dl),train_acc/len(train_dl)))\n",
    "        print(\"val loss: %.3f val acc: %.3f\"%(val_loss/len(val_dl),val_acc/len(val_dl)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss_list.append(train_loss/len(train_dl))\n",
    "    train_acc_list.append(train_acc/len(train_dl))\n",
    "    val_loss_list.append(val_loss/len(val_dl))\n",
    "    val_acc_list.append(val_acc/len(val_dl))\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
