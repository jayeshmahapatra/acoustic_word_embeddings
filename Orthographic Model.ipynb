{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Core Python, Pandas, and kaldi_io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter,OrderedDict \n",
    "import kaldi_io\n",
    "from datetime import datetime\n",
    "\n",
    "#ngrams\n",
    "import nltk,re\n",
    "import nltk.corpus\n",
    "from nltk.corpus import switchboard\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#Scikit\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances,average_precision_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels,paired_distances\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#BigPhoney\n",
    "from big_phoney import BigPhoney\n",
    "\n",
    "\n",
    "#Torch and utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader,random_split,ConcatDataset\n",
    "\n",
    "#Import User defined classes\n",
    "from data_helpers import DataHelper\n",
    "from sfba4.utils import alignSequences\n",
    "from models import SimpleNet, SiameseNet, OrthographicNet\n",
    "from siamese_dataset import SiameseTriplets\n",
    "from ami_dataset import AMI_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load source model\n",
    "source_net = SimpleNet(9974)\n",
    "source_net = source_net.to(dev)\n",
    "source_net_save_path = \"./Models/awe_best_model.pth\"\n",
    "source_net.load_state_dict(torch.load(source_net_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 125006\n",
      "Finished Loading the Data, 125006 examples\n",
      "Number of Unique words  9974\n",
      "torch.Size([59844, 3, 40, 100])\n",
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 125006\n",
      "Finished Loading the Data, 125006 examples\n",
      "Number of Unique words  9974\n",
      "torch.Size([19948, 3, 40, 100])\n",
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 125006\n",
      "Finished Loading the Data, 125006 examples\n",
      "Number of Unique words  9974\n",
      "torch.Size([19948, 3, 40, 100])\n"
     ]
    }
   ],
   "source": [
    "num_examples = np.Inf\n",
    "frequency_bounds = (0,155)\n",
    "train_sm_dataset = SiameseTriplets(num_examples = num_examples, split_set = \"train\", frequency_bounds = frequency_bounds)\n",
    "val_sm_dataset = SiameseTriplets(num_examples = num_examples, split_set = \"val\", frequency_bounds = frequency_bounds)\n",
    "test_sm_dataset = SiameseTriplets(num_examples = num_examples, split_set = \"test\", frequency_bounds = frequency_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_sm_dataset, shuffle = True, batch_size = 64, pin_memory = True)\n",
    "val_dl = torch.utils.data.DataLoader(val_sm_dataset, shuffle = True, batch_size = 64, pin_memory = True)\n",
    "test_dl = torch.utils.data.DataLoader(test_sm_dataset, shuffle = True, batch_size = 64, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num,num_to_word = train_sm_dataset.word_to_num,train_sm_dataset.num_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(word):\n",
    "    #Remove punctuation\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    return \"[\"+word.lower()+\"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_common_ngrams(num = 50000):\n",
    "    switchboard.ensure_loaded()\n",
    "    words = switchboard.words()\n",
    "    #Add start and end of word markers and make words lower case\n",
    "    words = list(map(process_words,words))\n",
    "    #Filter empty words\n",
    "    words = list(filter(lambda x: x!=\"[]\", words))\n",
    "\n",
    "    #get all n_grams up to n=10\n",
    "    n = 8\n",
    "    ngrams_list = []\n",
    "\n",
    "    for word in words:\n",
    "        ngrams_list.append(list(filter(lambda x: x!=tuple('[') and x!= tuple(']'),list(ngrams(list(word),1)))))\n",
    "        for i in range(2,n+1):\n",
    "            ngrams_list.append(list(ngrams(list(word),i)))\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #Unroll the list\n",
    "    ngrams_list = flatten(ngrams_list)\n",
    "\n",
    "    ngrams_counter = Counter(ngrams_list)\n",
    "    print(len(ngrams_counter.keys()))\n",
    "\n",
    "    common_ngrams = []\n",
    "    for index,(key,value) in enumerate(ngrams_counter.most_common(num)):\n",
    "        common_ngrams.append(key)\n",
    "    \n",
    "    return common_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51794\n"
     ]
    }
   ],
   "source": [
    "num_ngrams = 10000\n",
    "common_ngrams = give_common_ngrams(num_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e',),\n",
       " ('t',),\n",
       " ('o',),\n",
       " ('a',),\n",
       " ('h',),\n",
       " ('i',),\n",
       " ('n',),\n",
       " ('s',),\n",
       " ('u',),\n",
       " ('r',),\n",
       " ('e', ']'),\n",
       " ('[', 't'),\n",
       " ('t', ']'),\n",
       " ('l',),\n",
       " ('d',),\n",
       " ('y',),\n",
       " ('t', 'h'),\n",
       " ('[', 't', 'h'),\n",
       " ('[', 'i'),\n",
       " ('w',),\n",
       " ('[', 'a'),\n",
       " ('s', ']'),\n",
       " ('m',),\n",
       " ('h', 'e'),\n",
       " ('d', ']'),\n",
       " ('g',),\n",
       " ('[', 's'),\n",
       " ('c',),\n",
       " ('h', ']'),\n",
       " ('i', 'n'),\n",
       " ('t', 'h', 'e'),\n",
       " ('[', 'w'),\n",
       " ('y', ']'),\n",
       " ('f',),\n",
       " ('a', 'n'),\n",
       " ('[', 't', 'h', 'e'),\n",
       " ('o', 'u'),\n",
       " ('n', ']'),\n",
       " ('h', 'a'),\n",
       " ('o', ']'),\n",
       " ('b',),\n",
       " ('[', 'o'),\n",
       " ('k',),\n",
       " ('r', 'e'),\n",
       " ('[', 'y'),\n",
       " ('p',),\n",
       " ('a', 't'),\n",
       " ('i', ']'),\n",
       " ('e', 'r'),\n",
       " ('[', 'i', ']'),\n",
       " ('n', 'd'),\n",
       " ('u', 'h'),\n",
       " ('[', 'u'),\n",
       " ('i', 't'),\n",
       " ('n', 'd', ']'),\n",
       " ('v',),\n",
       " ('r', ']'),\n",
       " ('[', 'a', 'n'),\n",
       " ('e', 'a'),\n",
       " ('a', 't', ']'),\n",
       " ('[', 'b'),\n",
       " ('a', 'n', 'd'),\n",
       " ('[', 'h'),\n",
       " ('a', 'n', 'd', ']'),\n",
       " ('h', 'e', ']'),\n",
       " ('h', 'a', 't'),\n",
       " ('[', 'm'),\n",
       " ('h', 'a', 't', ']'),\n",
       " ('[', 'a', 'n', 'd'),\n",
       " ('[', 'a', 'n', 'd', ']'),\n",
       " ('v', 'e'),\n",
       " ('u', 'h', ']'),\n",
       " ('y', 'o'),\n",
       " ('n', 'g'),\n",
       " ('[', 'u', 'h'),\n",
       " ('[', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('[', 'y', 'o', 'u'),\n",
       " ('n', 'o'),\n",
       " ('e', 'n'),\n",
       " ('t', 'o'),\n",
       " ('t', 'h', 'a'),\n",
       " ('[', 'n'),\n",
       " ('[', 't', 'h', 'a'),\n",
       " ('[', 'd'),\n",
       " ('[', 'c'),\n",
       " ('u', ']'),\n",
       " ('n', 't'),\n",
       " ('r', 'e', ']'),\n",
       " ('t', 'h', 'e', ']'),\n",
       " ('[', 't', 'h', 'e', ']'),\n",
       " ('i', 'n', 'g'),\n",
       " ('o', 'u', ']'),\n",
       " ('y', 'o', 'u', ']'),\n",
       " ('[', 'y', 'o', 'u', ']'),\n",
       " ('g', ']'),\n",
       " ('a', ']'),\n",
       " ('o', 'n'),\n",
       " ('t', 'h', 'a', 't'),\n",
       " ('[', 't', 'h', 'a', 't'),\n",
       " ('t', 'h', 'a', 't', ']'),\n",
       " ('[', 't', 'h', 'a', 't', ']'),\n",
       " ('o', 'r'),\n",
       " ('i', 't', ']'),\n",
       " ('[', 't', 'o'),\n",
       " ('n', 'g', ']'),\n",
       " ('f', ']'),\n",
       " ('o', 'w'),\n",
       " ('[', 'i', 't'),\n",
       " ('h', 'i'),\n",
       " ('m', 'e'),\n",
       " ('[', 'i', 't', ']'),\n",
       " ('[', 'l'),\n",
       " ('i', 'n', 'g', ']'),\n",
       " ('s', 't'),\n",
       " ('l', 'l'),\n",
       " ('[', 'a', ']'),\n",
       " ('w', ']'),\n",
       " ('l', ']'),\n",
       " ('a', 'l'),\n",
       " ('t', 'o', ']'),\n",
       " ('[', 'g'),\n",
       " ('[', 'r'),\n",
       " ('[', 'f'),\n",
       " ('[', 'p'),\n",
       " ('[', 'k'),\n",
       " ('a', 'r'),\n",
       " ('[', 't', 'o', ']'),\n",
       " ('e', 's'),\n",
       " ('o', 'w', ']'),\n",
       " ('w', 'e'),\n",
       " ('y', 'e'),\n",
       " ('s', 'e'),\n",
       " ('t', 'e'),\n",
       " ('[', 's', ']'),\n",
       " ('[', 'w', 'e'),\n",
       " ('[', 'y', 'e'),\n",
       " ('o', 'f'),\n",
       " ('[', 'o', 'f'),\n",
       " ('u', 't'),\n",
       " ('n', 'o', 'w'),\n",
       " ('[', 'u', 'h', ']'),\n",
       " ('s', 'o'),\n",
       " ('n', 't', ']'),\n",
       " ('n', 'o', 'w', ']'),\n",
       " ('m', ']'),\n",
       " ('t', 'h', 'i'),\n",
       " ('a', 's'),\n",
       " ('o', 'f', ']'),\n",
       " ('[', 'o', 'f', ']'),\n",
       " ('l', 'e'),\n",
       " ('y', 'e', 'a'),\n",
       " ('[', 'y', 'e', 'a'),\n",
       " ('[', 'i', 'n'),\n",
       " ('v', 'e', ']'),\n",
       " ('d', 'o'),\n",
       " ('b', 'e'),\n",
       " ('k', 'n'),\n",
       " ('[', 'k', 'n'),\n",
       " ('n', 'e'),\n",
       " ('k', 'n', 'o'),\n",
       " ('[', 'k', 'n', 'o'),\n",
       " ('i', 's'),\n",
       " ('k', 'n', 'o', 'w'),\n",
       " ('[', 'k', 'n', 'o', 'w'),\n",
       " ('u', 's'),\n",
       " ('[', 'd', 'o'),\n",
       " ('l', 'i'),\n",
       " ('t', 'i'),\n",
       " ('u', 't', ']'),\n",
       " ('k', 'n', 'o', 'w', ']'),\n",
       " ('[', 'k', 'n', 'o', 'w', ']'),\n",
       " ('[', 'h', 'a'),\n",
       " ('[', 's', 'o'),\n",
       " ('e', 'd'),\n",
       " ('h', 'i', 'n'),\n",
       " ('e', 'y'),\n",
       " ('o', 't'),\n",
       " ('e', 'y', ']'),\n",
       " ('e', 'r', ']'),\n",
       " ('a', 'h'),\n",
       " ('w', 'a'),\n",
       " ('[', 'b', 'e'),\n",
       " ('a', 'h', ']'),\n",
       " ('e', 'a', 'h'),\n",
       " ('y', 'e', 'a', 'h'),\n",
       " ('e', 'a', 'h', ']'),\n",
       " ('[', 'y', 'e', 'a', 'h'),\n",
       " ('y', 'e', 'a', 'h', ']'),\n",
       " ('[', 'y', 'e', 'a', 'h', ']'),\n",
       " ('o', 'm'),\n",
       " ('h', 'e', 'r'),\n",
       " ('t', 'h', 'i', 'n'),\n",
       " ('c', 'a'),\n",
       " ('e', 'l'),\n",
       " ('[', 'r', 'e'),\n",
       " ('h', 'o'),\n",
       " ('h', 'e', 'y'),\n",
       " ('h', 'e', 'y', ']'),\n",
       " ('h', 'u'),\n",
       " ('t', 'h', 'e', 'y'),\n",
       " ('[', 't', 'h', 'e', 'y'),\n",
       " ('t', 'h', 'e', 'y', ']'),\n",
       " ('[', 't', 'h', 'e', 'y', ']'),\n",
       " ('i', 'n', ']'),\n",
       " ('w', 'h'),\n",
       " ('e', 'r', 'e'),\n",
       " ('[', 'w', 'h'),\n",
       " ('r', 'i'),\n",
       " ('l', 'y'),\n",
       " ('l', 'l', ']'),\n",
       " ('[', 't', 'h', 'i'),\n",
       " ('[', 'e'),\n",
       " ('l', 'y', ']'),\n",
       " ('a', 's', ']'),\n",
       " ('e', 'd', ']'),\n",
       " ('c', 'o'),\n",
       " ('h', 'u', 'h'),\n",
       " ('e', 'n', ']'),\n",
       " ('e', 't'),\n",
       " ('e', 'e'),\n",
       " ('k', ']'),\n",
       " ('[', 'w', 'a'),\n",
       " ('h', 'u', 'h', ']'),\n",
       " ('[', 'i', 'n', ']'),\n",
       " ('o', 'r', ']'),\n",
       " ('u', 'r'),\n",
       " ('k', 'e'),\n",
       " ('d', 'o', ']'),\n",
       " ('s', 't', ']'),\n",
       " ('e', 'r', 'e', ']'),\n",
       " ('[', 'd', 'o', ']'),\n",
       " ('a', 'y'),\n",
       " ('e', 's', ']'),\n",
       " ('a', 'l', 'l'),\n",
       " ('p', 'e'),\n",
       " ('c', 'h'),\n",
       " ('[', 'l', 'i'),\n",
       " ('[', 'n', 't'),\n",
       " ('[', 'n', 't', ']'),\n",
       " ('h', 'h'),\n",
       " ('u', 'h', 'h'),\n",
       " ('h', 'h', 'u'),\n",
       " ('[', 'u', 'h', 'h'),\n",
       " ('u', 'h', 'h', 'u'),\n",
       " ('h', 'h', 'u', 'h'),\n",
       " ('[', 'u', 'h', 'h', 'u'),\n",
       " ('u', 'h', 'h', 'u', 'h'),\n",
       " ('h', 'h', 'u', 'h', ']'),\n",
       " ('[', 'u', 'h', 'h', 'u', 'h'),\n",
       " ('u', 'h', 'h', 'u', 'h', ']'),\n",
       " ('[', 'u', 'h', 'h', 'u', 'h', ']'),\n",
       " ('g', 'o'),\n",
       " ('g', 'h'),\n",
       " ('[', 'c', 'o'),\n",
       " ('i', 's', ']'),\n",
       " ('[', 'g', 'o'),\n",
       " ('r', 'o'),\n",
       " ('t', 'a'),\n",
       " ('t', 'h', 'e', 'r'),\n",
       " ('o', 'n', ']'),\n",
       " ('a', 'v'),\n",
       " ('[', 't', 'h', 'i', 'n'),\n",
       " ('l', 'd'),\n",
       " ('o', 't', ']'),\n",
       " ('m', 'e', ']'),\n",
       " ('w', 'o'),\n",
       " ('a', 'n', ']'),\n",
       " ('[', 'n', 'o'),\n",
       " ('[', 'o', 'n'),\n",
       " ('j',),\n",
       " ('b', 'u'),\n",
       " ('c', 'e'),\n",
       " ('s', 'e', ']'),\n",
       " ('h', 'a', 'v'),\n",
       " ('[', 'h', 'a', 'v'),\n",
       " ('a', 'v', 'e'),\n",
       " ('[', 'b', 'u'),\n",
       " ('h', 'e', 'r', 'e'),\n",
       " ('h', 'e', 'r', 'e', ']'),\n",
       " ('o', 'm', 'e'),\n",
       " ('a', 'v', 'e', ']'),\n",
       " ('i', 'g'),\n",
       " ('u', 'l'),\n",
       " ('v', 'e', 'r'),\n",
       " ('f', 'o'),\n",
       " ('k', 'e', ']'),\n",
       " ('[', 'w', 'o'),\n",
       " ('h', 'a', 'v', 'e'),\n",
       " ('h', 'a', 'v', 'e', ']'),\n",
       " ('[', 'h', 'a', 'v', 'e'),\n",
       " ('[', 'h', 'a', 'v', 'e', ']'),\n",
       " ('i', 'c'),\n",
       " ('[', 'j'),\n",
       " ('d', 'e'),\n",
       " ('l', 'd', ']'),\n",
       " ('o', 'o'),\n",
       " ('h', 'i', 'n', 'g'),\n",
       " ('w', 'e', ']'),\n",
       " ('g', 'e'),\n",
       " ('[', 'w', 'e', ']'),\n",
       " ('l', 'e', ']'),\n",
       " ('u', 'n'),\n",
       " ('r', 'y'),\n",
       " ('r', 's'),\n",
       " ('m', 'a'),\n",
       " ('l', 'o'),\n",
       " ('e', 'v'),\n",
       " ('[', 'v'),\n",
       " ('h', 't'),\n",
       " ('g', 'h', 't'),\n",
       " ('t', 'h', 'i', 'n', 'g'),\n",
       " ('i', 'd'),\n",
       " ('a', 'd'),\n",
       " ('e', 'm'),\n",
       " ('b', 'u', 't'),\n",
       " ('[', 'c', 'a'),\n",
       " ('[', 'f', 'o'),\n",
       " ('r', 'e', 'a'),\n",
       " ('[', 'b', 'u', 't'),\n",
       " ('b', 'u', 't', ']'),\n",
       " ('[', 'b', 'u', 't', ']'),\n",
       " ('a', 'y', ']'),\n",
       " ('o', 'u', 't'),\n",
       " ('i', 'l'),\n",
       " ('e', 'c'),\n",
       " ('e', 'v', 'e'),\n",
       " ('[', 'i', 's'),\n",
       " ('m', 'o'),\n",
       " ('i', 'f'),\n",
       " ('l', 'a'),\n",
       " ('j', 'u'),\n",
       " ('f', 'o', 'r'),\n",
       " ('s', 'h'),\n",
       " ('w', 'a', 's'),\n",
       " ('[', 'w', 'a', 's'),\n",
       " ('[', 'i', 's', ']'),\n",
       " ('w', 'a', 's', ']'),\n",
       " ('[', 'w', 'a', 's', ']'),\n",
       " ('[', 'j', 'u'),\n",
       " ('s', 'i'),\n",
       " ('[', 'h', 'e'),\n",
       " ('b', 'o'),\n",
       " ('o', 'n', 'e'),\n",
       " ('i', 'g', 'h'),\n",
       " ('s', 'o', ']'),\n",
       " ('e', 'n', 't'),\n",
       " ('o', 'l'),\n",
       " ('n', 'e', ']'),\n",
       " ('h', 't', ']'),\n",
       " ('g', 'h', 't', ']'),\n",
       " ('a', 'c'),\n",
       " ('r', 't'),\n",
       " ('u', 's', 't'),\n",
       " ('o', 'u', 't', ']'),\n",
       " ('e', 'l', 'l'),\n",
       " ('r', 'a'),\n",
       " ('d', 'i'),\n",
       " ('[', 's', 'o', ']'),\n",
       " ('n', 'k'),\n",
       " ('u', 's', 't', ']'),\n",
       " ('i', 'o'),\n",
       " ('l', 'l', 'y'),\n",
       " ('t', 'h', 'e', 'r', 'e'),\n",
       " ('l', 'l', 'y', ']'),\n",
       " ('e', 'l', 'l', ']'),\n",
       " ('[', 't', 'h', 'e', 'r'),\n",
       " ('[', 't', 'h', 'e', 'r', 'e'),\n",
       " ('t', 'r'),\n",
       " ('t', 'h', 'e', 'r', 'e', ']'),\n",
       " ('[', 't', 'h', 'e', 'r', 'e', ']'),\n",
       " ('s', 'o', 'm'),\n",
       " ('[', 's', 'o', 'm'),\n",
       " ('s', 'o', 'm', 'e'),\n",
       " ('[', 's', 'o', 'm', 'e'),\n",
       " ('x',),\n",
       " ('o', 'u', 'l'),\n",
       " ('u', 'm'),\n",
       " ('j', 'u', 's'),\n",
       " ('j', 'u', 's', 't'),\n",
       " ('[', 'f', 'o', 'r'),\n",
       " ('u', 'l', 'd'),\n",
       " ('o', 'u', 'l', 'd'),\n",
       " ('u', 'l', 'd', ']'),\n",
       " ('o', 'u', 'l', 'd', ']'),\n",
       " ('i', 'k'),\n",
       " ('i', 'k', 'e'),\n",
       " ('a', 'l', 'l', 'y'),\n",
       " ('a', 'l', 'l', 'y', ']'),\n",
       " ('[', 'j', 'u', 's'),\n",
       " ('[', 'j', 'u', 's', 't'),\n",
       " ('w', 'i'),\n",
       " ('l', 'i', 'k'),\n",
       " ('[', 'l', 'i', 'k'),\n",
       " ('l', 'i', 'k', 'e'),\n",
       " ('[', 'l', 'i', 'k', 'e'),\n",
       " ('[', 's', 'e'),\n",
       " ('a', 'b'),\n",
       " ('j', 'u', 's', 't', ']'),\n",
       " ('[', 'j', 'u', 's', 't', ']'),\n",
       " ('i', 'g', 'h', 't'),\n",
       " ('h', 'i', 'n', 'g', ']'),\n",
       " ('i', 'k', 'e', ']'),\n",
       " ('l', 'i', 'k', 'e', ']'),\n",
       " ('[', 'l', 'i', 'k', 'e', ']'),\n",
       " ('a', 'r', 'e'),\n",
       " ('p', 'l'),\n",
       " ('i', 'n', 'k'),\n",
       " ('w', 'e', 'l'),\n",
       " ('[', 'v', 'e'),\n",
       " ('p', 'r'),\n",
       " ('n', 'k', ']'),\n",
       " ('h', 'i', 'n', 'k'),\n",
       " ('t', 'h', 'i', 'n', 'k'),\n",
       " ('[', 't', 'h', 'i', 'n', 'k'),\n",
       " ('t', 't'),\n",
       " ('k', 'i'),\n",
       " ('[', 'm', 'o'),\n",
       " ('[', 'w', 'e', 'l'),\n",
       " ('n', 's'),\n",
       " ('r', 'y', ']'),\n",
       " ('[', 'm', 'a'),\n",
       " ('s', 's'),\n",
       " ('w', 'e', 'l', 'l'),\n",
       " ('t', 'h', ']'),\n",
       " ('w', 'e', 'l', 'l', ']'),\n",
       " ('[', 'w', 'e', 'l', 'l'),\n",
       " ('s', 'a'),\n",
       " ('[', 'w', 'e', 'l', 'l', ']'),\n",
       " ('i', 'n', 'k', ']'),\n",
       " ('c', 'e', ']'),\n",
       " ('[', 'r', 'e', 'a'),\n",
       " ('[', 's', 't'),\n",
       " ('t', 'e', 'r'),\n",
       " ('h', 'i', 'n', 'k', ']'),\n",
       " ('t', 'h', 'i', 'n', 'k', ']'),\n",
       " ('[', 't', 'h', 'i', 'n', 'k', ']'),\n",
       " ('i', 'g', 'h', 't', ']'),\n",
       " ('[', 'l', 'o'),\n",
       " ('[', 'a', 'l'),\n",
       " ('w', 'h', 'a'),\n",
       " ('[', 'w', 'i'),\n",
       " ('i', 'm'),\n",
       " ('[', 'w', 'h', 'a'),\n",
       " ('w', 'h', 'a', 't'),\n",
       " ('[', 'm', 'e'),\n",
       " ('[', 'p', 'e'),\n",
       " ('o', 'n', 'e', ']'),\n",
       " ('t', 'h', 'i', 'n', 'g', ']'),\n",
       " ('b', 'e', ']'),\n",
       " ('[', 'w', 'h', 'a', 't'),\n",
       " ('o', 'p'),\n",
       " ('[', 'a', 'r'),\n",
       " ('o', 'u', 'r'),\n",
       " ('i', 'r'),\n",
       " ('i', 'o', 'n'),\n",
       " ('e', 'a', 'l'),\n",
       " ('f', 'o', 'r', ']'),\n",
       " ('[', 'f', 'o', 'r', ']'),\n",
       " ('n', 'i'),\n",
       " ('[', 's', 'h'),\n",
       " ('[', 'p', 'r'),\n",
       " ('a', 'i'),\n",
       " ('a', 'r', 'e', ']'),\n",
       " ('n', 'o', 't'),\n",
       " ('h', 'e', 'n'),\n",
       " ('i', 'e'),\n",
       " ('[', 'o', 'u'),\n",
       " ('[', 's', 'a'),\n",
       " ('f', 'i'),\n",
       " ('w', 'h', 'a', 't', ']'),\n",
       " ('u', 's', 'e'),\n",
       " ('p', 'a'),\n",
       " ('[', 'r', 'i'),\n",
       " ('h', 'e', 'n', ']'),\n",
       " ('[', 'w', 'h', 'a', 't', ']'),\n",
       " ('a', 'd', ']'),\n",
       " ('[', 'o', 'r'),\n",
       " ('r', 'e', 'a', 'l'),\n",
       " ('[', 'r', 'e', 'a', 'l'),\n",
       " ('e', 't', ']'),\n",
       " ('o', 's'),\n",
       " ('p', ']'),\n",
       " ('r', 'i', 'g'),\n",
       " ('n', 'c'),\n",
       " ('[', 'b', 'e', ']'),\n",
       " ('[', 'n', 'o', 't'),\n",
       " ('o', 'i'),\n",
       " ('a', 'm'),\n",
       " ('[', 'h', 'o'),\n",
       " ('i', 't', 'h'),\n",
       " ('t', 'y'),\n",
       " ('o', 'd'),\n",
       " ('[', 'i', 'f'),\n",
       " ('i', 'f', ']'),\n",
       " ('[', 'i', 'f', ']'),\n",
       " ('[', 'o', 'n', ']'),\n",
       " ('b', 'l'),\n",
       " ('a', 'u'),\n",
       " ('e', 'v', 'e', 'r'),\n",
       " ('n', 'y'),\n",
       " ('e', 'x'),\n",
       " ('[', 'o', 'r', ']'),\n",
       " ('r', 'i', 'g', 'h'),\n",
       " ('[', 'r', 'i', 'g'),\n",
       " ('r', 'i', 'g', 'h', 't'),\n",
       " ('u', 'm', ']'),\n",
       " ('c', 't'),\n",
       " ('[', 'r', 'i', 'g', 'h'),\n",
       " ('w', 'h', 'e'),\n",
       " ('[', 'r', 'i', 'g', 'h', 't'),\n",
       " ('h', 'e', 'r', ']'),\n",
       " ('o', 'm', 'e', ']'),\n",
       " ('r', 'i', 'g', 'h', 't', ']'),\n",
       " ('[', 'r', 'i', 'g', 'h', 't', ']'),\n",
       " ('r', 's', ']'),\n",
       " ('[', 'a', 'r', 'e'),\n",
       " ('[', 'u', 'm'),\n",
       " ('u', 'p'),\n",
       " ('[', 'u', 'm', ']'),\n",
       " ('[', 'd', 'i'),\n",
       " ('[', 'r', 'e', ']'),\n",
       " ('f', 'e'),\n",
       " ('e', 'i'),\n",
       " ('g', 'e', 't'),\n",
       " ('[', 'w', 'h', 'e'),\n",
       " ('a', 'l', ']'),\n",
       " ('w', 'i', 't'),\n",
       " ('[', 'w', 'i', 't'),\n",
       " ('e', 'a', 'r'),\n",
       " ('u', 's', 'e', ']'),\n",
       " ('s', 'u'),\n",
       " ('w', 'i', 't', 'h'),\n",
       " ('[', 'w', 'i', 't', 'h'),\n",
       " ('k', 'i', 'n'),\n",
       " ('a', 'n', 'y'),\n",
       " ('[', 'g', 'e'),\n",
       " ('v', 'i'),\n",
       " ('m', 'i'),\n",
       " ('e', 's', 's'),\n",
       " ('b', 'a'),\n",
       " ('[', 'a', 'r', 'e', ']'),\n",
       " ('p', 'o'),\n",
       " ('[', 'a', 'b'),\n",
       " ('c', 'h', ']'),\n",
       " ('c', 'o', 'u'),\n",
       " ('[', 'm', ']'),\n",
       " ('e', 'm', ']'),\n",
       " ('i', 'd', ']'),\n",
       " ('n', 'o', 't', ']'),\n",
       " ('t', 'i', 'o'),\n",
       " ('m', 'y'),\n",
       " ('[', 'c', 'h'),\n",
       " ('i', 'o', 'n', ']'),\n",
       " ('[', 'n', 'o', 't', ']'),\n",
       " ('t', 'i', 'o', 'n'),\n",
       " ('i', 'v'),\n",
       " ('t', 'u'),\n",
       " ('o', 'i', 'n'),\n",
       " ('p', 'l', 'e'),\n",
       " ('[', 'n', 'e'),\n",
       " ('o', 'h'),\n",
       " ('t', 's'),\n",
       " ('e', 'n', 't', ']'),\n",
       " ('o', 'h', ']'),\n",
       " ('[', 'c', 'o', 'u'),\n",
       " ('w', 'o', 'u'),\n",
       " ('[', 'w', 'o', 'u'),\n",
       " ('w', 'o', 'u', 'l'),\n",
       " ('[', 'w', 'o', 'u', 'l'),\n",
       " ('[', 'm', 'y'),\n",
       " ('[', 'o', 'h'),\n",
       " ('[', 'o', 'h', ']'),\n",
       " ('a', 't', 'e'),\n",
       " ('w', 'o', 'u', 'l', 'd'),\n",
       " ('[', 'w', 'o', 'u', 'l', 'd'),\n",
       " ('w', 'o', 'u', 'l', 'd', ']'),\n",
       " ('[', 'w', 'o', 'u', 'l', 'd', ']'),\n",
       " ('[', 't', 'h', 'i', 'n', 'g'),\n",
       " ('h', 'o', 'u'),\n",
       " ('e', 'o'),\n",
       " ('m', 'y', ']'),\n",
       " ('i', 't', 'h', ']'),\n",
       " ('e', 'r', 'y'),\n",
       " ('u', 'r', ']'),\n",
       " ('u', 'g'),\n",
       " ('g', 'u'),\n",
       " ('[', 'e', 'v'),\n",
       " ('o', 'u', 'r', ']'),\n",
       " ('w', 'i', 't', 'h', ']'),\n",
       " ('[', 'w', 'i', 't', 'h', ']'),\n",
       " ('s', 's', ']'),\n",
       " ('[', 't', 'e'),\n",
       " ('v', 'e', 'r', 'y'),\n",
       " ('[', 'v', 'e', ']'),\n",
       " ('[', 'o', 'n', 'e'),\n",
       " ('[', 'g', 'e', 't'),\n",
       " ('a', 'k'),\n",
       " ('[', 'm', 'y', ']'),\n",
       " ('a', 'b', 'o'),\n",
       " ('[', 'f', 'i'),\n",
       " ('[', 'k', 'i'),\n",
       " ('a', 'l', 'l', ']'),\n",
       " ('[', 't', 'a'),\n",
       " ('[', 'e', 'v', 'e'),\n",
       " ('e', 'a', 'l', 'l'),\n",
       " ('r', 'e', 'a', 'l', 'l'),\n",
       " ('e', 'a', 'l', 'l', 'y'),\n",
       " ('[', 'r', 'e', 'a', 'l', 'l'),\n",
       " ('r', 'e', 'a', 'l', 'l', 'y'),\n",
       " ('e', 'a', 'l', 'l', 'y', ']'),\n",
       " ('[', 'r', 'e', 'a', 'l', 'l', 'y'),\n",
       " ('r', 'e', 'a', 'l', 'l', 'y', ']'),\n",
       " ('[', 'r', 'e', 'a', 'l', 'l', 'y', ']'),\n",
       " ('[', 'a', 's'),\n",
       " ('t', 'y', ']'),\n",
       " ('[', 'a', 'b', 'o'),\n",
       " ('[', 't', 'r'),\n",
       " ('[', 'a', 't'),\n",
       " ('p', 'l', 'e', ']'),\n",
       " ('o', 'u', 'n'),\n",
       " ('u', 'e'),\n",
       " ('b', 'o', 'u'),\n",
       " ('e', 'e', ']'),\n",
       " ('t', 's', ']'),\n",
       " ('[', 'p', 'a'),\n",
       " ('o', 'i', 'n', 'g'),\n",
       " ('o', 'i', 'n', 'g', ']'),\n",
       " ('s', 't', 'a'),\n",
       " ('[', 'o', 'u', 't'),\n",
       " ('n', 'c', 'e'),\n",
       " ('c', 'a', 'u'),\n",
       " ('u', 'g', 'h'),\n",
       " ('f', 'r'),\n",
       " ('i', 'm', 'e'),\n",
       " ('t', 'h', 'e', 'r', ']'),\n",
       " ('b', 'e', 'c'),\n",
       " ('[', 'o', 'n', 'e', ']'),\n",
       " ('u', 'c'),\n",
       " ('[', 'b', 'e', 'c'),\n",
       " ('a', 'u', 's'),\n",
       " ('w', 'o', 'r'),\n",
       " ('v', 'e', 'r', ']'),\n",
       " ('c', 'a', 'u', 's'),\n",
       " ('a', 'u', 's', 'e'),\n",
       " ('c', 'a', 'u', 's', 'e'),\n",
       " ('a', 'b', 'o', 'u'),\n",
       " ('b', 'o', 'u', 't'),\n",
       " ('[', 'a', 'b', 'o', 'u'),\n",
       " ('a', 'b', 'o', 'u', 't'),\n",
       " ('b', 'o', 'u', 't', ']'),\n",
       " ('[', 'a', 'b', 'o', 'u', 't'),\n",
       " ('a', 'b', 'o', 'u', 't', ']'),\n",
       " ('[', 'a', 'b', 'o', 'u', 't', ']'),\n",
       " ('[', 'w', 'o', 'r'),\n",
       " ('d', 's'),\n",
       " ('[', 'f', 'r'),\n",
       " ('c', 'o', 'm'),\n",
       " ('a', 'u', 's', 'e', ']'),\n",
       " ('c', 'a', 'u', 's', 'e', ']'),\n",
       " ('t', 'e', ']'),\n",
       " ('e', 's', 's', ']'),\n",
       " ('e', 'r', 's'),\n",
       " ('h', 'e', 'm'),\n",
       " ('i', 'v', 'e'),\n",
       " ('g', 'r'),\n",
       " ('a', 't', 'i'),\n",
       " ('d', 's', ']'),\n",
       " ('m', 'u'),\n",
       " ('w', 'a', 'y'),\n",
       " ('p', 'r', 'o'),\n",
       " ('s', 'e', 'e'),\n",
       " ('e', 'o', 'p'),\n",
       " ('[', 's', 'e', 'e'),\n",
       " ('p', 'e', 'o'),\n",
       " ('[', 'p', 'e', 'o'),\n",
       " ('p', 'e', 'o', 'p'),\n",
       " ('[', 'p', 'e', 'o', 'p'),\n",
       " ('e', 'c', 'a'),\n",
       " ('[', 'a', 's', ']'),\n",
       " ('t', 'i', 'm'),\n",
       " ('o', 'v'),\n",
       " ('o', 'p', 'l'),\n",
       " ('e', 'o', 'p', 'l'),\n",
       " ('o', 'p', 'l', 'e'),\n",
       " ('p', 'e', 'o', 'p', 'l'),\n",
       " ('e', 'o', 'p', 'l', 'e'),\n",
       " ('[', 'p', 'e', 'o', 'p', 'l'),\n",
       " ('p', 'e', 'o', 'p', 'l', 'e'),\n",
       " ('[', 'p', 'e', 'o', 'p', 'l', 'e'),\n",
       " ('h', 'i', 's'),\n",
       " ('t', 'l'),\n",
       " ('c', 'k'),\n",
       " ('l', 'o', 't'),\n",
       " ('t', 'h', 'e', 'm'),\n",
       " ('[', 't', 'h', 'e', 'm'),\n",
       " ('g', 'e', 't', ']'),\n",
       " ('o', 'p', 'l', 'e', ']'),\n",
       " ('e', 'o', 'p', 'l', 'e', ']'),\n",
       " ('p', 'e', 'o', 'p', 'l', 'e', ']'),\n",
       " ('[', 'p', 'e', 'o', 'p', 'l', 'e', ']'),\n",
       " ('[', 'l', 'o', 't'),\n",
       " ('c', 'a', 'n'),\n",
       " ('n', 'a'),\n",
       " ('b', 'e', 'c', 'a'),\n",
       " ('[', 'b', 'e', 'c', 'a'),\n",
       " ('i', 'n', 'd'),\n",
       " ('[', 'o', 'u', 't', ']'),\n",
       " ('[', 'a', 'l', 'l'),\n",
       " ('o', 'b'),\n",
       " ('l', 'o', 't', ']'),\n",
       " ('e', 'a', 'n'),\n",
       " ('o', 'r', 'e'),\n",
       " ('[', 'l', 'o', 't', ']'),\n",
       " ('a', 'r', ']'),\n",
       " ('e', 'c', 'a', 'u'),\n",
       " ('b', 'e', 'c', 'a', 'u'),\n",
       " ('[', 'b', 'e', 'c', 'a', 'u'),\n",
       " ('[', 'p', 'r', 'o'),\n",
       " ('e', 'c', 'a', 'u', 's'),\n",
       " ('b', 'e', 'c', 'a', 'u', 's'),\n",
       " ('e', 'c', 'a', 'u', 's', 'e'),\n",
       " ('[', 'b', 'e', 'c', 'a', 'u', 's'),\n",
       " ('b', 'e', 'c', 'a', 'u', 's', 'e'),\n",
       " ('e', 'c', 'a', 'u', 's', 'e', ']'),\n",
       " ('[', 'b', 'e', 'c', 'a', 'u', 's', 'e'),\n",
       " ('b', 'e', 'c', 'a', 'u', 's', 'e', ']'),\n",
       " ('m', 'e', 'a'),\n",
       " ('o', 'k'),\n",
       " ('[', 'm', 'e', 'a'),\n",
       " ('[', 's', 'u'),\n",
       " ('t', 'e', 'r', ']'),\n",
       " ('a', 'k', 'e'),\n",
       " ('r', 'd'),\n",
       " ('o', 't', 'h'),\n",
       " ('t', 'i', 'o', 'n', ']'),\n",
       " ('a', 'g'),\n",
       " ('d', 'a'),\n",
       " ('u', 'p', ']'),\n",
       " ('[', 'g', 'u'),\n",
       " ('b', 'l', 'e'),\n",
       " ('h', 'e', 'm', ']'),\n",
       " ('t', 'h', 'e', 'm', ']'),\n",
       " ('[', 't', 'h', 'e', 'm', ']'),\n",
       " ('h', 'i', 's', ']'),\n",
       " ('[', 'a', 'n', 'y'),\n",
       " ('t', 'h', 'o'),\n",
       " ('[', 'a', 't', ']'),\n",
       " ('t', 'i', 'm', 'e'),\n",
       " ('f', 'f'),\n",
       " ('[', 'a', 'l', 'l', ']'),\n",
       " ('[', 'g', 'e', 't', ']'),\n",
       " ('o', 'u', 'g'),\n",
       " ('o', 'u', 'g', 'h'),\n",
       " ('[', 'u', 'p'),\n",
       " ('[', 'm', 'u'),\n",
       " ('h', 'a', 'd'),\n",
       " ('[', 'h', 'a', 'd'),\n",
       " ('h', 'a', 'd', ']'),\n",
       " ('[', 'h', 'a', 'd', ']'),\n",
       " ('f', 'a'),\n",
       " ('m', 'e', 'a', 'n'),\n",
       " ('t', 'i', 'n'),\n",
       " ('[', 'm', 'e', 'a', 'n'),\n",
       " ('e', 'e', 'n'),\n",
       " ('e', 'a', 'n', ']'),\n",
       " ('o', 'r', 'e', ']'),\n",
       " ('u', 'a'),\n",
       " ('n', 's', ']'),\n",
       " ('[', 'b', 'a'),\n",
       " ('g', 's'),\n",
       " ('e', 's', 't'),\n",
       " ('g', 's', ']'),\n",
       " ('s', 'h', 'e'),\n",
       " ('[', 'u', 'p', ']'),\n",
       " ('a', 'n', 't'),\n",
       " ('e', 't', 'h'),\n",
       " ('r', 'k'),\n",
       " ('o', 'd', ']'),\n",
       " ('m', 'e', 'a', 'n', ']'),\n",
       " ('[', 'm', 'e', 'a', 'n', ']'),\n",
       " ('l', 'i', 't'),\n",
       " ('[', 'h', 'e', ']'),\n",
       " ('[', 't', 'i'),\n",
       " ('h', 'a', 'n'),\n",
       " ('o', 'o', 'd'),\n",
       " ('s', 'p'),\n",
       " ('w', 'h', 'e', 'n'),\n",
       " ('[', 'w', 'h', 'e', 'n'),\n",
       " ('n', 'y', ']'),\n",
       " ('e', 'r', 'y', ']'),\n",
       " ('e', 'e', 'n', ']'),\n",
       " ('n', 'c', 'e', ']'),\n",
       " ('o', 'r', 't'),\n",
       " ('[', 'f', 'a'),\n",
       " ('u', 'n', 'd'),\n",
       " ('p', 'e', 'r'),\n",
       " ('[', 'c', 'o', 'm'),\n",
       " ('[', 's', 'h', 'e'),\n",
       " ('i', 'a'),\n",
       " ('v', 'e', 'r', 'y', ']'),\n",
       " ('s', 't', 'i'),\n",
       " ('t', 'i', 'n', 'g'),\n",
       " ('g', 'o', ']'),\n",
       " ('t', 'i', 'n', 'g', ']'),\n",
       " ('g', 'o', 't'),\n",
       " ('w', 'e', 'r'),\n",
       " ('m', 'e', 't'),\n",
       " ('[', 's', 'i'),\n",
       " ('w', 'h', 'e', 'n', ']'),\n",
       " ('[', 'w', 'h', 'e', 'n', ']'),\n",
       " ('e', 'v', 'e', 'r', ']'),\n",
       " ('o', 'm', 'e', 't'),\n",
       " ('s', 'o', 'm', 'e', 't'),\n",
       " ('[', 's', 'o', 'm', 'e', 't'),\n",
       " ('o', 'r', 'k'),\n",
       " ('n', 'g', 's'),\n",
       " ('g', 'h', ']'),\n",
       " ('[', 'g', 'o', 't'),\n",
       " ('y', 's'),\n",
       " ('c', 'a', 'n', ']'),\n",
       " ('s', 'o', 'm', 'e', ']'),\n",
       " ('[', 's', 'o', 'm', 'e', ']'),\n",
       " ('n', 'g', 's', ']'),\n",
       " ('a', 'p'),\n",
       " ('[', 'l', 'e'),\n",
       " ('c', 'i'),\n",
       " ('o', 'o', 'd', ']'),\n",
       " ('r', 'e', 'n'),\n",
       " ('s', 'h', 'e', ']'),\n",
       " ('[', 's', 'h', 'e', ']'),\n",
       " ('i', 'n', 't'),\n",
       " ('i', 'n', 'g', 's'),\n",
       " ('i', 'n', 'g', 's', ']'),\n",
       " ('t', 'h', 'e', 'n'),\n",
       " ('p', 'u'),\n",
       " ('t', 'h', 'e', 'n', ']'),\n",
       " ('o', 'm', ']'),\n",
       " ('[', 't', 'i', 'm'),\n",
       " ('n', 'o', ']'),\n",
       " ('i', 'r', ']'),\n",
       " ('[', 't', 'h', 'e', 'n'),\n",
       " ('[', 't', 'h', 'e', 'n', ']'),\n",
       " ('[', 't', 'i', 'm', 'e'),\n",
       " ('r', 'e', 's'),\n",
       " ('m', 'o', 'r'),\n",
       " ('o', 'v', 'e'),\n",
       " ('m', 'p'),\n",
       " ('i', 'v', 'e', ']'),\n",
       " ('[', 'c', 'a', 'n'),\n",
       " ('a', 'k', 'e', ']'),\n",
       " ('g', 'o', 'i'),\n",
       " ('[', 'g', 'o', 'i'),\n",
       " ('[', 'e', 'x'),\n",
       " ('t', 'h', 'i', 's'),\n",
       " ('[', 't', 'h', 'i', 's'),\n",
       " ('t', 'h', 'i', 's', ']'),\n",
       " ('[', 't', 'h', 'i', 's', ']'),\n",
       " ('a', 'c', 't'),\n",
       " ('b', 'i'),\n",
       " ('i', 'n', 'd', ']'),\n",
       " ('g', 'o', 'i', 'n'),\n",
       " ('[', 'g', 'o', 'i', 'n'),\n",
       " ('g', 'o', 'i', 'n', 'g'),\n",
       " ('[', 'g', 'o', 'i', 'n', 'g'),\n",
       " ('g', 'o', 'i', 'n', 'g', ']'),\n",
       " ('[', 'g', 'o', 'i', 'n', 'g', ']'),\n",
       " ('w', 'o', 'r', 'k'),\n",
       " ('[', 't', 'h', 'o'),\n",
       " ('i', 'm', 'e', ']'),\n",
       " ('w', 'n'),\n",
       " ('r', 'e', 'e'),\n",
       " ('[', 's', 't', 'a'),\n",
       " ('g', 'e', ']'),\n",
       " ('[', 'w', 'o', 'r', 'k'),\n",
       " ('r', 'n'),\n",
       " ('a', 't', 'i', 'o'),\n",
       " ('a', 'm', 'e'),\n",
       " ('[', 'd', 'e'),\n",
       " ('h', 'o', 'w'),\n",
       " ('i', 'n', 'e'),\n",
       " ('[', 'g', 'o', ']'),\n",
       " ('[', 'h', 'i'),\n",
       " ('a', 'n', 'y', ']'),\n",
       " ('o', 'w', 'n'),\n",
       " ('a', 't', 'i', 'o', 'n'),\n",
       " ('a', 'r', 't'),\n",
       " ('r', 'u'),\n",
       " ('u', 'r', 'e'),\n",
       " ('g', 'o', 't', ']'),\n",
       " ('[', 'p', 'o'),\n",
       " ('[', 'g', 'o', 't', ']'),\n",
       " ('d', 'i', 'd'),\n",
       " ('d', 'i', 'd', ']'),\n",
       " ('[', 'e', 'v', 'e', 'r'),\n",
       " ('u', 'e', 's'),\n",
       " ('[', 'n', 'o', ']'),\n",
       " ('[', 'c', 'a', 'n', ']'),\n",
       " ('o', 't', 'h', 'e'),\n",
       " ('o', 't', 'h', 'e', 'r'),\n",
       " ('[', 'd', 'i', 'd'),\n",
       " ('[', 'd', 'i', 'd', ']'),\n",
       " ('i', 't', 't'),\n",
       " ('t', 'e', 'd'),\n",
       " ('s', 'c'),\n",
       " ('[', 'w', 'e', 'r'),\n",
       " ('w', 'e', 'r', 'e'),\n",
       " ('[', 'w', 'e', 'r', 'e'),\n",
       " ('w', 'e', 'r', 'e', ']'),\n",
       " ('[', 'w', 'e', 'r', 'e', ']'),\n",
       " ('l', 's'),\n",
       " ('[', 'l', 'a'),\n",
       " ('y', 'e', 'a', 'r'),\n",
       " ('[', 'y', 'e', 'a', 'r'),\n",
       " ('e', 't', 't'),\n",
       " ('t', 'e', 'd', ']'),\n",
       " ('h', 'i', 'n', 'g', 's'),\n",
       " ('t', 'h', 'i', 'n', 'g', 's'),\n",
       " ('h', 'i', 'n', 'g', 's', ']'),\n",
       " ('[', 't', 'h', 'i', 'n', 'g', 's'),\n",
       " ('t', 'h', 'i', 'n', 'g', 's', ']'),\n",
       " ('[', 't', 'h', 'i', 'n', 'g', 's', ']'),\n",
       " ('[', 'h', 'e', 'r'),\n",
       " ('w', 'n', ']'),\n",
       " ('c', 'k', ']'),\n",
       " ('k', 'i', 'n', 'g'),\n",
       " ('k', 'i', 'n', 'g', ']'),\n",
       " ('c', 'h', 'i'),\n",
       " ('i', 'c', 'e'),\n",
       " ('[', 'g', 'r'),\n",
       " ('i', 'c', 'a'),\n",
       " ('h', 'o', 'w', ']'),\n",
       " ('p', 'p'),\n",
       " ('m', 'o', 'r', 'e'),\n",
       " ('m', 'o', 'r', 'e', ']'),\n",
       " ('r', 't', ']'),\n",
       " ('o', 'w', 'n', ']'),\n",
       " ('[', 'v', 'e', 'r'),\n",
       " ('t', 'i', 'm', 'e', ']'),\n",
       " ('o', 's', 'e'),\n",
       " ('m', 'e', 't', 'h'),\n",
       " ('e', 't', 'h', 'i'),\n",
       " ('o', 'm', 'e', 't', 'h'),\n",
       " ('m', 'e', 't', 'h', 'i'),\n",
       " ('e', 't', 'h', 'i', 'n'),\n",
       " ('s', 'o', 'm', 'e', 't', 'h'),\n",
       " ('o', 'm', 'e', 't', 'h', 'i'),\n",
       " ('m', 'e', 't', 'h', 'i', 'n'),\n",
       " ('e', 't', 'h', 'i', 'n', 'g'),\n",
       " ('[', 's', 'o', 'm', 'e', 't', 'h'),\n",
       " ('s', 'o', 'm', 'e', 't', 'h', 'i'),\n",
       " ('o', 'm', 'e', 't', 'h', 'i', 'n'),\n",
       " ('m', 'e', 't', 'h', 'i', 'n', 'g'),\n",
       " ('e', 't', 'h', 'i', 'n', 'g', ']'),\n",
       " ('[', 's', 'o', 'm', 'e', 't', 'h', 'i'),\n",
       " ('s', 'o', 'm', 'e', 't', 'h', 'i', 'n'),\n",
       " ('o', 'm', 'e', 't', 'h', 'i', 'n', 'g'),\n",
       " ('m', 'e', 't', 'h', 'i', 'n', 'g', ']'),\n",
       " ('e', 'n', 'd'),\n",
       " ('o', 't', 'h', 'e', 'r', ']'),\n",
       " ('m', 'e', 'n'),\n",
       " ('y', 'o', 'u', 'r'),\n",
       " ('[', 'y', 'o', 'u', 'r'),\n",
       " ('d', 'e', 'r'),\n",
       " ('e', 'i', 'r'),\n",
       " ('[', 'm', 'i'),\n",
       " ('i', 'l', 'l'),\n",
       " ('w', 'a', 'y', ']'),\n",
       " ('[', 'm', 'o', 'r'),\n",
       " ('l', 't'),\n",
       " ('t', 'r', 'y'),\n",
       " ('a', 'i', 'n'),\n",
       " ('v', 'e', 'n'),\n",
       " ('t', 'e', 'n'),\n",
       " ('s', 'h', 'o'),\n",
       " ('c', 'u'),\n",
       " ('g', 'u', 'e'),\n",
       " ('[', 's', 'h', 'o'),\n",
       " ('[', 'p', 'u'),\n",
       " ('e', 'a', 't'),\n",
       " ('[', 't', 'i', 'm', 'e', ']'),\n",
       " ('u', 'n', 't'),\n",
       " ('[', 'k', 'i', 'n'),\n",
       " ('k', 'i', 'n', 'd'),\n",
       " ('[', 'k', 'i', 'n', 'd'),\n",
       " ('t', 'h', 'o', 'u'),\n",
       " ('h', 'e', 'i'),\n",
       " ('t', 'h', 'e', 'i'),\n",
       " ('h', 'e', 'i', 'r'),\n",
       " ('e', 'i', 'r', ']'),\n",
       " ('[', 't', 'h', 'e', 'i'),\n",
       " ('t', 'h', 'e', 'i', 'r'),\n",
       " ('h', 'e', 'i', 'r', ']'),\n",
       " ('[', 't', 'h', 'e', 'i', 'r'),\n",
       " ('t', 'h', 'e', 'i', 'r', ']'),\n",
       " ('[', 't', 'h', 'e', 'i', 'r', ']'),\n",
       " ('u', 'c', 'h'),\n",
       " ('f', 'r', 'o'),\n",
       " ('[', 'f', 'r', 'o'),\n",
       " ('m', 'b'),\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map common ngrams to index values for one hot encoding\n",
    "ngram_to_index = {}\n",
    "#ngram_to_index\n",
    "for index,ngram in enumerate(common_ngrams):\n",
    "    ngram_to_index[ngram] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_letter_ngram(word):\n",
    "    \n",
    "    n=10\n",
    "    word_list = list(word)\n",
    "    letter_ngram = np.zeros(len(common_ngrams))\n",
    "    \n",
    "    #Extract ngrams from the word\n",
    "    ngrams_list = []\n",
    "    \n",
    "    ngrams_list.append(list(filter(lambda x: x!=tuple('[') and x!= tuple(']'),list(ngrams(list(word),1)))))\n",
    "    for i in range(2,n+1):\n",
    "        ngrams_list.append(list(ngrams(list(word),i)))\n",
    "    \n",
    "    #Flatten\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    #Unroll the list\n",
    "    ngrams_list = flatten(ngrams_list)\n",
    "    \n",
    "    for ngram in ngrams_list:\n",
    "        if ngram in ngram_to_index.keys():\n",
    "            letter_ngram[ngram_to_index[ngram]] += 1\n",
    "        \n",
    "    return letter_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_letter_ngrams(words):\n",
    "    letter_ngrams = []\n",
    "    for word in words:\n",
    "        letter_ngrams.append(give_letter_ngram(word))\n",
    "    \n",
    "    return np.stack(letter_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_cos(x1,x2,cos):\n",
    "    \n",
    "    return (1-cos(x1,x2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(word_embedding,same_word_embedding,diff_word_embedding):\n",
    "    m = torch.tensor(1.0, dtype = torch.float).to(dev, non_blocking = True)\n",
    "    lower_bound = torch.tensor(0.0, dtype = torch.float).to(dev, non_blocking = True)\n",
    "    a = torch.max(lower_bound,m - cos(word_embedding ,same_word_embedding) + cos(word_embedding ,diff_word_embedding))\n",
    "\n",
    "    \n",
    "    return torch.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthographicNet(nn.Module):\n",
    "    def __init__(self,num_input,num_output):\n",
    "        super(OrthographicNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def give_embeddings(self,x,dev):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #print(x.shape)\n",
    "        #print(\"Done\")\n",
    "        return x.cpu().detach().numpy() if dev.type == 'cuda' else x.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input,num_output = 50000,9974\n",
    "orthographic_net = OrthographicNet(num_input,num_output)\n",
    "orthographic_net = orthographic_net.float()\n",
    "orthographic_net.to(dev)\n",
    "optimizer = optim.SGD(orthographic_net.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = torch.optim.Adadelta(orthographic_net.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "verbose = True\n",
    "model_save_path = \"./Models/best_orthographic_model2.pth\"\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "batch_limit = 10\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "    if verbose:\n",
    "            print('epoch %d '%(epoch))\n",
    "\n",
    "    train_loss = 0\n",
    "    orthographic_net.train()\n",
    "    for batch_idx, (train_data,train_labels) in enumerate(train_dl):\n",
    "\n",
    "        #print(train_data.shape)\n",
    "        #Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        train_data = train_data.to(dev, non_blocking=True)\n",
    "        #Get word mfcc features\n",
    "        word = train_data[:,0,:]\n",
    "        #Get labels\n",
    "        word_labels = [num_to_word[int(train_labels[i,0])] for i in range(train_labels.shape[0])]\n",
    "        diff_word_labels = [num_to_word[int(train_labels[i,1])] for i in range(train_labels.shape[0])]\n",
    "            \n",
    "            \n",
    "        #Get letter_ngrams\n",
    "        word_letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "        diff_letter_ngrams = torch.tensor(batch_letter_ngrams(diff_word_labels), dtype =torch.float,device = dev)\n",
    "\n",
    "        #Get the word embedding and letter_ngram embeddings\n",
    "        with torch.no_grad():\n",
    "            word_embedding = source_net(word)\n",
    "        \n",
    "        #word_embedding = np.stack([saved_word_embedding_dict.item().get(word).squeeze() for word in word_labels ])\n",
    "        #word_embedding = torch.tensor(word_embedding,dtype =torch.float, device = dev)\n",
    "        \n",
    "        word_ngram_embedding = orthographic_net(word_letter_ngrams)\n",
    "        diff_word_ngram_embedding = orthographic_net(diff_letter_ngrams)\n",
    "        \n",
    "        \n",
    "        #Calculate the triplet loss\n",
    "        \n",
    "        loss = triplet_loss(word_embedding,word_ngram_embedding,diff_word_ngram_embedding)\n",
    "        pdb.set_trace()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if batch_idx == batch_limit:\n",
    "            break\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    '''\n",
    "    orthographic_net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (val_data,val_labels) in enumerate(val_dl):\n",
    "\n",
    "            val_data = val_data.to(dev, non_blocking=True)\n",
    "            #Get word mfcc features\n",
    "            word = train_data[:,0,:]\n",
    "            #Get labels\n",
    "            word_labels = [num_to_word[int(val_labels[i,0])] for i in range(train_labels.shape[0])]\n",
    "            diff_word_labels = [num_to_word[int(val_labels[i,1])] for i in range(train_labels.shape[0])]\n",
    "            \n",
    "            #Get letter_ngrams\n",
    "            word_letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "            diff_letter_ngrams = torch.tensor(batch_letter_ngrams(diff_word_labels), dtype =torch.float,device = dev)\n",
    "\n",
    "            #Get the word embedding and letter_ngram embeddings\n",
    "            word_embedding = source_net(word)\n",
    "            \n",
    "            #word_embedding = np.stack([saved_word_embedding_dict.item().get(word).squeeze() for word in word_labels ])\n",
    "            #word_embedding = torch.tensor(word_embedding,dtype =torch.float, device = dev)\n",
    "            \n",
    "            word_ngram_embedding = orthographic_net(word_letter_ngrams)\n",
    "            diff_word_ngram_embedding = orthographic_net(diff_letter_ngrams)\n",
    "        \n",
    "            \n",
    "\n",
    "            #Calculate the triplet loss\n",
    "            val_loss += triplet_loss(word_embedding,word_ngram_embedding,diff_word_ngram_embedding)\n",
    "\n",
    "            if batch_idx == batch_limit:\n",
    "                break\n",
    "            \n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Best val loss %.3f Saving Model...\"%(val_loss/len(val_dl)))\n",
    "            torch.save(orthographic_net.state_dict(),model_save_path)\n",
    "        \n",
    "        '''\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"train loss: %.8f\"%(train_loss/len(train_dl)))\n",
    "        #print(\"val loss: %.5f\"%(val_loss/len(val_dl)))\n",
    "        \n",
    "    train_loss_list.append(train_loss/len(train_dl))\n",
    "    #val_loss_list.append(val_loss/len(val_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the learning curves\n",
    "\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(len(train_loss_list)),train_loss_list, label = 'train')\n",
    "plt.plot(range(len(train_loss_list)), val_loss_list, label = 'val')\n",
    "plt.legend()\n",
    "plt.savefig('orthographic_lc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best orthographic model\n",
    "model_save_path = \"./Models/best_orthographic_model2.pth\"\n",
    "orthographic_net.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_list = ['Data/feats_cmvn.ark']\n",
    "num_examples = np.Inf\n",
    "dh = DataHelper(load_list,num_examples)\n",
    "dh.load_data()\n",
    "dh.process_data()\n",
    "c,word_to_num,num_to_word = dh.generate_key_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save orthographic model word embeddings\n",
    "\n",
    "#Load Words\n",
    "words = list(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate word embedding dict\n",
    "word_embedding_dict = {}\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        word_features = give_letter_ngram(word)\n",
    "        #print(word_features)\n",
    "        word_embedding = orthographic_net(torch.tensor(word_features, dtype =torch.float, device = dev))\n",
    "\n",
    "        word_embedding_dict[word] = word_embedding.detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/orthographic_word_embedding_dict2.npy\",word_embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how similar are the word embeddings to the saved one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 173657\n",
      "Finished Loading the Data, 173657 examples\n",
      "Number of Unique words  9974\n"
     ]
    }
   ],
   "source": [
    "load_list = ['Data/feats_cmvn.ark']\n",
    "num_examples = np.Inf\n",
    "dh = DataHelper(load_list,num_examples)\n",
    "dh.load_data()\n",
    "dh.process_data()\n",
    "c,word_to_num_cl,num_to_word_cl = dh.generate_key_dicts()\n",
    "inputs,labels = dh.give_inputs_and_labels()\n",
    "del dh\n",
    "words = list(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_embedding_dict(words,net):\n",
    "    word_embedding_dict = OrderedDict()\n",
    "    #Calculate embeddings\n",
    "    for word in words:\n",
    "        #Find the mfcc features of the acoustic representation of the word in the data\n",
    "        word_features = inputs[np.where(np.isin(labels,word_to_num[word]))]\n",
    "        \n",
    "        #Calculate embeddings for the feature\n",
    "        word_embedding = net.give_embeddings(torch.tensor(word_features, device = dev, dtype=torch.float),dev)\n",
    "        \n",
    "        #If the number of representation is more than one, take the average embedding\n",
    "        word_embedding_dict[word] = np.mean(word_embedding, axis = 0).reshape(1,-1)\n",
    "    \n",
    "    return word_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = generate_word_embedding_dict(words,source_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the word embeddings\n",
    "saved_word_embedding_dict = np.load('Data/word_embedding_dict.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95097256\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for word in words:\n",
    "    calc_em = word_embedding_dict[word].reshape(1,-1)\n",
    "    saved_em = saved_word_embedding_dict.item().get(word).squeeze().reshape(1,-1)\n",
    "    a = pairwise_kernels(calc_em,saved_em, metric = \"cosine\")\n",
    "    #print(word,a)\n",
    "    d.append(a)\n",
    "print(np.mean(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the siamese dataset is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    }
   ],
   "source": [
    "#Check if the words are mapped to same mfcc or not\n",
    "matches = 0\n",
    "total_data = 0\n",
    "word_index = 1\n",
    "#Loop through training examples\n",
    "for batch_idx, (train_data,train_labels) in enumerate(train_dl):\n",
    "\n",
    "    #Loop through individual examples\n",
    "    for i in range(train_data.shape[0]):\n",
    "        \n",
    "        #Extract a word and it's mfcc\n",
    "        mfcc_index = 0 if word_index ==0 else 2\n",
    "        mfcc = train_data[i,mfcc_index]\n",
    "        label = train_labels[i,word_index]\n",
    "        word = num_to_word[int(label.numpy())]\n",
    "        \n",
    "        \n",
    "        #Get all mfccs for this word from the classic dataloader\n",
    "        label_cl_num = word_to_num_cl[word]\n",
    "        #print(label_cl_num)\n",
    "        ids = np.where(np.isin(labels,label_cl_num))\n",
    "        \n",
    "        #mfccs\n",
    "        mfccs = inputs[ids]\n",
    "        \n",
    "        for j in range(mfccs.shape[0]):\n",
    "            if np.array_equal(mfcc,mfccs[j]):\n",
    "                matches+=1\n",
    "    \n",
    "    total_data += train_data.shape[0]\n",
    "    break\n",
    "    \n",
    "print(total_data,matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if n-gram vector is rich enough to predict words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_one_hot(words):\n",
    "    \n",
    "    one_hot = np.zeros((len(words),len(word_to_num.keys())))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        one_hot[i,word_to_num[word]] = 1\n",
    "    \n",
    "    return one_hot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramNet(nn.Module):\n",
    "    def __init__(self,num_input,num_output):\n",
    "        super(NgramNet, self).__init__()\n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(num_input, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #print(x.shape)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_helpers import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 173657\n",
      "Finished Loading the Data, 173657 examples\n",
      "Number of Unique words  9974\n",
      "Length before filtering on char length 317927\n",
      "Length after filtering on char length 173657\n",
      "Length before filtering on frequency_bounds 173657\n",
      "Length after filtering on frequency_bounds 173657\n",
      "Finished Loading the Data, 173657 examples\n",
      "Number of Unique words  9974\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "num_examples = np.Inf\n",
    "train_ds = AMI_dataset(num_examples = num_examples, split_set = \"train\", data_filepath = \"Data/feats_cmvn.ark\", char_threshold = 5, frequency_bounds = (0,np.Inf))\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, pin_memory = True, shuffle = True, drop_last = True)\n",
    "\n",
    "val_ds = AMI_dataset(num_examples = num_examples, split_set = \"val\", data_filepath = \"Data/feats_cmvn.ark\", char_threshold = 5, frequency_bounds = (0,np.Inf))\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, pin_memory = True, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_word,word_to_num = train_ds.num_to_word,train_ds.word_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_to_num.keys())\n",
    "ngram_net = NgramNet(num_ngrams,num_words)\n",
    "ngram_net = ngram_net.float()\n",
    "ngram_net = ngram_net.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining training criterion\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngram_net.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \n",
      "Best val loss 9.183 Saving Model...\n",
      "train loss: 9.196 train acc: 0.018\n",
      "val loss: 9.183 val acc: 0.020\n",
      "epoch 1 \n",
      "Best val loss 9.134 Saving Model...\n",
      "train loss: 9.163 train acc: 0.027\n",
      "val loss: 9.134 val acc: 0.026\n",
      "epoch 2 \n",
      "Best val loss 8.708 Saving Model...\n",
      "train loss: 9.016 train acc: 0.015\n",
      "val loss: 8.708 val acc: 0.016\n",
      "epoch 3 \n",
      "Best val loss 8.110 Saving Model...\n",
      "train loss: 8.341 train acc: 0.039\n",
      "val loss: 8.110 val acc: 0.039\n",
      "epoch 4 \n",
      "Best val loss 7.843 Saving Model...\n",
      "train loss: 7.980 train acc: 0.061\n",
      "val loss: 7.843 val acc: 0.078\n",
      "epoch 5 \n",
      "Best val loss 7.561 Saving Model...\n",
      "train loss: 7.712 train acc: 0.091\n",
      "val loss: 7.561 val acc: 0.121\n",
      "epoch 6 \n",
      "Best val loss 7.219 Saving Model...\n",
      "train loss: 7.401 train acc: 0.141\n",
      "val loss: 7.219 val acc: 0.173\n",
      "epoch 7 \n",
      "Best val loss 6.820 Saving Model...\n",
      "train loss: 7.022 train acc: 0.192\n",
      "val loss: 6.820 val acc: 0.212\n",
      "epoch 8 \n",
      "Best val loss 6.374 Saving Model...\n",
      "train loss: 6.596 train acc: 0.242\n",
      "val loss: 6.374 val acc: 0.280\n",
      "epoch 9 \n",
      "Best val loss 5.945 Saving Model...\n",
      "train loss: 6.148 train acc: 0.302\n",
      "val loss: 5.945 val acc: 0.326\n",
      "epoch 10 \n",
      "Best val loss 5.587 Saving Model...\n",
      "train loss: 5.750 train acc: 0.357\n",
      "val loss: 5.587 val acc: 0.379\n",
      "epoch 11 \n",
      "Best val loss 5.286 Saving Model...\n",
      "train loss: 5.416 train acc: 0.397\n",
      "val loss: 5.286 val acc: 0.405\n",
      "epoch 12 \n",
      "Best val loss 5.028 Saving Model...\n",
      "train loss: 5.133 train acc: 0.430\n",
      "val loss: 5.028 val acc: 0.446\n",
      "epoch 13 \n",
      "Best val loss 4.813 Saving Model...\n",
      "train loss: 4.894 train acc: 0.460\n",
      "val loss: 4.813 val acc: 0.472\n",
      "epoch 14 \n",
      "Best val loss 4.632 Saving Model...\n",
      "train loss: 4.694 train acc: 0.486\n",
      "val loss: 4.632 val acc: 0.498\n",
      "epoch 15 \n",
      "Best val loss 4.483 Saving Model...\n",
      "train loss: 4.529 train acc: 0.508\n",
      "val loss: 4.483 val acc: 0.517\n",
      "epoch 16 \n",
      "Best val loss 4.358 Saving Model...\n",
      "train loss: 4.389 train acc: 0.527\n",
      "val loss: 4.358 val acc: 0.537\n",
      "epoch 17 \n",
      "Best val loss 4.253 Saving Model...\n",
      "train loss: 4.273 train acc: 0.544\n",
      "val loss: 4.253 val acc: 0.549\n",
      "epoch 18 \n",
      "Best val loss 4.161 Saving Model...\n",
      "train loss: 4.174 train acc: 0.556\n",
      "val loss: 4.161 val acc: 0.562\n",
      "epoch 19 \n",
      "Best val loss 4.088 Saving Model...\n",
      "train loss: 4.091 train acc: 0.567\n",
      "val loss: 4.088 val acc: 0.569\n",
      "epoch 20 \n",
      "Best val loss 4.027 Saving Model...\n",
      "train loss: 4.020 train acc: 0.574\n",
      "val loss: 4.027 val acc: 0.576\n",
      "epoch 21 \n",
      "Best val loss 3.970 Saving Model...\n",
      "train loss: 3.959 train acc: 0.582\n",
      "val loss: 3.970 val acc: 0.584\n",
      "epoch 22 \n",
      "Best val loss 3.921 Saving Model...\n",
      "train loss: 3.907 train acc: 0.588\n",
      "val loss: 3.921 val acc: 0.589\n",
      "epoch 23 \n",
      "Best val loss 3.881 Saving Model...\n",
      "train loss: 3.860 train acc: 0.592\n",
      "val loss: 3.881 val acc: 0.592\n",
      "epoch 24 \n",
      "Best val loss 3.843 Saving Model...\n",
      "train loss: 3.820 train acc: 0.597\n",
      "val loss: 3.843 val acc: 0.597\n",
      "epoch 25 \n",
      "Best val loss 3.813 Saving Model...\n",
      "train loss: 3.784 train acc: 0.601\n",
      "val loss: 3.813 val acc: 0.600\n",
      "epoch 26 \n",
      "Best val loss 3.786 Saving Model...\n",
      "train loss: 3.752 train acc: 0.604\n",
      "val loss: 3.786 val acc: 0.603\n",
      "epoch 27 \n",
      "Best val loss 3.760 Saving Model...\n",
      "train loss: 3.724 train acc: 0.607\n",
      "val loss: 3.760 val acc: 0.605\n",
      "epoch 28 \n",
      "Best val loss 3.741 Saving Model...\n",
      "train loss: 3.699 train acc: 0.609\n",
      "val loss: 3.741 val acc: 0.607\n",
      "epoch 29 \n",
      "Best val loss 3.714 Saving Model...\n",
      "train loss: 3.675 train acc: 0.612\n",
      "val loss: 3.714 val acc: 0.611\n",
      "epoch 30 \n",
      "Best val loss 3.698 Saving Model...\n",
      "train loss: 3.655 train acc: 0.613\n",
      "val loss: 3.698 val acc: 0.611\n",
      "epoch 31 \n",
      "Best val loss 3.679 Saving Model...\n",
      "train loss: 3.636 train acc: 0.616\n",
      "val loss: 3.679 val acc: 0.612\n",
      "epoch 32 \n",
      "Best val loss 3.664 Saving Model...\n",
      "train loss: 3.619 train acc: 0.617\n",
      "val loss: 3.664 val acc: 0.614\n",
      "epoch 33 \n",
      "Best val loss 3.651 Saving Model...\n",
      "train loss: 3.603 train acc: 0.618\n",
      "val loss: 3.651 val acc: 0.615\n",
      "epoch 34 \n",
      "Best val loss 3.637 Saving Model...\n",
      "train loss: 3.589 train acc: 0.620\n",
      "val loss: 3.637 val acc: 0.616\n",
      "epoch 35 \n",
      "Best val loss 3.623 Saving Model...\n",
      "train loss: 3.575 train acc: 0.621\n",
      "val loss: 3.623 val acc: 0.618\n",
      "epoch 36 \n",
      "Best val loss 3.615 Saving Model...\n",
      "train loss: 3.563 train acc: 0.622\n",
      "val loss: 3.615 val acc: 0.619\n",
      "epoch 37 \n",
      "Best val loss 3.605 Saving Model...\n",
      "train loss: 3.552 train acc: 0.623\n",
      "val loss: 3.605 val acc: 0.618\n",
      "epoch 38 \n",
      "Best val loss 3.595 Saving Model...\n",
      "train loss: 3.541 train acc: 0.624\n",
      "val loss: 3.595 val acc: 0.620\n",
      "epoch 39 \n",
      "Best val loss 3.589 Saving Model...\n",
      "train loss: 3.531 train acc: 0.625\n",
      "val loss: 3.589 val acc: 0.621\n",
      "epoch 40 \n",
      "Best val loss 3.576 Saving Model...\n",
      "train loss: 3.522 train acc: 0.625\n",
      "val loss: 3.576 val acc: 0.622\n",
      "epoch 41 \n",
      "Best val loss 3.570 Saving Model...\n",
      "train loss: 3.513 train acc: 0.626\n",
      "val loss: 3.570 val acc: 0.622\n",
      "epoch 42 \n",
      "Best val loss 3.565 Saving Model...\n",
      "train loss: 3.506 train acc: 0.626\n",
      "val loss: 3.565 val acc: 0.622\n",
      "epoch 43 \n",
      "Best val loss 3.556 Saving Model...\n",
      "train loss: 3.498 train acc: 0.627\n",
      "val loss: 3.556 val acc: 0.623\n",
      "epoch 44 \n",
      "Best val loss 3.556 Saving Model...\n",
      "train loss: 3.492 train acc: 0.628\n",
      "val loss: 3.556 val acc: 0.624\n",
      "epoch 45 \n",
      "Best val loss 3.547 Saving Model...\n",
      "train loss: 3.485 train acc: 0.628\n",
      "val loss: 3.547 val acc: 0.624\n",
      "epoch 46 \n",
      "Best val loss 3.543 Saving Model...\n",
      "train loss: 3.479 train acc: 0.629\n",
      "val loss: 3.543 val acc: 0.623\n",
      "epoch 47 \n",
      "Best val loss 3.536 Saving Model...\n",
      "train loss: 3.473 train acc: 0.629\n",
      "val loss: 3.536 val acc: 0.625\n",
      "epoch 48 \n",
      "Best val loss 3.531 Saving Model...\n",
      "train loss: 3.468 train acc: 0.630\n",
      "val loss: 3.531 val acc: 0.625\n",
      "epoch 49 \n",
      "Best val loss 3.525 Saving Model...\n",
      "train loss: 3.462 train acc: 0.630\n",
      "val loss: 3.525 val acc: 0.626\n",
      "epoch 50 \n",
      "Best val loss 3.523 Saving Model...\n",
      "train loss: 3.458 train acc: 0.630\n",
      "val loss: 3.523 val acc: 0.626\n",
      "epoch 51 \n",
      "Best val loss 3.519 Saving Model...\n",
      "train loss: 3.453 train acc: 0.631\n",
      "val loss: 3.519 val acc: 0.627\n",
      "epoch 52 \n",
      "Best val loss 3.517 Saving Model...\n",
      "train loss: 3.448 train acc: 0.631\n",
      "val loss: 3.517 val acc: 0.626\n",
      "epoch 53 \n",
      "Best val loss 3.511 Saving Model...\n",
      "train loss: 3.444 train acc: 0.631\n",
      "val loss: 3.511 val acc: 0.627\n",
      "epoch 54 \n",
      "Best val loss 3.508 Saving Model...\n",
      "train loss: 3.440 train acc: 0.632\n",
      "val loss: 3.508 val acc: 0.627\n",
      "epoch 55 \n",
      "Best val loss 3.504 Saving Model...\n",
      "train loss: 3.437 train acc: 0.632\n",
      "val loss: 3.504 val acc: 0.627\n",
      "epoch 56 \n",
      "train loss: 3.433 train acc: 0.632\n",
      "val loss: 3.505 val acc: 0.627\n",
      "epoch 57 \n",
      "Best val loss 3.498 Saving Model...\n",
      "train loss: 3.430 train acc: 0.632\n",
      "val loss: 3.498 val acc: 0.628\n",
      "epoch 58 \n",
      "train loss: 3.426 train acc: 0.633\n",
      "val loss: 3.500 val acc: 0.627\n",
      "epoch 59 \n",
      "Best val loss 3.494 Saving Model...\n",
      "train loss: 3.424 train acc: 0.633\n",
      "val loss: 3.494 val acc: 0.628\n",
      "epoch 60 \n",
      "Best val loss 3.492 Saving Model...\n",
      "train loss: 3.421 train acc: 0.633\n",
      "val loss: 3.492 val acc: 0.628\n",
      "epoch 61 \n",
      "Best val loss 3.488 Saving Model...\n",
      "train loss: 3.418 train acc: 0.634\n",
      "val loss: 3.488 val acc: 0.628\n",
      "epoch 62 \n",
      "Best val loss 3.487 Saving Model...\n",
      "train loss: 3.415 train acc: 0.634\n",
      "val loss: 3.487 val acc: 0.628\n",
      "epoch 63 \n",
      "Best val loss 3.483 Saving Model...\n",
      "train loss: 3.413 train acc: 0.634\n",
      "val loss: 3.483 val acc: 0.628\n",
      "epoch 64 \n",
      "Best val loss 3.481 Saving Model...\n",
      "train loss: 3.410 train acc: 0.634\n",
      "val loss: 3.481 val acc: 0.629\n",
      "epoch 65 \n",
      "Best val loss 3.480 Saving Model...\n",
      "train loss: 3.407 train acc: 0.634\n",
      "val loss: 3.480 val acc: 0.629\n",
      "epoch 66 \n",
      "train loss: 3.406 train acc: 0.635\n",
      "val loss: 3.481 val acc: 0.629\n",
      "epoch 67 \n",
      "Best val loss 3.476 Saving Model...\n",
      "train loss: 3.403 train acc: 0.635\n",
      "val loss: 3.476 val acc: 0.628\n",
      "epoch 68 \n",
      "Best val loss 3.476 Saving Model...\n",
      "train loss: 3.401 train acc: 0.635\n",
      "val loss: 3.476 val acc: 0.629\n",
      "epoch 69 \n",
      "Best val loss 3.474 Saving Model...\n",
      "train loss: 3.399 train acc: 0.635\n",
      "val loss: 3.474 val acc: 0.629\n",
      "epoch 70 \n",
      "Best val loss 3.472 Saving Model...\n",
      "train loss: 3.397 train acc: 0.635\n",
      "val loss: 3.472 val acc: 0.629\n",
      "epoch 71 \n",
      "Best val loss 3.472 Saving Model...\n",
      "train loss: 3.396 train acc: 0.635\n",
      "val loss: 3.472 val acc: 0.629\n",
      "epoch 72 \n",
      "Best val loss 3.471 Saving Model...\n",
      "train loss: 3.394 train acc: 0.635\n",
      "val loss: 3.471 val acc: 0.629\n",
      "epoch 73 \n",
      "Best val loss 3.468 Saving Model...\n",
      "train loss: 3.392 train acc: 0.636\n",
      "val loss: 3.468 val acc: 0.629\n",
      "epoch 74 \n",
      "Best val loss 3.465 Saving Model...\n",
      "train loss: 3.391 train acc: 0.636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 3.465 val acc: 0.629\n",
      "epoch 75 \n",
      "Best val loss 3.464 Saving Model...\n",
      "train loss: 3.389 train acc: 0.636\n",
      "val loss: 3.464 val acc: 0.630\n",
      "epoch 76 \n"
     ]
    }
   ],
   "source": [
    "#Loop through words\n",
    "num_epochs = 100\n",
    "verbose = True\n",
    "model_save_path = \"./Models/best_ngram_model.pth\"\n",
    "best_val_loss = np.Inf\n",
    "\n",
    "batch_limit = np.Inf\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "    if verbose:\n",
    "            print('epoch %d '%(epoch))\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    ngram_net.train()\n",
    "    for batch_idx, (train_data,train_labels) in enumerate(train_dl):\n",
    "\n",
    "        #print(train_data.shape)\n",
    "        #Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        #Get labels\n",
    "        word_labels = [num_to_word[int(train_labels[i])] for i in range(train_labels.shape[0])]\n",
    "        \n",
    "        \n",
    "        #Get letter_ngrams\n",
    "        letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "\n",
    "        \n",
    "        #Get labels as one hot\n",
    "        labels = train_labels.to(dev)\n",
    "        \n",
    "        #print(letter_ngrams.shape)\n",
    "        #print(labels)\n",
    "        \n",
    "        #Predict words using the model\n",
    "        predicted_labels = ngram_net(letter_ngrams)\n",
    "        \n",
    "        #Calculate loss\n",
    "        loss = criterion(predicted_labels,labels.long())\n",
    "        loss.backward()\n",
    "        plot_grad_flow(ngram_net.named_parameters())\n",
    "        \n",
    "        #for n,p in ngram_net.named_parameters():\n",
    "        #    print(n)\n",
    "        #    #print(p)\n",
    "        #    print(p.grad.abs().mean())\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(predicted_labels,labels)\n",
    "\n",
    "        if batch_idx == batch_limit:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    ngram_net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        for batch_idx, (val_data,val_labels) in enumerate(val_dl):\n",
    "            \n",
    "\n",
    "\n",
    "            #Get labels\n",
    "            word_labels = [num_to_word[int(val_labels[i])] for i in range(train_labels.shape[0])]\n",
    "            \n",
    "            \n",
    "            #Get letter_ngrams\n",
    "            letter_ngrams = torch.tensor(batch_letter_ngrams(word_labels), dtype =torch.float, device = dev)\n",
    "\n",
    "            \n",
    "            #Get labels as one hot\n",
    "            labels = val_labels.to(dev)\n",
    "            \n",
    "            #Predict words using the model\n",
    "            predicted_labels = ngram_net(letter_ngrams)\n",
    "            \n",
    "            #Calculate loss\n",
    "            batch_val_loss = criterion(predicted_labels,labels.long())\n",
    "        \n",
    "            \n",
    "\n",
    "            #Calculate the triplet loss\n",
    "            val_loss += batch_val_loss.item()\n",
    "            val_acc += accuracy(predicted_labels,labels)\n",
    "\n",
    "            \n",
    "            if batch_idx == batch_limit:\n",
    "                break\n",
    "            \n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Best val loss %.3f Saving Model...\"%(val_loss/len(val_dl)))\n",
    "            torch.save(ngram_net.state_dict(),model_save_path)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"train loss: %.3f train acc: %.3f\"%(train_loss/len(train_dl),train_acc/len(train_dl)))\n",
    "        print(\"val loss: %.3f val acc: %.3f\"%(val_loss/len(val_dl),val_acc/len(val_dl)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss_list.append(train_loss/len(train_dl))\n",
    "    train_acc_list.append(train_acc/len(train_dl))\n",
    "    val_loss_list.append(val_loss/len(val_dl))\n",
    "    val_acc_list.append(val_acc/len(val_dl))\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
