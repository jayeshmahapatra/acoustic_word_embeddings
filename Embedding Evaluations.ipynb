{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Core Python, Pandas, and kaldi_io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter,OrderedDict \n",
    "import kaldi_io\n",
    "from datetime import datetime\n",
    "\n",
    "#Scikit\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances,average_precision_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels,paired_distances\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#BigPhoney\n",
    "from big_phoney import BigPhoney\n",
    "\n",
    "\n",
    "#Torch and utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader,random_split,ConcatDataset\n",
    "\n",
    "#Import User defined classes\n",
    "from data_helpers import DataHelper\n",
    "from models import SimpleNet\n",
    "from train_test_helpers import accuracy,train_model,evaluate_model,evaluate_model_paper,test_model,plot_learning_curves\n",
    "from sfba4.utils import alignSequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_list = ['Data/feats_cmvn.ark']\n",
    "#number_list = [9,12,14,18,21,25,27,28]\n",
    "#load_list = ['Data/raw_mfcc_AMI_Segments.%d.scp'%(number) for number in number_list]\n",
    "num_examples = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHelper(load_list,num_examples)\n",
    "dh.load_data()\n",
    "dh.process_data()\n",
    "c,word_to_num,num_to_word = dh.generate_key_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = dh.give_inputs_and_labels()\n",
    "del dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = False\n",
    "if split:\n",
    "    x_trainval,x_test,y_trainval,y_test = train_test_split(inputs, labels, test_size=0.2, random_state=32)\n",
    "    x_train,x_val,y_train,y_val = train_test_split(x_trainval,y_trainval,test_size =0.25, random_state = 32)\n",
    "    x_train,y_train = torch.tensor(x_train,dtype= torch.float),torch.tensor(y_train, dtype= torch.float)\n",
    "    x_val,y_val = torch.tensor(x_val, dtype= torch.float),torch.tensor(y_val, dtype= torch.float)\n",
    "    x_test,y_test = torch.tensor(x_test, dtype= torch.float),torch.tensor(y_test, dtype= torch.float)\n",
    "    print(x_train.shape,y_train.shape)\n",
    "    print(x_val.shape,y_val.shape)\n",
    "    print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = SimpleNet()\n",
    "num_output = len(c.keys())\n",
    "net = SimpleNet(num_output)\n",
    "net = net.float()\n",
    "net.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best model\n",
    "best_model_path = \"./Models/awe_best_model.pth\"\n",
    "net.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_time():\n",
    "\tnow = datetime.now()\n",
    "\tcurrent_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\tprint(\"Current Date and Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the word_pairs DataFrame\n",
    "wordpairs_df = pd.read_csv('Data/wordpairs_test.txt', sep = ',')\n",
    "#del wordpairs_df[\"raw_phonetic_edit_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>orthographic_edit_distance</th>\n",
       "      <th>phonetic_edit_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>share</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complexity</td>\n",
       "      <td>simple</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chance</td>\n",
       "      <td>probably</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covered</td>\n",
       "      <td>sheet</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shifting</td>\n",
       "      <td>topics</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547915</th>\n",
       "      <td>pilots</td>\n",
       "      <td>producing</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547916</th>\n",
       "      <td>devices</td>\n",
       "      <td>forcing</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547917</th>\n",
       "      <td>group</td>\n",
       "      <td>smartboards</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547918</th>\n",
       "      <td>agreed</td>\n",
       "      <td>compress</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547919</th>\n",
       "      <td>display</td>\n",
       "      <td>handy</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1547920 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_1       word_2  orthographic_edit_distance  \\\n",
       "0         marketing        share                           7   \n",
       "1        complexity       simple                           6   \n",
       "2            chance     probably                           7   \n",
       "3           covered        sheet                           5   \n",
       "4          shifting       topics                           7   \n",
       "...             ...          ...                         ...   \n",
       "1547915      pilots    producing                           8   \n",
       "1547916     devices      forcing                           7   \n",
       "1547917       group  smartboards                           9   \n",
       "1547918      agreed     compress                           6   \n",
       "1547919     display        handy                           6   \n",
       "\n",
       "         phonetic_edit_distance  \n",
       "0                             7  \n",
       "1                             8  \n",
       "2                             8  \n",
       "3                             5  \n",
       "4                             6  \n",
       "...                         ...  \n",
       "1547915                       6  \n",
       "1547916                       6  \n",
       "1547917                       9  \n",
       "1547918                       6  \n",
       "1547919                       6  \n",
       "\n",
       "[1547920 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.670146326746115"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1547920/331450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248.676755"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(49735351/100000)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.133333333333334"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "248/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = np.load('Data/word_embedding_dict.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date and Time = 20/08/2020 03:40:15\n",
      "100000\n",
      "Current Date and Time = 20/08/2020 03:40:41\n",
      "200000\n",
      "Current Date and Time = 20/08/2020 03:41:08\n",
      "300000\n",
      "Current Date and Time = 20/08/2020 03:41:33\n",
      "400000\n",
      "Current Date and Time = 20/08/2020 03:41:59\n",
      "500000\n",
      "Current Date and Time = 20/08/2020 03:42:25\n",
      "600000\n",
      "Current Date and Time = 20/08/2020 03:42:51\n",
      "700000\n",
      "Current Date and Time = 20/08/2020 03:43:17\n",
      "800000\n",
      "Current Date and Time = 20/08/2020 03:43:43\n",
      "900000\n",
      "Current Date and Time = 20/08/2020 03:44:09\n",
      "1000000\n",
      "Current Date and Time = 20/08/2020 03:44:35\n",
      "1100000\n",
      "Current Date and Time = 20/08/2020 03:45:01\n",
      "1200000\n",
      "Current Date and Time = 20/08/2020 03:45:28\n",
      "1300000\n",
      "Current Date and Time = 20/08/2020 03:45:54\n",
      "1400000\n",
      "Current Date and Time = 20/08/2020 03:46:20\n",
      "1500000\n"
     ]
    }
   ],
   "source": [
    "wordpairs = list(zip(wordpairs_df[\"word_1\"].to_list(),wordpairs_df[\"word_2\"].to_list()))\n",
    "metric = \"cosine\"\n",
    "\n",
    "embedding_similarity_dict = {}\n",
    "embedding_similarity_dict[\"word_1\"] = []\n",
    "embedding_similarity_dict[\"word_2\"] = []\n",
    "embedding_similarity_dict[\"%s_similarity\"%(metric)] = []\n",
    "\n",
    "\n",
    "for i,wordpair in enumerate(wordpairs):\n",
    "    \n",
    "    word_1_embedding = word_embedding_dict.item().get(wordpair[0]).squeeze().reshape(1,-1)\n",
    "    word_2_embedding = word_embedding_dict.item().get(wordpair[1]).squeeze().reshape(1,-1)\n",
    "    similarity = pairwise_kernels(word_1_embedding,word_2_embedding, metric = metric )\n",
    "    \n",
    "    embedding_similarity_dict[\"word_1\"].append(wordpair[0])\n",
    "    embedding_similarity_dict[\"word_2\"].append(wordpair[1])\n",
    "    embedding_similarity_dict[\"%s_similarity\"%(metric)].append(similarity.item())\n",
    "    \n",
    "    if ((i+1)%100000 == 0):\n",
    "        show_time()\n",
    "        print(i+1)\n",
    "\n",
    "embedding_similarity_df = pd.DataFrame(embedding_similarity_dict)\n",
    "embedding_similarity_df.to_csv(\"Data/test_embedding_similarity.txt\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>emitting</td>\n",
       "      <td>technicalwise</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development</td>\n",
       "      <td>skins</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>arrow</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>fabulous</td>\n",
       "      <td>reminder</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547896</th>\n",
       "      <td>occurs</td>\n",
       "      <td>visavis</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547906</th>\n",
       "      <td>proper</td>\n",
       "      <td>synchronise</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547912</th>\n",
       "      <td>becau</td>\n",
       "      <td>multiselect</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547915</th>\n",
       "      <td>pilots</td>\n",
       "      <td>producing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547916</th>\n",
       "      <td>devices</td>\n",
       "      <td>forcing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331450 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word_1         word_2  cosine_similarity\n",
       "6           emitting  technicalwise                0.0\n",
       "11       development          skins                0.0\n",
       "24             arrow          wheat                0.0\n",
       "30         knowledge        tuesday                0.0\n",
       "35          fabulous       reminder                0.0\n",
       "...              ...            ...                ...\n",
       "1547896       occurs        visavis                0.0\n",
       "1547906       proper    synchronise                0.0\n",
       "1547912        becau    multiselect                0.0\n",
       "1547915       pilots      producing                0.0\n",
       "1547916      devices        forcing                0.0\n",
       "\n",
       "[331450 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_df[embedding_similarity_df[\"cosine_similarity\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "wordpair = (\"pilots\",\"producing\")\n",
    "word_1_embedding = word_embedding_dict.item().get(wordpair[0]).squeeze().reshape(1,-1)\n",
    "word_2_embedding = word_embedding_dict.item().get(wordpair[1]).squeeze().reshape(1,-1)\n",
    "similarity = pairwise_kernels(word_1_embedding,word_2_embedding, metric = metric )\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate all the unique words\n",
    "def words_from_dataframe(dataframe):\n",
    "    wordpairs_list = dataframe[\"word_pairs\"].apply(lambda x: x.strip('()').split(','))\n",
    "    words = [word.strip(' \\'') for wordpair in wordpairs_list for word in wordpair]\n",
    "    words = set(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_embedding_dict(words):\n",
    "    word_embedding_dict = OrderedDict()\n",
    "    #Calculate embeddings\n",
    "    for word in words:\n",
    "        #Find the mfcc features of the acoustic representation of the word in the data\n",
    "        word_features = inputs[np.where(np.isin(labels,word_to_num[word]))]\n",
    "        \n",
    "        #Calculate embeddings for the feature\n",
    "        word_embedding = net.give_embeddings(torch.tensor(word_features, device = dev, dtype=torch.float),dev)\n",
    "        \n",
    "        #If the number of representation is more than one, take the average embedding\n",
    "        word_embedding_dict[word] = np.mean(word_embedding, axis = 0).reshape(1,-1)\n",
    "    \n",
    "    return word_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_distance(homophone_df,word_embedding_dict,metrics = ['cosine']):\n",
    "\n",
    "    word1_embeddings = None\n",
    "    word2_embeddings = None\n",
    "    \n",
    "    metric_distance_dict = {}\n",
    "    for metric in metrics:\n",
    "        metric_distance_dict[metric] = []\n",
    "        \n",
    "    for row in homophone_df.itertuples():\n",
    "        word1, word2 = map(lambda x: x.strip(' \\''),row.word_pairs.strip('()').split(','))\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric_distance_dict[metric].append(paired_distances(word_embedding_dict[word1],word_embedding_dict[word2], metric = metric)[0])\n",
    "        \n",
    "        \n",
    "        #if word1_embeddings is None and word2_embeddings is None:\n",
    "        #    word1_embeddings = word_embedding_dict[word1]\n",
    "        #    word2_embeddings = word_embedding_dict[word2]\n",
    "        #else:\n",
    "        #    word1_embeddings = np.vstack((word1_embeddings, word_embedding_dict[word1]))\n",
    "        #    word2_embeddings = np.vstack((word2_embeddings, word_embedding_dict[word2]))\n",
    "            \n",
    "        \n",
    "\n",
    "    #Calculate the distance\n",
    "    #print(word1_embeddings.shape)\n",
    "    for metric in metrics:\n",
    "        #metric_distance = paired_distances(word1_embeddings,word2_embeddings, metric = metric)\n",
    "        homophone_df.insert(len(homophone_df.columns),\"%s_distance\"%(metric), metric_distance_dict[metric], True)\n",
    "    \n",
    "    return homophone_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_nearest_neighbours_on_embeddings(word_embedding_dict, n_neighbours = 10, metric = 'cosine', split = False):\n",
    "    \n",
    "    embeddings = None\n",
    "    \n",
    "    embeddings = np.stack(list(word_embedding_dict.values())).squeeze()\n",
    "    \n",
    "    print('Calculating Nearest Neighbours')\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbours, algorithm='brute',metric = metric, n_jobs = 4).fit(embeddings)\n",
    "    distances,indices = nbrs.kneighbors(embeddings)\n",
    "    \n",
    "    columns = [\"word\",\"neighbours\"]\n",
    "    #nearest_neighbours_df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    words = list(word_embedding_dict.keys())\n",
    "    print('num of words %d'%(len(words)))\n",
    "    \n",
    "    \n",
    "    nearest_neighbours_df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    for i,word in enumerate(word_embedding_dict.keys()):\n",
    "        \n",
    "        neighbours = ','.join([words[indices[i,j]] for j in range(indices.shape[1]) if words[indices[i,j]]!= word])\n",
    "        #print(neighbours)\n",
    "        row = pd.DataFrame(np.array([[word],[neighbours]]).T, columns = columns)\n",
    "        nearest_neighbours_df = nearest_neighbours_df.append(row)\n",
    "        \n",
    "    \n",
    "    #pd.concat([pd.DataFrame(np.array([[word],[','.join([words[indices[i,j]] for j in range(indices.shape[1]) if words[indices[i,j]]!=word ])]]).T, columns = columns) for i,word in enumerate(word_embedding_dict.keys())])\n",
    "    \n",
    "    if split:\n",
    "        neighbour_col_names = [\"neighbour_%d\"%(i) for i in range(n_neighbours)]\n",
    "        nearest_neighbours_df[neighbour_col_names] = nearest_neighbours_df.neighbours.str.split(',', expand = True )\n",
    "        nearest_neighbours_df.drop(columns = [\"neighbours\"],inplace = True)\n",
    "    \n",
    "    \n",
    "    #Reset index\n",
    "    nearest_neighbours_df = nearest_neighbours_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return nearest_neighbours_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = generate_word_embedding_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = generate_word_embedding_dict(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/word_embedding_dict.npy\",word_embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_nearest_neighbours = give_nearest_neighbours_on_embeddings(word_embedding_dict, 10,'cosine', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_embedding_distance(wordpairs_df,word_embedding_dict,metrics = ['cosine', 'euclidean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[0]),\n",
    "    #hue=\"Word\",\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[1]),\n",
    "    #hue=\"Word\",\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('phonetic_edit_distance', as_index = False).agg(['mean', 'count', 'std'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[0]),\n",
    "    #hue=\"Word\",\n",
    "    data=df.groupby('phonetic_edit_distance', as_index = False).mean(),\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "plt.ylabel('average cosine distance')\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[1]),\n",
    "    #hue=\"Word\",\n",
    "    data=df.groupby('phonetic_edit_distance', as_index = False).mean(),\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "plt.ylabel('average euclidean distance')\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the homophones_df and split it word pairs into indiviudal columns\n",
    "homophones = pd.read_csv('Data/homophones.txt')\n",
    "column_names = ['word_1','word_2']\n",
    "homophones[column_names] = homophones.word_pairs.str.strip('()').str.split(',', expand = True)\n",
    "homophones[\"word_1\"] = homophones.word_1.str.strip(' \\'\\'')\n",
    "homophones[\"word_2\"] = homophones.word_2.str.strip(' \\'')\n",
    "del homophones[\"word_pairs\"]\n",
    "cols = list(homophones)\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(0, cols.pop(cols.index('word_2')))\n",
    "cols.insert(0, cols.pop(cols.index('word_1')))\n",
    "homophones = homophones.loc[:, cols]\n",
    "homophones.to_csv('Data/homophones_expanded.txt', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of Nearest Neighbour Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_commas(string):\n",
    "    '''Takes a string and returns a filtered string with only alphabet and commas'''\n",
    "    \n",
    "    return ''.join(e for e in string if (e.isalpha() or e == \",\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spearman_with_awe(word,nn_words,word_embedding_dict):\n",
    "    '''Takes a  word and it's list of nearest neighbour words, \n",
    "    calculates their awe and calculates the spearman rank coefficient'''\n",
    "    \n",
    "    nn_words_ranks = np.array((np.arange(1,len(nn_words)+1))).reshape(1,-1)\n",
    "    \n",
    "    word_embedding = word_embedding_dict.item().get(word).squeeze()\n",
    "    nn_words_embeddings = np.stack([word_embedding_dict.item().get(word).squeeze() for word in nn_words])\n",
    "    #print(word_embedding.shape,nn_words_embeddings.shape)\n",
    "    similarity = pairwise_kernels(word_embedding.reshape(1,-1),nn_words_embeddings, metric = 'cosine')\n",
    "    awe_words_ranks = np.argsort(-similarity)+1\n",
    "    \n",
    "    #print(nn_words_ranks, awe_words_ranks)\n",
    "    rho,p_value = stats.spearmanr(nn_words_ranks.ravel(), awe_words_ranks.ravel())\n",
    "    return rho,p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the nearest neighbours based on embeddings and orthographic/phonetic representation\n",
    "#em_cosine_nn = pd.read_csv('Data/em_nearest_neighbours.txt')\n",
    "edit_distance_nn = pd.read_csv('Data/edit_nearest_neighbours.txt')\n",
    "sim_distance_nn = pd.read_csv('Data/sim_nearest_neighbours.txt')\n",
    "homophones = pd.read_csv('Data/raw_ph_homophones.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = np.load('Data/word_embedding_dict.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0415986e-01, 2.4921808e+01, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "       0.0000000e+00, 3.6583733e-04], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_dict.item().get(\"thank\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>neighbours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mmhmm</td>\n",
       "      <td>regional,mhhmm,hmmmm,parallel,bumps,mmmmmm,mmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thank</td>\n",
       "      <td>thinked,think,thing,thingll,pinned,seemed,hang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uhhuh</td>\n",
       "      <td>avril,avocado,crack,liger,addon,mmhmm,whatnot,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>already</td>\n",
       "      <td>roller,cloak,coordinate,laundry,figleaf,orally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analyse</td>\n",
       "      <td>penlight,anonymous,fabulous,analysed,dialects,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>ponnen</td>\n",
       "      <td>problem,scrollbutton,probabl,profitmargin,trun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>vanna</td>\n",
       "      <td>dimensional,banana,bananabando,bananarama,frui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>origi</td>\n",
       "      <td>exhibit,misplace,hourish,upstairs,azerty,robin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>refresh</td>\n",
       "      <td>wordperfect,imagination,demonstration,weixuns,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>thataway</td>\n",
       "      <td>plotters,other,hardish,outline,parallelogram,f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9974 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word                                         neighbours\n",
       "0        mmhmm  regional,mhhmm,hmmmm,parallel,bumps,mmmmmm,mmm...\n",
       "1        thank  thinked,think,thing,thingll,pinned,seemed,hang...\n",
       "2        uhhuh  avril,avocado,crack,liger,addon,mmhmm,whatnot,...\n",
       "3      already  roller,cloak,coordinate,laundry,figleaf,orally...\n",
       "4      analyse  penlight,anonymous,fabulous,analysed,dialects,...\n",
       "...        ...                                                ...\n",
       "9969    ponnen  problem,scrollbutton,probabl,profitmargin,trun...\n",
       "9970     vanna  dimensional,banana,bananabando,bananarama,frui...\n",
       "9971     origi  exhibit,misplace,hourish,upstairs,azerty,robin...\n",
       "9972   refresh  wordperfect,imagination,demonstration,weixuns,...\n",
       "9973  thataway  plotters,other,hardish,outline,parallelogram,f...\n",
       "\n",
       "[9974 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_cosine_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>orthographic</th>\n",
       "      <th>raw_phonetic</th>\n",
       "      <th>filtered_phonetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cheapie</td>\n",
       "      <td>('cheaper', 'cheaply', 'cheap', 'cheapy', 'che...</td>\n",
       "      <td>('cheapy', 'chippy', 'cheaps', 'cheaper', 'che...</td>\n",
       "      <td>('cheapy', 'cheap', 'cheaply', 'chippy', 'chea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conjunction</td>\n",
       "      <td>('connection', 'conjunct', 'convention', 'cons...</td>\n",
       "      <td>('connection', 'consumption', 'conjunctural', ...</td>\n",
       "      <td>('conjunctural', 'connection', 'consumption', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nicer</td>\n",
       "      <td>('niche', 'never', 'nicety', 'timer', 'univer'...</td>\n",
       "      <td>('lesser', 'night', 'fibre', 'dicier', 'nines'...</td>\n",
       "      <td>('nigger', 'wiper', 'answer', 'never', 'buyer'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ourselves</td>\n",
       "      <td>('yourselves', 'ourself', 'relies', 'courses',...</td>\n",
       "      <td>('ourself', 'yourselves', 'yourself', 'solves'...</td>\n",
       "      <td>('ourself', 'sells', 'cells', 'solves', 'thems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temporarily</td>\n",
       "      <td>('temporary', 'temporal', 'arbitrarily', 'terr...</td>\n",
       "      <td>('necessarily', 'temporal', 'temporary', 'temp...</td>\n",
       "      <td>('temporary', 'temporal', 'necessarily', 'temp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>global</td>\n",
       "      <td>('globe', 'local', 'loyal', 'globs', 'globby',...</td>\n",
       "      <td>('globally', 'globe', 'mobile', 'label', 'loca...</td>\n",
       "      <td>('globally', 'local', 'mobile', 'label', 'glob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>hearing</td>\n",
       "      <td>('healing', 'heating', 'wearing', 'gearing', '...</td>\n",
       "      <td>('heating', 'healing', 'keyring', 'hitting', '...</td>\n",
       "      <td>('healing', 'heating', 'keyring', 'string', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>slidebar</td>\n",
       "      <td>('slider', 'slides', 'slidey', 'spider', 'line...</td>\n",
       "      <td>('slideshow', 'slider', 'sliders', 'sliding', ...</td>\n",
       "      <td>('sliders', 'slideshow', 'slides', 'slide', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>dedicated</td>\n",
       "      <td>('dedicate', 'educated', 'indicated', 'delicat...</td>\n",
       "      <td>('dedicate', 'edited', 'indicated', 'dissected...</td>\n",
       "      <td>('indicated', 'dedicate', 'educated', 'edited'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>pathetic</td>\n",
       "      <td>('partic', 'aesthetic', 'magnetic', 'cosmetic'...</td>\n",
       "      <td>('kinetic', 'aesthetic', 'predic', 'generic', ...</td>\n",
       "      <td>('kinetic', 'aesthetic', 'pedantic', 'plastic'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9974 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                       orthographic  \\\n",
       "0         cheapie  ('cheaper', 'cheaply', 'cheap', 'cheapy', 'che...   \n",
       "1     conjunction  ('connection', 'conjunct', 'convention', 'cons...   \n",
       "2           nicer  ('niche', 'never', 'nicety', 'timer', 'univer'...   \n",
       "3       ourselves  ('yourselves', 'ourself', 'relies', 'courses',...   \n",
       "4     temporarily  ('temporary', 'temporal', 'arbitrarily', 'terr...   \n",
       "...           ...                                                ...   \n",
       "9969       global  ('globe', 'local', 'loyal', 'globs', 'globby',...   \n",
       "9970      hearing  ('healing', 'heating', 'wearing', 'gearing', '...   \n",
       "9971     slidebar  ('slider', 'slides', 'slidey', 'spider', 'line...   \n",
       "9972    dedicated  ('dedicate', 'educated', 'indicated', 'delicat...   \n",
       "9973     pathetic  ('partic', 'aesthetic', 'magnetic', 'cosmetic'...   \n",
       "\n",
       "                                           raw_phonetic  \\\n",
       "0     ('cheapy', 'chippy', 'cheaps', 'cheaper', 'che...   \n",
       "1     ('connection', 'consumption', 'conjunctural', ...   \n",
       "2     ('lesser', 'night', 'fibre', 'dicier', 'nines'...   \n",
       "3     ('ourself', 'yourselves', 'yourself', 'solves'...   \n",
       "4     ('necessarily', 'temporal', 'temporary', 'temp...   \n",
       "...                                                 ...   \n",
       "9969  ('globally', 'globe', 'mobile', 'label', 'loca...   \n",
       "9970  ('heating', 'healing', 'keyring', 'hitting', '...   \n",
       "9971  ('slideshow', 'slider', 'sliders', 'sliding', ...   \n",
       "9972  ('dedicate', 'edited', 'indicated', 'dissected...   \n",
       "9973  ('kinetic', 'aesthetic', 'predic', 'generic', ...   \n",
       "\n",
       "                                      filtered_phonetic  \n",
       "0     ('cheapy', 'cheap', 'cheaply', 'chippy', 'chea...  \n",
       "1     ('conjunctural', 'connection', 'consumption', ...  \n",
       "2     ('nigger', 'wiper', 'answer', 'never', 'buyer'...  \n",
       "3     ('ourself', 'sells', 'cells', 'solves', 'thems...  \n",
       "4     ('temporary', 'temporal', 'necessarily', 'temp...  \n",
       "...                                                 ...  \n",
       "9969  ('globally', 'local', 'mobile', 'label', 'glob...  \n",
       "9970  ('healing', 'heating', 'keyring', 'string', 'h...  \n",
       "9971  ('sliders', 'slideshow', 'slides', 'slide', 's...  \n",
       "9972  ('indicated', 'dedicate', 'educated', 'edited'...  \n",
       "9973  ('kinetic', 'aesthetic', 'pedantic', 'plastic'...  \n",
       "\n",
       "[9974 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_distance_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_task(nn_df,column_name = \"raw_phonetic\"):\n",
    "    words = set(sim_distance_nn[\"word\"].to_list())\n",
    "    #words = [\"cheapie\"]\n",
    "    avg_rho = 0\n",
    "    for word in words:\n",
    "        query = nn_df.query(\"word == '%s'\"%(word))[column_name].item()\n",
    "        nn_words = alphabet_commas(query).split(\",\")\n",
    "        rho,p_value = calc_spearman_with_awe(word,nn_words, word_embedding_dict)\n",
    "        avg_rho += rho\n",
    "\n",
    "\n",
    "    avg_rho = avg_rho/len(words)\n",
    "    return avg_rho\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4954177832060372\n",
      "0.487934083161677\n",
      "0.49291430446433687\n"
     ]
    }
   ],
   "source": [
    "print(similarity_task(sim_distance_nn,\"raw_phonetic\"))\n",
    "print(similarity_task(sim_distance_nn,\"filtered_phonetic\"))\n",
    "print(similarity_task(sim_distance_nn,\"orthographic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4885781820612404\n",
      "0.4897861713181568\n",
      "0.4951018101609663\n"
     ]
    }
   ],
   "source": [
    "print(similarity_task(edit_distance_nn,\"raw_phonetic\"))\n",
    "print(similarity_task(edit_distance_nn,\"filtered_phonetic\"))\n",
    "print(similarity_task(edit_distance_nn,\"orthographic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoney = BigPhoney()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(homophones[\"word\"].to_list())\n",
    "phoneme_dict = {}\n",
    "for word in words:\n",
    "    phoneme_dict[word] = phoney.phonize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>homophone_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>slidey</td>\n",
       "      <td>slidy,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word homophone_words\n",
       "24  slidey          slidy,"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homophones[homophones[\"word\"] == \"slidey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_alphabets(string):\n",
    "\treturn ''.join(e for e in string if (e.isalpha() or e.isspace()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homophone_task(homophones):\n",
    "    words = set(homophones[\"word\"].to_list())\n",
    "    homophones[\"homophone_words\"] = homophones[\"homophone_words\"].apply(alphabet_commas) \n",
    "\n",
    "    avg_precision = 0\n",
    "    \n",
    "    phoneme_eDistance_list = []\n",
    "    for word in words:\n",
    "        homophone_query = homophones.query(\"word == '%s'\"%(word))\n",
    "        awe_nn_words_query = em_cosine_nn.query(\"word == '%s'\"%(word))\n",
    "\n",
    "        homophone_words = list(filter(lambda x: x.isalpha(), homophone_query[\"homophone_words\"].item().split(\",\")))\n",
    "        awe_nn_words = list(awe_nn_words_query[\"neighbours\"].item().split(\",\"))[:len(homophone_words)]\n",
    "\n",
    "        #print(awe_nn_words)\n",
    "\n",
    "        #Set of homophone words\n",
    "        set_homophone_words = set(homophone_words)\n",
    "\n",
    "        #Set of Nearest neighbours based on cosine_similarity of embeddings\n",
    "        set_awe_nn_words = set(awe_nn_words)\n",
    "        \n",
    "        \n",
    "        #print(word,set_homophone_words,set_awe_nn_words)\n",
    "        \n",
    "        for word_1,word_2 in zip(list(set_homophone_words),list(set_awe_nn_words)):\n",
    "            \n",
    "            aligned_seq1, aligned_seq2, eDistance = alignSequences.align(filter_alphabets(phoney.phonize(word_1)),filter_alphabets(phoney.phonize(word_2)))\n",
    "            \n",
    "            print('{%s , %s , %s } Phoneme eDistance %d'%(word,word_1,word_2,eDistance))\n",
    "            \n",
    "            phoneme_eDistance_list.append(eDistance)\n",
    "        \n",
    "        \n",
    "        #Calculate precision score\n",
    "        word_precision = len(set_homophone_words.intersection(set_awe_nn_words))/len(set_homophone_words)\n",
    "\n",
    "        avg_precision += word_precision\n",
    "\n",
    "    avg_precision = avg_precision/len(words)\n",
    "    print(avg_precision)\n",
    "    \n",
    "    return phoneme_eDistance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{through , threw , carrier } Phoneme eDistance 4\n",
      "{commitment , committment , equipments } Phoneme eDistance 5\n",
      "{weights , waits , weight } Phoneme eDistance 1\n",
      "{handy , handi , napki } Phoneme eDistance 3\n",
      "{simpl , simple , should } Phoneme eDistance 6\n",
      "{strawberrys , strawberries , forgo } Phoneme eDistance 7\n",
      "{corinne , corrine , pretty } Phoneme eDistance 5\n",
      "{prioritise , prioritize , prioritized } Phoneme eDistance 1\n",
      "{pares , pairs , players } Phoneme eDistance 3\n",
      "{plain , plane , playing } Phoneme eDistance 2\n",
      "{parti , party , their } Phoneme eDistance 4\n",
      "{mails , males , names } Phoneme eDistance 2\n",
      "{acquaintance , aquaintance , gorgeous } Phoneme eDistance 7\n",
      "{peoplell , peopl , poodle } Phoneme eDistance 2\n",
      "{peoplell , people , people } Phoneme eDistance 0\n",
      "{disks , discs , these } Phoneme eDistance 5\n",
      "{franc , frank , throwing } Phoneme eDistance 4\n",
      "{weighted , waited , wasting } Phoneme eDistance 2\n",
      "{frank , franc , print } Phoneme eDistance 4\n",
      "{minimalised , minimalized , encouragement } Phoneme eDistance 10\n",
      "{focusing , focussing , forcing } Phoneme eDistance 3\n",
      "{earli , early , writing } Phoneme eDistance 5\n",
      "{waiting , weighting , willing } Phoneme eDistance 2\n",
      "{spacial , spatial , special } Phoneme eDistance 1\n",
      "{theirs , theres , guess } Phoneme eDistance 3\n",
      "{minut , minute , moved } Phoneme eDistance 4\n",
      "{purse , perce , paris } Phoneme eDistance 3\n",
      "{brake , break , break } Phoneme eDistance 0\n",
      "{right , write , backforward } Phoneme eDistance 8\n",
      "{thingys , thingies , thingies } Phoneme eDistance 0\n",
      "{hightech , hitech , sidetracked } Phoneme eDistance 5\n",
      "{centre , centr , senden } Phoneme eDistance 3\n",
      "{fourteenth , fourteenthd , putting } Phoneme eDistance 6\n",
      "{penlight , penlite , analyse } Phoneme eDistance 4\n",
      "{perce , purse , percent } Phoneme eDistance 3\n",
      "{ionizing , ionising , requested } Phoneme eDistance 8\n",
      "{summary , summery , silvery } Phoneme eDistance 3\n",
      "{slidy , slidey , sponge } Phoneme eDistance 4\n",
      "{logon , logan , mobile } Phoneme eDistance 3\n",
      "{committment , commitment , plusminus } Phoneme eDistance 7\n",
      "{paired , pared , higher } Phoneme eDistance 4\n",
      "{summarise , summarize , requir } Phoneme eDistance 6\n",
      "{summaries , summarys , familys } Phoneme eDistance 3\n",
      "{recogniser , recognizer , incrementally } Phoneme eDistance 9\n",
      "{summarys , summaries , summaries } Phoneme eDistance 0\n",
      "{clare , claire , prefer } Phoneme eDistance 5\n",
      "{spatial , spacial , special } Phoneme eDistance 1\n",
      "{piece , peace , tastes } Phoneme eDistance 4\n",
      "{comman , commen , trendly } Phoneme eDistance 7\n",
      "{comman , common , trend } Phoneme eDistance 5\n",
      "{recognize , recognise , grandad } Phoneme eDistance 8\n",
      "{cheapie , cheapy , particularly } Phoneme eDistance 11\n",
      "{sombodys , somebodys , somebodys } Phoneme eDistance 0\n",
      "{definite , definit , decimal } Phoneme eDistance 3\n",
      "{tellys , tellies , seventies } Phoneme eDistance 5\n",
      "{sells , cells , cells } Phoneme eDistance 0\n",
      "{summarizer , summariser , summarisers } Phoneme eDistance 1\n",
      "{tudumm , tudum , times } Phoneme eDistance 4\n",
      "{penlite , penlight , standalone } Phoneme eDistance 7\n",
      "{plasticine , plasticene , wrestling } Phoneme eDistance 7\n",
      "{party , parti , evalu } Phoneme eDistance 5\n",
      "{summarising , summarizing , service } Phoneme eDistance 6\n",
      "{ambience , ambiance , convince } Phoneme eDistance 5\n",
      "{greyed , grade , green } Phoneme eDistance 2\n",
      "{definit , definite , presentation } Phoneme eDistance 7\n",
      "{devision , division , attain } Phoneme eDistance 6\n",
      "{programs , programmes , relies } Phoneme eDistance 6\n",
      "{principal , principle , principle } Phoneme eDistance 0\n",
      "{stylised , stylized , stand } Phoneme eDistance 4\n",
      "{carrots , carets , covers } Phoneme eDistance 5\n",
      "{krista , christa , person } Phoneme eDistance 5\n",
      "{hitech , hightech , exhibit } Phoneme eDistance 7\n",
      "{grease , greece , please } Phoneme eDistance 3\n",
      "{coarse , course , course } Phoneme eDistance 0\n",
      "{aquaintance , acquaintance , hypotheses } Phoneme eDistance 9\n",
      "{techne , techni , apparently } Phoneme eDistance 6\n",
      "{rights , writes , device } Phoneme eDistance 4\n",
      "{specialized , specialised , amplifiers } Phoneme eDistance 7\n",
      "{greece , grease , reduces } Phoneme eDistance 6\n",
      "{threw , through , through } Phoneme eDistance 0\n",
      "{logan , logon , evaluala } Phoneme eDistance 7\n",
      "{somebodys , sombodys , maximising } Phoneme eDistance 8\n",
      "{people , peoplell , toolbar } Phoneme eDistance 6\n",
      "{people , peopl , poodle } Phoneme eDistance 2\n",
      "{handi , handy , anyway } Phoneme eDistance 5\n",
      "{differe , differ , different } Phoneme eDistance 3\n",
      "{differ , differe , different } Phoneme eDistance 3\n",
      "{memories , memorys , reminders } Phoneme eDistance 6\n",
      "{deali , dealy , frape } Phoneme eDistance 4\n",
      "{functionalities , functionalitys , functionalitys } Phoneme eDistance 0\n",
      "{summarizing , summarising , stressy } Phoneme eDistance 7\n",
      "{dependant , dependent , anything } Phoneme eDistance 7\n",
      "{focused , focussed , vantage } Phoneme eDistance 6\n",
      "{speek , speak , speaker } Phoneme eDistance 1\n",
      "{early , earli , steveo } Phoneme eDistance 5\n",
      "{standardized , standardised , cedric } Phoneme eDistance 8\n",
      "{characterisation , characterization , unintelligible } Phoneme eDistance 11\n",
      "{recognise , recognize , grandad } Phoneme eDistance 8\n",
      "{plasticene , plasticine , multimedia } Phoneme eDistance 8\n",
      "{dependent , dependant , commented } Phoneme eDistance 6\n",
      "{elans , ellens , always } Phoneme eDistance 3\n",
      "{grade , greyed , great } Phoneme eDistance 1\n",
      "{highly , highli , pioneering } Phoneme eDistance 7\n",
      "{weight , whate , weights } Phoneme eDistance 1\n",
      "{bodys , bodies , buttons } Phoneme eDistance 4\n",
      "{cheapy , cheapie , screen } Phoneme eDistance 4\n",
      "{fills , phils , skills } Phoneme eDistance 2\n",
      "{there , their , their } Phoneme eDistance 0\n",
      "{middl , middle , normally } Phoneme eDistance 5\n",
      "{peopl , peoplell , youll } Phoneme eDistance 4\n",
      "{peopl , people , people } Phoneme eDistance 0\n",
      "{summar , summer , similar } Phoneme eDistance 3\n",
      "{woohoo , whoohoo , mmhmm } Phoneme eDistance 5\n",
      "{panes , pains , cones } Phoneme eDistance 2\n",
      "{steel , steal , skill } Phoneme eDistance 2\n",
      "{gunnpeterson , gunnpetersen , exhibit } Phoneme eDistance 9\n",
      "{chilli , chile , today } Phoneme eDistance 4\n",
      "{whate , weight , mightnt } Phoneme eDistance 5\n",
      "{visualize , visualise , device } Phoneme eDistance 6\n",
      "{judgment , judgement , congratulate } Phoneme eDistance 8\n",
      "{customized , customised , customers } Phoneme eDistance 2\n",
      "{knoing , knowing , joint } Phoneme eDistance 4\n",
      "{specialised , specialized , thered } Phoneme eDistance 7\n",
      "{dealy , deali , exhibit } Phoneme eDistance 7\n",
      "{chile , chilli , silly } Phoneme eDistance 1\n",
      "{pared , paired , carried } Phoneme eDistance 3\n",
      "{rolls , roles , roles } Phoneme eDistance 0\n",
      "{metre , meter , meeting } Phoneme eDistance 2\n",
      "{moulds , molds , mould } Phoneme eDistance 1\n",
      "{slidey , slidy , slide } Phoneme eDistance 2\n",
      "{spongy , spongey , snowmanshape } Phoneme eDistance 6\n",
      "{freak , freek , investigated } Phoneme eDistance 12\n",
      "{custom , custome , accustomed } Phoneme eDistance 2\n",
      "{forrest , forest , files } Phoneme eDistance 5\n",
      "{steal , steel , still } Phoneme eDistance 1\n",
      "{cells , sells , sells } Phoneme eDistance 0\n",
      "{discs , disks , guess } Phoneme eDistance 4\n",
      "{prioritize , prioritise , spellings } Phoneme eDistance 8\n",
      "{strawberries , strawberrys , strengths } Phoneme eDistance 6\n",
      "{analyser , analyzer , motif } Phoneme eDistance 7\n",
      "{break , brake , brake } Phoneme eDistance 0\n",
      "{summariser , summarizer , summarisers } Phoneme eDistance 1\n",
      "{summer , summar , temporary } Phoneme eDistance 6\n",
      "{minute , minut , mailing } Phoneme eDistance 4\n",
      "{thingies , thingys , things } Phoneme eDistance 1\n",
      "{carets , carrots , curved } Phoneme eDistance 5\n",
      "{wooden , wouldn , winning } Phoneme eDistance 4\n",
      "{wales , whales , minus } Phoneme eDistance 5\n",
      "{wierd , weird , wouldve } Phoneme eDistance 3\n",
      "{commen , comman , exhibit } Phoneme eDistance 7\n",
      "{commen , common , misplace } Phoneme eDistance 7\n",
      "{adaptor , adapter , button } Phoneme eDistance 6\n",
      "{presen , preson , dozen } Phoneme eDistance 3\n",
      "{whales , wales , rather } Phoneme eDistance 4\n",
      "{summery , summary , forty } Phoneme eDistance 4\n",
      "{weighting , waiting , chrome } Phoneme eDistance 5\n",
      "{gunnpetersen , gunnpeterson , phonebased } Phoneme eDistance 9\n",
      "{playdoh , playdo , katie } Phoneme eDistance 4\n",
      "{pairs , pares , contain } Phoneme eDistance 6\n",
      "{edition , addition , addition } Phoneme eDistance 0\n",
      "{males , mails , metal } Phoneme eDistance 4\n",
      "{customizing , customising , technologically } Phoneme eDistance 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{characterization , characterisation , particularity } Phoneme eDistance 12\n",
      "{shuts , schutz , schutz } Phoneme eDistance 0\n",
      "{knowing , knoing , ellen } Phoneme eDistance 4\n",
      "{custome , custom , customers } Phoneme eDistance 2\n",
      "{stylized , stylised , sacrificed } Phoneme eDistance 7\n",
      "{emphasise , emphasize , nondescript } Phoneme eDistance 10\n",
      "{channelll , channel , transcribed } Phoneme eDistance 8\n",
      "{usable , useable , usersll } Phoneme eDistance 3\n",
      "{course , coarse , coolest } Phoneme eDistance 4\n",
      "{hairy , harry , battery } Phoneme eDistance 4\n",
      "{moldable , mouldable , animal } Phoneme eDistance 5\n",
      "{theres , theirs , luxurious } Phoneme eDistance 8\n",
      "{centr , centre , fruit } Phoneme eDistance 4\n",
      "{board , bored , avoidable } Phoneme eDistance 7\n",
      "{claire , clare , foldout } Phoneme eDistance 5\n",
      "{waits , weights , lights } Phoneme eDistance 2\n",
      "{pains , panes , change } Phoneme eDistance 2\n",
      "{programmes , programs , provenance } Phoneme eDistance 7\n",
      "{wouldn , wooden , wouldnt } Phoneme eDistance 1\n",
      "{waited , weighted , limited } Phoneme eDistance 5\n",
      "{plane , plain , being } Phoneme eDistance 4\n",
      "{hmhmm , hmmhmm , remember } Phoneme eDistance 4\n",
      "{peace , piece , pieces } Phoneme eDistance 2\n",
      "{speak , speek , speech } Phoneme eDistance 1\n",
      "{butts , buttes , parts } Phoneme eDistance 3\n",
      "{maarten , martin , husband } Phoneme eDistance 5\n",
      "{succe , sucks , switch } Phoneme eDistance 3\n",
      "{ellens , elans , counts } Phoneme eDistance 5\n",
      "{bodies , bodys , utterancebased } Phoneme eDistance 10\n",
      "{ambiance , ambience , regions } Phoneme eDistance 5\n",
      "{roles , rolls , rolls } Phoneme eDistance 0\n",
      "{sucks , succe , television } Phoneme eDistance 8\n",
      "{simple , simpl , subgoal } Phoneme eDistance 4\n",
      "{techni , techne , implemen } Phoneme eDistance 7\n",
      "{buttes , butts , those } Phoneme eDistance 4\n",
      "{preson , presen , luxury } Phoneme eDistance 6\n",
      "{highli , highly , camera } Phoneme eDistance 5\n",
      "{bored , board , board } Phoneme eDistance 0\n",
      "{addition , edition , interject } Phoneme eDistance 8\n",
      "{meter , metre , alienate } Phoneme eDistance 7\n",
      "{visualisation , visualization , television } Phoneme eDistance 8\n",
      "{forth , fourth , fourth } Phoneme eDistance 0\n",
      "{functionalitys , functionalities , functionalities } Phoneme eDistance 0\n",
      "{weird , wierd , wierd } Phoneme eDistance 0\n",
      "{missus , misses , traces } Phoneme eDistance 4\n",
      "{visualization , visualisation , multifunctional } Phoneme eDistance 10\n",
      "{recognizer , recogniser , exercises } Phoneme eDistance 6\n",
      "{programme , program , program } Phoneme eDistance 0\n",
      "{useable , usable , usable } Phoneme eDistance 0\n",
      "{focussed , focused , focus } Phoneme eDistance 1\n",
      "{middle , middl , newly } Phoneme eDistance 5\n",
      "{emphasize , emphasise , ideas } Phoneme eDistance 5\n",
      "{write , right , right } Phoneme eDistance 0\n",
      "{fourth , forth , formed } Phoneme eDistance 2\n",
      "{whether , weather , overhead } Phoneme eDistance 6\n",
      "{division , devision , efficient } Phoneme eDistance 4\n",
      "{misses , missus , missed } Phoneme eDistance 2\n",
      "{channel , channelll , channe } Phoneme eDistance 2\n",
      "{phils , fills , chance } Phoneme eDistance 4\n",
      "{freek , freak , click } Phoneme eDistance 3\n",
      "{tellies , tellys , times } Phoneme eDistance 3\n",
      "{standardised , standardized , standards } Phoneme eDistance 2\n",
      "{molds , moulds , models } Phoneme eDistance 4\n",
      "{wheres , wears , arrears } Phoneme eDistance 2\n",
      "{harry , hairy , thirty } Phoneme eDistance 3\n",
      "{spongey , spongy , snowmanshape } Phoneme eDistance 6\n",
      "{martin , maarten , might } Phoneme eDistance 4\n",
      "{summarize , summarise , superficial } Phoneme eDistance 7\n",
      "{wears , wheres , whereas } Phoneme eDistance 1\n",
      "{their , there , there } Phoneme eDistance 0\n",
      "{forest , forrest , forced } Phoneme eDistance 1\n",
      "{batterys , batteries , batteries } Phoneme eDistance 0\n",
      "{minimalized , minimalised , realises } Phoneme eDistance 7\n",
      "{hmmhmm , hmhmm , learn } Phoneme eDistance 5\n",
      "{judgement , judgment , potentialmeter } Phoneme eDistance 9\n",
      "{principle , principal , consult } Phoneme eDistance 6\n",
      "{tudum , tudumm , touch } Phoneme eDistance 3\n",
      "{whoohoo , woohoo , rework } Phoneme eDistance 5\n",
      "{fourteenthd , fourteenth , specimen } Phoneme eDistance 8\n",
      "{analyzer , analyser , hours } Phoneme eDistance 6\n",
      "{visualise , visualize , especialised } Phoneme eDistance 6\n",
      "{weather , whether , whether } Phoneme eDistance 0\n",
      "{playdo , playdoh , taking } Phoneme eDistance 5\n",
      "{batteries , batterys , laughters } Phoneme eDistance 3\n",
      "{ionising , ionizing , ninety } Phoneme eDistance 5\n",
      "{schutz , shuts , objects } Phoneme eDistance 5\n",
      "{customising , customizing , distinguishing } Phoneme eDistance 8\n",
      "{writes , rights , class } Phoneme eDistance 3\n",
      "{adapter , adaptor , nokia } Phoneme eDistance 6\n",
      "{mouldable , moldable , wonderful } Phoneme eDistance 5\n",
      "{common , comman , cochrane } Phoneme eDistance 2\n",
      "{common , commen , coming } Phoneme eDistance 3\n",
      "{memorys , memories , minimise } Phoneme eDistance 5\n",
      "{focussing , focusing , functon } Phoneme eDistance 6\n",
      "{christa , krista , christmas } Phoneme eDistance 2\n",
      "{corrine , corinne , committed } Phoneme eDistance 6\n",
      "{customised , customized , optimises } Phoneme eDistance 5\n",
      "{program , programme , programme } Phoneme eDistance 0\n",
      "0.11023622047244094\n"
     ]
    }
   ],
   "source": [
    "a = homophone_task(homophones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 38,\n",
       "         5: 38,\n",
       "         1: 18,\n",
       "         3: 29,\n",
       "         6: 28,\n",
       "         7: 25,\n",
       "         2: 25,\n",
       "         0: 29,\n",
       "         10: 4,\n",
       "         8: 16,\n",
       "         9: 5,\n",
       "         11: 3,\n",
       "         12: 2})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
