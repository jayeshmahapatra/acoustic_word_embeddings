{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Core Python, Pandas, and kaldi_io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter,OrderedDict \n",
    "import kaldi_io\n",
    "from datetime import datetime\n",
    "\n",
    "#Scikit\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances,average_precision_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels,paired_distances\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#BigPhoney\n",
    "from big_phoney import BigPhoney\n",
    "\n",
    "\n",
    "#Torch and utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader,random_split,ConcatDataset\n",
    "\n",
    "#Import User defined classes\n",
    "from data_helpers import DataHelper\n",
    "from models import SimpleNet\n",
    "from train_test_helpers import accuracy,train_model,evaluate_model,evaluate_model_paper,test_model,plot_learning_curves\n",
    "from sfba4.utils import alignSequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_list = ['Data/feats_cmvn.ark']\n",
    "#number_list = [9,12,14,18,21,25,27,28]\n",
    "#load_list = ['Data/raw_mfcc_AMI_Segments.%d.scp'%(number) for number in number_list]\n",
    "num_examples = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHelper(load_list,num_examples)\n",
    "dh.load_data()\n",
    "dh.process_data()\n",
    "c,word_to_num,num_to_word = dh.generate_key_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = dh.give_inputs_and_labels()\n",
    "del dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = False\n",
    "if split:\n",
    "    x_trainval,x_test,y_trainval,y_test = train_test_split(inputs, labels, test_size=0.2, random_state=32)\n",
    "    x_train,x_val,y_train,y_val = train_test_split(x_trainval,y_trainval,test_size =0.25, random_state = 32)\n",
    "    x_train,y_train = torch.tensor(x_train,dtype= torch.float),torch.tensor(y_train, dtype= torch.float)\n",
    "    x_val,y_val = torch.tensor(x_val, dtype= torch.float),torch.tensor(y_val, dtype= torch.float)\n",
    "    x_test,y_test = torch.tensor(x_test, dtype= torch.float),torch.tensor(y_test, dtype= torch.float)\n",
    "    print(x_train.shape,y_train.shape)\n",
    "    print(x_val.shape,y_val.shape)\n",
    "    print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = SimpleNet()\n",
    "num_output = len(c.keys())\n",
    "net = SimpleNet(num_output)\n",
    "net = net.float()\n",
    "net.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best model\n",
    "best_model_path = \"./Models/awe_best_model.pth\"\n",
    "net.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_time():\n",
    "\tnow = datetime.now()\n",
    "\tcurrent_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\tprint(\"Current Date and Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the word_pairs DataFrame\n",
    "wordpairs_df = pd.read_csv('Data/wordpairs.txt', sep = ',')\n",
    "#del wordpairs_df[\"raw_phonetic_edit_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate all the unique words\n",
    "def words_from_dataframe(dataframe):\n",
    "    wordpairs_list = dataframe[\"word_pairs\"].apply(lambda x: x.strip('()').split(','))\n",
    "    words = [word.strip(' \\'') for wordpair in wordpairs_list for word in wordpair]\n",
    "    words = set(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_embedding_dict(words):\n",
    "    word_embedding_dict = OrderedDict()\n",
    "    #Calculate embeddings\n",
    "    for word in words:\n",
    "        #Find the mfcc features of the acoustic representation of the word in the data\n",
    "        word_features = inputs[np.where(np.isin(labels,word_to_num[word]))]\n",
    "        \n",
    "        #Calculate embeddings for the feature\n",
    "        word_embedding = net.give_embeddings(torch.tensor(word_features, device = dev, dtype=torch.float),dev)\n",
    "        \n",
    "        #If the number of representation is more than one, take the average embedding\n",
    "        word_embedding_dict[word] = np.mean(word_embedding, axis = 0).reshape(1,-1)\n",
    "    \n",
    "    return word_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_distance(homophone_df,word_embedding_dict,metrics = ['cosine']):\n",
    "\n",
    "    word1_embeddings = None\n",
    "    word2_embeddings = None\n",
    "    \n",
    "    metric_distance_dict = {}\n",
    "    for metric in metrics:\n",
    "        metric_distance_dict[metric] = []\n",
    "        \n",
    "    for row in homophone_df.itertuples():\n",
    "        word1, word2 = map(lambda x: x.strip(' \\''),row.word_pairs.strip('()').split(','))\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric_distance_dict[metric].append(paired_distances(word_embedding_dict[word1],word_embedding_dict[word2], metric = metric)[0])\n",
    "        \n",
    "        \n",
    "        #if word1_embeddings is None and word2_embeddings is None:\n",
    "        #    word1_embeddings = word_embedding_dict[word1]\n",
    "        #    word2_embeddings = word_embedding_dict[word2]\n",
    "        #else:\n",
    "        #    word1_embeddings = np.vstack((word1_embeddings, word_embedding_dict[word1]))\n",
    "        #    word2_embeddings = np.vstack((word2_embeddings, word_embedding_dict[word2]))\n",
    "            \n",
    "        \n",
    "\n",
    "    #Calculate the distance\n",
    "    #print(word1_embeddings.shape)\n",
    "    for metric in metrics:\n",
    "        #metric_distance = paired_distances(word1_embeddings,word2_embeddings, metric = metric)\n",
    "        homophone_df.insert(len(homophone_df.columns),\"%s_distance\"%(metric), metric_distance_dict[metric], True)\n",
    "    \n",
    "    return homophone_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_nearest_neighbours_on_embeddings(word_embedding_dict, n_neighbours = 10, metric = 'cosine', split = False):\n",
    "    \n",
    "    embeddings = None\n",
    "    \n",
    "    embeddings = np.stack(list(word_embedding_dict.values())).squeeze()\n",
    "    \n",
    "    print('Calculating Nearest Neighbours')\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbours, algorithm='brute',metric = metric, n_jobs = 4).fit(embeddings)\n",
    "    distances,indices = nbrs.kneighbors(embeddings)\n",
    "    \n",
    "    columns = [\"word\",\"neighbours\"]\n",
    "    #nearest_neighbours_df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    words = list(word_embedding_dict.keys())\n",
    "    print('num of words %d'%(len(words)))\n",
    "    \n",
    "    \n",
    "    nearest_neighbours_df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    for i,word in enumerate(word_embedding_dict.keys()):\n",
    "        \n",
    "        neighbours = ','.join([words[indices[i,j]] for j in range(indices.shape[1]) if words[indices[i,j]]!= word])\n",
    "        #print(neighbours)\n",
    "        row = pd.DataFrame(np.array([[word],[neighbours]]).T, columns = columns)\n",
    "        nearest_neighbours_df = nearest_neighbours_df.append(row)\n",
    "        \n",
    "    \n",
    "    #pd.concat([pd.DataFrame(np.array([[word],[','.join([words[indices[i,j]] for j in range(indices.shape[1]) if words[indices[i,j]]!=word ])]]).T, columns = columns) for i,word in enumerate(word_embedding_dict.keys())])\n",
    "    \n",
    "    if split:\n",
    "        neighbour_col_names = [\"neighbour_%d\"%(i) for i in range(n_neighbours)]\n",
    "        nearest_neighbours_df[neighbour_col_names] = nearest_neighbours_df.neighbours.str.split(',', expand = True )\n",
    "        nearest_neighbours_df.drop(columns = [\"neighbours\"],inplace = True)\n",
    "    \n",
    "    \n",
    "    #Reset index\n",
    "    nearest_neighbours_df = nearest_neighbours_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return nearest_neighbours_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = generate_word_embedding_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = generate_word_embedding_dict(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/word_embedding_dict.npy\",word_embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_nearest_neighbours = give_nearest_neighbours_on_embeddings(word_embedding_dict, 10,'cosine', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_embedding_distance(wordpairs_df,word_embedding_dict,metrics = ['cosine', 'euclidean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[0]),\n",
    "    #hue=\"Word\",\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[1]),\n",
    "    #hue=\"Word\",\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('phonetic_edit_distance', as_index = False).agg(['mean', 'count', 'std'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[0]),\n",
    "    #hue=\"Word\",\n",
    "    data=df.groupby('phonetic_edit_distance', as_index = False).mean(),\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "plt.ylabel('average cosine distance')\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x=\"phonetic_edit_distance\", y=\"%s_distance\"%(metrics[1]),\n",
    "    #hue=\"Word\",\n",
    "    data=df.groupby('phonetic_edit_distance', as_index = False).mean(),\n",
    "    legend=\"full\",\n",
    "    alpha=0.5)\n",
    "plt.ylabel('average euclidean distance')\n",
    "#g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the homophones_df and split it word pairs into indiviudal columns\n",
    "homophones = pd.read_csv('Data/homophones.txt')\n",
    "column_names = ['word_1','word_2']\n",
    "homophones[column_names] = homophones.word_pairs.str.strip('()').str.split(',', expand = True)\n",
    "homophones[\"word_1\"] = homophones.word_1.str.strip(' \\'\\'')\n",
    "homophones[\"word_2\"] = homophones.word_2.str.strip(' \\'')\n",
    "del homophones[\"word_pairs\"]\n",
    "cols = list(homophones)\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(0, cols.pop(cols.index('word_2')))\n",
    "cols.insert(0, cols.pop(cols.index('word_1')))\n",
    "homophones = homophones.loc[:, cols]\n",
    "homophones.to_csv('Data/homophones_expanded.txt', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of Nearest Neighbour Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_commas(string):\n",
    "    '''Takes a string and returns a filtered string with only alphabet and commas'''\n",
    "    \n",
    "    return ''.join(e for e in string if (e.isalpha() or e == \",\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spearman_with_awe(word,nn_words,word_embedding_dict):\n",
    "    '''Takes a  word and it's list of nearest neighbour words, \n",
    "    calculates their awe and calculates the spearman rank coefficient'''\n",
    "    \n",
    "    nn_words_ranks = np.array((np.arange(1,len(nn_words)+1))).reshape(1,-1)\n",
    "    \n",
    "    word_embedding = word_embedding_dict.item().get(word).squeeze()\n",
    "    nn_words_embeddings = np.stack([word_embedding_dict.item().get(word).squeeze() for word in nn_words])\n",
    "    #print(word_embedding.shape,nn_words_embeddings.shape)\n",
    "    similarity = pairwise_kernels(word_embedding.reshape(1,-1),nn_words_embeddings, metric = 'cosine')\n",
    "    awe_words_ranks = np.argsort(-similarity)+1\n",
    "    \n",
    "    #print(nn_words_ranks, awe_words_ranks)\n",
    "    rho,p_value = stats.spearmanr(nn_words_ranks.ravel(), awe_words_ranks.ravel())\n",
    "    return rho,p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the nearest neighbours based on embeddings and orthographic/phonetic representation\n",
    "#em_cosine_nn = pd.read_csv('Data/em_nearest_neighbours.txt')\n",
    "edit_distance_nn = pd.read_csv('Data/edit_nearest_neighbours.txt')\n",
    "sim_distance_nn = pd.read_csv('Data/sim_nearest_neighbours.txt')\n",
    "homophones = pd.read_csv('Data/raw_ph_homophones.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dict = np.load('Data/word_embedding_dict.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0415986e-01, 2.4921808e+01, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "       0.0000000e+00, 3.6583733e-04], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_dict.item().get(\"thank\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>neighbours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mmhmm</td>\n",
       "      <td>regional,mhhmm,hmmmm,parallel,bumps,mmmmmm,mmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thank</td>\n",
       "      <td>thinked,think,thing,thingll,pinned,seemed,hang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uhhuh</td>\n",
       "      <td>avril,avocado,crack,liger,addon,mmhmm,whatnot,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>already</td>\n",
       "      <td>roller,cloak,coordinate,laundry,figleaf,orally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analyse</td>\n",
       "      <td>penlight,anonymous,fabulous,analysed,dialects,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>ponnen</td>\n",
       "      <td>problem,scrollbutton,probabl,profitmargin,trun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>vanna</td>\n",
       "      <td>dimensional,banana,bananabando,bananarama,frui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>origi</td>\n",
       "      <td>exhibit,misplace,hourish,upstairs,azerty,robin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>refresh</td>\n",
       "      <td>wordperfect,imagination,demonstration,weixuns,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>thataway</td>\n",
       "      <td>plotters,other,hardish,outline,parallelogram,f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9974 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word                                         neighbours\n",
       "0        mmhmm  regional,mhhmm,hmmmm,parallel,bumps,mmmmmm,mmm...\n",
       "1        thank  thinked,think,thing,thingll,pinned,seemed,hang...\n",
       "2        uhhuh  avril,avocado,crack,liger,addon,mmhmm,whatnot,...\n",
       "3      already  roller,cloak,coordinate,laundry,figleaf,orally...\n",
       "4      analyse  penlight,anonymous,fabulous,analysed,dialects,...\n",
       "...        ...                                                ...\n",
       "9969    ponnen  problem,scrollbutton,probabl,profitmargin,trun...\n",
       "9970     vanna  dimensional,banana,bananabando,bananarama,frui...\n",
       "9971     origi  exhibit,misplace,hourish,upstairs,azerty,robin...\n",
       "9972   refresh  wordperfect,imagination,demonstration,weixuns,...\n",
       "9973  thataway  plotters,other,hardish,outline,parallelogram,f...\n",
       "\n",
       "[9974 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_cosine_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>orthographic</th>\n",
       "      <th>raw_phonetic</th>\n",
       "      <th>filtered_phonetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cheapie</td>\n",
       "      <td>('cheaper', 'cheaply', 'cheap', 'cheapy', 'che...</td>\n",
       "      <td>('cheapy', 'chippy', 'cheaps', 'cheaper', 'che...</td>\n",
       "      <td>('cheapy', 'cheap', 'cheaply', 'chippy', 'chea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conjunction</td>\n",
       "      <td>('connection', 'conjunct', 'convention', 'cons...</td>\n",
       "      <td>('connection', 'consumption', 'conjunctural', ...</td>\n",
       "      <td>('conjunctural', 'connection', 'consumption', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nicer</td>\n",
       "      <td>('niche', 'never', 'nicety', 'timer', 'univer'...</td>\n",
       "      <td>('lesser', 'night', 'fibre', 'dicier', 'nines'...</td>\n",
       "      <td>('nigger', 'wiper', 'answer', 'never', 'buyer'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ourselves</td>\n",
       "      <td>('yourselves', 'ourself', 'relies', 'courses',...</td>\n",
       "      <td>('ourself', 'yourselves', 'yourself', 'solves'...</td>\n",
       "      <td>('ourself', 'sells', 'cells', 'solves', 'thems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temporarily</td>\n",
       "      <td>('temporary', 'temporal', 'arbitrarily', 'terr...</td>\n",
       "      <td>('necessarily', 'temporal', 'temporary', 'temp...</td>\n",
       "      <td>('temporary', 'temporal', 'necessarily', 'temp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>global</td>\n",
       "      <td>('globe', 'local', 'loyal', 'globs', 'globby',...</td>\n",
       "      <td>('globally', 'globe', 'mobile', 'label', 'loca...</td>\n",
       "      <td>('globally', 'local', 'mobile', 'label', 'glob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>hearing</td>\n",
       "      <td>('healing', 'heating', 'wearing', 'gearing', '...</td>\n",
       "      <td>('heating', 'healing', 'keyring', 'hitting', '...</td>\n",
       "      <td>('healing', 'heating', 'keyring', 'string', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>slidebar</td>\n",
       "      <td>('slider', 'slides', 'slidey', 'spider', 'line...</td>\n",
       "      <td>('slideshow', 'slider', 'sliders', 'sliding', ...</td>\n",
       "      <td>('sliders', 'slideshow', 'slides', 'slide', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>dedicated</td>\n",
       "      <td>('dedicate', 'educated', 'indicated', 'delicat...</td>\n",
       "      <td>('dedicate', 'edited', 'indicated', 'dissected...</td>\n",
       "      <td>('indicated', 'dedicate', 'educated', 'edited'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>pathetic</td>\n",
       "      <td>('partic', 'aesthetic', 'magnetic', 'cosmetic'...</td>\n",
       "      <td>('kinetic', 'aesthetic', 'predic', 'generic', ...</td>\n",
       "      <td>('kinetic', 'aesthetic', 'pedantic', 'plastic'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9974 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                       orthographic  \\\n",
       "0         cheapie  ('cheaper', 'cheaply', 'cheap', 'cheapy', 'che...   \n",
       "1     conjunction  ('connection', 'conjunct', 'convention', 'cons...   \n",
       "2           nicer  ('niche', 'never', 'nicety', 'timer', 'univer'...   \n",
       "3       ourselves  ('yourselves', 'ourself', 'relies', 'courses',...   \n",
       "4     temporarily  ('temporary', 'temporal', 'arbitrarily', 'terr...   \n",
       "...           ...                                                ...   \n",
       "9969       global  ('globe', 'local', 'loyal', 'globs', 'globby',...   \n",
       "9970      hearing  ('healing', 'heating', 'wearing', 'gearing', '...   \n",
       "9971     slidebar  ('slider', 'slides', 'slidey', 'spider', 'line...   \n",
       "9972    dedicated  ('dedicate', 'educated', 'indicated', 'delicat...   \n",
       "9973     pathetic  ('partic', 'aesthetic', 'magnetic', 'cosmetic'...   \n",
       "\n",
       "                                           raw_phonetic  \\\n",
       "0     ('cheapy', 'chippy', 'cheaps', 'cheaper', 'che...   \n",
       "1     ('connection', 'consumption', 'conjunctural', ...   \n",
       "2     ('lesser', 'night', 'fibre', 'dicier', 'nines'...   \n",
       "3     ('ourself', 'yourselves', 'yourself', 'solves'...   \n",
       "4     ('necessarily', 'temporal', 'temporary', 'temp...   \n",
       "...                                                 ...   \n",
       "9969  ('globally', 'globe', 'mobile', 'label', 'loca...   \n",
       "9970  ('heating', 'healing', 'keyring', 'hitting', '...   \n",
       "9971  ('slideshow', 'slider', 'sliders', 'sliding', ...   \n",
       "9972  ('dedicate', 'edited', 'indicated', 'dissected...   \n",
       "9973  ('kinetic', 'aesthetic', 'predic', 'generic', ...   \n",
       "\n",
       "                                      filtered_phonetic  \n",
       "0     ('cheapy', 'cheap', 'cheaply', 'chippy', 'chea...  \n",
       "1     ('conjunctural', 'connection', 'consumption', ...  \n",
       "2     ('nigger', 'wiper', 'answer', 'never', 'buyer'...  \n",
       "3     ('ourself', 'sells', 'cells', 'solves', 'thems...  \n",
       "4     ('temporary', 'temporal', 'necessarily', 'temp...  \n",
       "...                                                 ...  \n",
       "9969  ('globally', 'local', 'mobile', 'label', 'glob...  \n",
       "9970  ('healing', 'heating', 'keyring', 'string', 'h...  \n",
       "9971  ('sliders', 'slideshow', 'slides', 'slide', 's...  \n",
       "9972  ('indicated', 'dedicate', 'educated', 'edited'...  \n",
       "9973  ('kinetic', 'aesthetic', 'pedantic', 'plastic'...  \n",
       "\n",
       "[9974 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_distance_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_task(nn_df,column_name = \"raw_phonetic\"):\n",
    "    words = set(sim_distance_nn[\"word\"].to_list())\n",
    "    #words = [\"cheapie\"]\n",
    "    avg_rho = 0\n",
    "    for word in words:\n",
    "        query = nn_df.query(\"word == '%s'\"%(word))[column_name].item()\n",
    "        nn_words = alphabet_commas(query).split(\",\")\n",
    "        rho,p_value = calc_spearman_with_awe(word,nn_words, word_embedding_dict)\n",
    "        avg_rho += rho\n",
    "\n",
    "\n",
    "    avg_rho = avg_rho/len(words)\n",
    "    return avg_rho\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4954177832060372\n",
      "0.487934083161677\n",
      "0.49291430446433687\n"
     ]
    }
   ],
   "source": [
    "print(similarity_task(sim_distance_nn,\"raw_phonetic\"))\n",
    "print(similarity_task(sim_distance_nn,\"filtered_phonetic\"))\n",
    "print(similarity_task(sim_distance_nn,\"orthographic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4885781820612404\n",
      "0.4897861713181568\n",
      "0.4951018101609663\n"
     ]
    }
   ],
   "source": [
    "print(similarity_task(edit_distance_nn,\"raw_phonetic\"))\n",
    "print(similarity_task(edit_distance_nn,\"filtered_phonetic\"))\n",
    "print(similarity_task(edit_distance_nn,\"orthographic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoney = BigPhoney()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(homophones[\"word\"].to_list())\n",
    "phoneme_dict = {}\n",
    "for word in words:\n",
    "    phoneme_dict[word] = phoney.phonize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>homophone_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>slidey</td>\n",
       "      <td>slidy,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word homophone_words\n",
       "24  slidey          slidy,"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homophones[homophones[\"word\"] == \"slidey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_alphabets(string):\n",
    "\treturn ''.join(e for e in string if (e.isalpha() or e.isspace()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homophone_task(homophones):\n",
    "    words = set(homophones[\"word\"].to_list())\n",
    "    homophones[\"homophone_words\"] = homophones[\"homophone_words\"].apply(alphabet_commas) \n",
    "\n",
    "    avg_precision = 0\n",
    "    \n",
    "    phoneme_eDistance_list = []\n",
    "    for word in words:\n",
    "        homophone_query = homophones.query(\"word == '%s'\"%(word))\n",
    "        awe_nn_words_query = em_cosine_nn.query(\"word == '%s'\"%(word))\n",
    "\n",
    "        homophone_words = list(filter(lambda x: x.isalpha(), homophone_query[\"homophone_words\"].item().split(\",\")))\n",
    "        awe_nn_words = list(awe_nn_words_query[\"neighbours\"].item().split(\",\"))[:len(homophone_words)]\n",
    "\n",
    "        #print(awe_nn_words)\n",
    "\n",
    "        #Set of homophone words\n",
    "        set_homophone_words = set(homophone_words)\n",
    "\n",
    "        #Set of Nearest neighbours based on cosine_similarity of embeddings\n",
    "        set_awe_nn_words = set(awe_nn_words)\n",
    "        \n",
    "        \n",
    "        #print(word,set_homophone_words,set_awe_nn_words)\n",
    "        \n",
    "        for word_1,word_2 in zip(list(set_homophone_words),list(set_awe_nn_words)):\n",
    "            \n",
    "            aligned_seq1, aligned_seq2, eDistance = alignSequences.align(filter_alphabets(phoney.phonize(word_1)),filter_alphabets(phoney.phonize(word_2)))\n",
    "            \n",
    "            print('{%s , %s , %s } Phoneme eDistance %d'%(word,word_1,word_2,eDistance))\n",
    "            \n",
    "            phoneme_eDistance_list.append(eDistance)\n",
    "        \n",
    "        \n",
    "        #Calculate precision score\n",
    "        word_precision = len(set_homophone_words.intersection(set_awe_nn_words))/len(set_homophone_words)\n",
    "\n",
    "        avg_precision += word_precision\n",
    "\n",
    "    avg_precision = avg_precision/len(words)\n",
    "    print(avg_precision)\n",
    "    \n",
    "    return phoneme_eDistance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{through , threw , carrier } Phoneme eDistance 4\n",
      "{commitment , committment , equipments } Phoneme eDistance 5\n",
      "{weights , waits , weight } Phoneme eDistance 1\n",
      "{handy , handi , napki } Phoneme eDistance 3\n",
      "{simpl , simple , should } Phoneme eDistance 6\n",
      "{strawberrys , strawberries , forgo } Phoneme eDistance 7\n",
      "{corinne , corrine , pretty } Phoneme eDistance 5\n",
      "{prioritise , prioritize , prioritized } Phoneme eDistance 1\n",
      "{pares , pairs , players } Phoneme eDistance 3\n",
      "{plain , plane , playing } Phoneme eDistance 2\n",
      "{parti , party , their } Phoneme eDistance 4\n",
      "{mails , males , names } Phoneme eDistance 2\n",
      "{acquaintance , aquaintance , gorgeous } Phoneme eDistance 7\n",
      "{peoplell , peopl , poodle } Phoneme eDistance 2\n",
      "{peoplell , people , people } Phoneme eDistance 0\n",
      "{disks , discs , these } Phoneme eDistance 5\n",
      "{franc , frank , throwing } Phoneme eDistance 4\n",
      "{weighted , waited , wasting } Phoneme eDistance 2\n",
      "{frank , franc , print } Phoneme eDistance 4\n",
      "{minimalised , minimalized , encouragement } Phoneme eDistance 10\n",
      "{focusing , focussing , forcing } Phoneme eDistance 3\n",
      "{earli , early , writing } Phoneme eDistance 5\n",
      "{waiting , weighting , willing } Phoneme eDistance 2\n",
      "{spacial , spatial , special } Phoneme eDistance 1\n",
      "{theirs , theres , guess } Phoneme eDistance 3\n",
      "{minut , minute , moved } Phoneme eDistance 4\n",
      "{purse , perce , paris } Phoneme eDistance 3\n",
      "{brake , break , break } Phoneme eDistance 0\n",
      "{right , write , backforward } Phoneme eDistance 8\n",
      "{thingys , thingies , thingies } Phoneme eDistance 0\n",
      "{hightech , hitech , sidetracked } Phoneme eDistance 5\n",
      "{centre , centr , senden } Phoneme eDistance 3\n",
      "{fourteenth , fourteenthd , putting } Phoneme eDistance 6\n",
      "{penlight , penlite , analyse } Phoneme eDistance 4\n",
      "{perce , purse , percent } Phoneme eDistance 3\n",
      "{ionizing , ionising , requested } Phoneme eDistance 8\n",
      "{summary , summery , silvery } Phoneme eDistance 3\n",
      "{slidy , slidey , sponge } Phoneme eDistance 4\n",
      "{logon , logan , mobile } Phoneme eDistance 3\n",
      "{committment , commitment , plusminus } Phoneme eDistance 7\n",
      "{paired , pared , higher } Phoneme eDistance 4\n",
      "{summarise , summarize , requir } Phoneme eDistance 6\n",
      "{summaries , summarys , familys } Phoneme eDistance 3\n",
      "{recogniser , recognizer , incrementally } Phoneme eDistance 9\n",
      "{summarys , summaries , summaries } Phoneme eDistance 0\n",
      "{clare , claire , prefer } Phoneme eDistance 5\n",
      "{spatial , spacial , special } Phoneme eDistance 1\n",
      "{piece , peace , tastes } Phoneme eDistance 4\n",
      "{comman , commen , trendly } Phoneme eDistance 7\n",
      "{comman , common , trend } Phoneme eDistance 5\n",
      "{recognize , recognise , grandad } Phoneme eDistance 8\n",
      "{cheapie , cheapy , particularly } Phoneme eDistance 11\n",
      "{sombodys , somebodys , somebodys } Phoneme eDistance 0\n",
      "{definite , definit , decimal } Phoneme eDistance 3\n",
      "{tellys , tellies , seventies } Phoneme eDistance 5\n",
      "{sells , cells , cells } Phoneme eDistance 0\n",
      "{summarizer , summariser , summarisers } Phoneme eDistance 1\n",
      "{tudumm , tudum , times } Phoneme eDistance 4\n",
      "{penlite , penlight , standalone } Phoneme eDistance 7\n",
      "{plasticine , plasticene , wrestling } Phoneme eDistance 7\n",
      "{party , parti , evalu } Phoneme eDistance 5\n",
      "{summarising , summarizing , service } Phoneme eDistance 6\n",
      "{ambience , ambiance , convince } Phoneme eDistance 5\n",
      "{greyed , grade , green } Phoneme eDistance 2\n",
      "{definit , definite , presentation } Phoneme eDistance 7\n",
      "{devision , division , attain } Phoneme eDistance 6\n",
      "{programs , programmes , relies } Phoneme eDistance 6\n",
      "{principal , principle , principle } Phoneme eDistance 0\n",
      "{stylised , stylized , stand } Phoneme eDistance 4\n",
      "{carrots , carets , covers } Phoneme eDistance 5\n",
      "{krista , christa , person } Phoneme eDistance 5\n",
      "{hitech , hightech , exhibit } Phoneme eDistance 7\n",
      "{grease , greece , please } Phoneme eDistance 3\n",
      "{coarse , course , course } Phoneme eDistance 0\n",
      "{aquaintance , acquaintance , hypotheses } Phoneme eDistance 9\n",
      "{techne , techni , apparently } Phoneme eDistance 6\n",
      "{rights , writes , device } Phoneme eDistance 4\n",
      "{specialized , specialised , amplifiers } Phoneme eDistance 7\n",
      "{greece , grease , reduces } Phoneme eDistance 6\n",
      "{threw , through , through } Phoneme eDistance 0\n",
      "{logan , logon , evaluala } Phoneme eDistance 7\n",
      "{somebodys , sombodys , maximising } Phoneme eDistance 8\n",
      "{people , peoplell , toolbar } Phoneme eDistance 6\n",
      "{people , peopl , poodle } Phoneme eDistance 2\n",
      "{handi , handy , anyway } Phoneme eDistance 5\n",
      "{differe , differ , different } Phoneme eDistance 3\n",
      "{differ , differe , different } Phoneme eDistance 3\n",
      "{memories , memorys , reminders } Phoneme eDistance 6\n",
      "{deali , dealy , frape } Phoneme eDistance 4\n",
      "{functionalities , functionalitys , functionalitys } Phoneme eDistance 0\n",
      "{summarizing , summarising , stressy } Phoneme eDistance 7\n",
      "{dependant , dependent , anything } Phoneme eDistance 7\n",
      "{focused , focussed , vantage } Phoneme eDistance 6\n",
      "{speek , speak , speaker } Phoneme eDistance 1\n",
      "{early , earli , steveo } Phoneme eDistance 5\n",
      "{standardized , standardised , cedric } Phoneme eDistance 8\n",
      "{characterisation , characterization , unintelligible } Phoneme eDistance 11\n",
      "{recognise , recognize , grandad } Phoneme eDistance 8\n",
      "{plasticene , plasticine , multimedia } Phoneme eDistance 8\n",
      "{dependent , dependant , commented } Phoneme eDistance 6\n",
      "{elans , ellens , always } Phoneme eDistance 3\n",
      "{grade , greyed , great } Phoneme eDistance 1\n",
      "{highly , highli , pioneering } Phoneme eDistance 7\n",
      "{weight , whate , weights } Phoneme eDistance 1\n",
      "{bodys , bodies , buttons } Phoneme eDistance 4\n",
      "{cheapy , cheapie , screen } Phoneme eDistance 4\n",
      "{fills , phils , skills } Phoneme eDistance 2\n",
      "{there , their , their } Phoneme eDistance 0\n",
      "{middl , middle , normally } Phoneme eDistance 5\n",
      "{peopl , peoplell , youll } Phoneme eDistance 4\n",
      "{peopl , people , people } Phoneme eDistance 0\n",
      "{summar , summer , similar } Phoneme eDistance 3\n",
      "{woohoo , whoohoo , mmhmm } Phoneme eDistance 5\n",
      "{panes , pains , cones } Phoneme eDistance 2\n",
      "{steel , steal , skill } Phoneme eDistance 2\n",
      "{gunnpeterson , gunnpetersen , exhibit } Phoneme eDistance 9\n",
      "{chilli , chile , today } Phoneme eDistance 4\n",
      "{whate , weight , mightnt } Phoneme eDistance 5\n",
      "{visualize , visualise , device } Phoneme eDistance 6\n",
      "{judgment , judgement , congratulate } Phoneme eDistance 8\n",
      "{customized , customised , customers } Phoneme eDistance 2\n",
      "{knoing , knowing , joint } Phoneme eDistance 4\n",
      "{specialised , specialized , thered } Phoneme eDistance 7\n",
      "{dealy , deali , exhibit } Phoneme eDistance 7\n",
      "{chile , chilli , silly } Phoneme eDistance 1\n",
      "{pared , paired , carried } Phoneme eDistance 3\n",
      "{rolls , roles , roles } Phoneme eDistance 0\n",
      "{metre , meter , meeting } Phoneme eDistance 2\n",
      "{moulds , molds , mould } Phoneme eDistance 1\n",
      "{slidey , slidy , slide } Phoneme eDistance 2\n",
      "{spongy , spongey , snowmanshape } Phoneme eDistance 6\n",
      "{freak , freek , investigated } Phoneme eDistance 12\n",
      "{custom , custome , accustomed } Phoneme eDistance 2\n",
      "{forrest , forest , files } Phoneme eDistance 5\n",
      "{steal , steel , still } Phoneme eDistance 1\n",
      "{cells , sells , sells } Phoneme eDistance 0\n",
      "{discs , disks , guess } Phoneme eDistance 4\n",
      "{prioritize , prioritise , spellings } Phoneme eDistance 8\n",
      "{strawberries , strawberrys , strengths } Phoneme eDistance 6\n",
      "{analyser , analyzer , motif } Phoneme eDistance 7\n",
      "{break , brake , brake } Phoneme eDistance 0\n",
      "{summariser , summarizer , summarisers } Phoneme eDistance 1\n",
      "{summer , summar , temporary } Phoneme eDistance 6\n",
      "{minute , minut , mailing } Phoneme eDistance 4\n",
      "{thingies , thingys , things } Phoneme eDistance 1\n",
      "{carets , carrots , curved } Phoneme eDistance 5\n",
      "{wooden , wouldn , winning } Phoneme eDistance 4\n",
      "{wales , whales , minus } Phoneme eDistance 5\n",
      "{wierd , weird , wouldve } Phoneme eDistance 3\n",
      "{commen , comman , exhibit } Phoneme eDistance 7\n",
      "{commen , common , misplace } Phoneme eDistance 7\n",
      "{adaptor , adapter , button } Phoneme eDistance 6\n",
      "{presen , preson , dozen } Phoneme eDistance 3\n",
      "{whales , wales , rather } Phoneme eDistance 4\n",
      "{summery , summary , forty } Phoneme eDistance 4\n",
      "{weighting , waiting , chrome } Phoneme eDistance 5\n",
      "{gunnpetersen , gunnpeterson , phonebased } Phoneme eDistance 9\n",
      "{playdoh , playdo , katie } Phoneme eDistance 4\n",
      "{pairs , pares , contain } Phoneme eDistance 6\n",
      "{edition , addition , addition } Phoneme eDistance 0\n",
      "{males , mails , metal } Phoneme eDistance 4\n",
      "{customizing , customising , technologically } Phoneme eDistance 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{characterization , characterisation , particularity } Phoneme eDistance 12\n",
      "{shuts , schutz , schutz } Phoneme eDistance 0\n",
      "{knowing , knoing , ellen } Phoneme eDistance 4\n",
      "{custome , custom , customers } Phoneme eDistance 2\n",
      "{stylized , stylised , sacrificed } Phoneme eDistance 7\n",
      "{emphasise , emphasize , nondescript } Phoneme eDistance 10\n",
      "{channelll , channel , transcribed } Phoneme eDistance 8\n",
      "{usable , useable , usersll } Phoneme eDistance 3\n",
      "{course , coarse , coolest } Phoneme eDistance 4\n",
      "{hairy , harry , battery } Phoneme eDistance 4\n",
      "{moldable , mouldable , animal } Phoneme eDistance 5\n",
      "{theres , theirs , luxurious } Phoneme eDistance 8\n",
      "{centr , centre , fruit } Phoneme eDistance 4\n",
      "{board , bored , avoidable } Phoneme eDistance 7\n",
      "{claire , clare , foldout } Phoneme eDistance 5\n",
      "{waits , weights , lights } Phoneme eDistance 2\n",
      "{pains , panes , change } Phoneme eDistance 2\n",
      "{programmes , programs , provenance } Phoneme eDistance 7\n",
      "{wouldn , wooden , wouldnt } Phoneme eDistance 1\n",
      "{waited , weighted , limited } Phoneme eDistance 5\n",
      "{plane , plain , being } Phoneme eDistance 4\n",
      "{hmhmm , hmmhmm , remember } Phoneme eDistance 4\n",
      "{peace , piece , pieces } Phoneme eDistance 2\n",
      "{speak , speek , speech } Phoneme eDistance 1\n",
      "{butts , buttes , parts } Phoneme eDistance 3\n",
      "{maarten , martin , husband } Phoneme eDistance 5\n",
      "{succe , sucks , switch } Phoneme eDistance 3\n",
      "{ellens , elans , counts } Phoneme eDistance 5\n",
      "{bodies , bodys , utterancebased } Phoneme eDistance 10\n",
      "{ambiance , ambience , regions } Phoneme eDistance 5\n",
      "{roles , rolls , rolls } Phoneme eDistance 0\n",
      "{sucks , succe , television } Phoneme eDistance 8\n",
      "{simple , simpl , subgoal } Phoneme eDistance 4\n",
      "{techni , techne , implemen } Phoneme eDistance 7\n",
      "{buttes , butts , those } Phoneme eDistance 4\n",
      "{preson , presen , luxury } Phoneme eDistance 6\n",
      "{highli , highly , camera } Phoneme eDistance 5\n",
      "{bored , board , board } Phoneme eDistance 0\n",
      "{addition , edition , interject } Phoneme eDistance 8\n",
      "{meter , metre , alienate } Phoneme eDistance 7\n",
      "{visualisation , visualization , television } Phoneme eDistance 8\n",
      "{forth , fourth , fourth } Phoneme eDistance 0\n",
      "{functionalitys , functionalities , functionalities } Phoneme eDistance 0\n",
      "{weird , wierd , wierd } Phoneme eDistance 0\n",
      "{missus , misses , traces } Phoneme eDistance 4\n",
      "{visualization , visualisation , multifunctional } Phoneme eDistance 10\n",
      "{recognizer , recogniser , exercises } Phoneme eDistance 6\n",
      "{programme , program , program } Phoneme eDistance 0\n",
      "{useable , usable , usable } Phoneme eDistance 0\n",
      "{focussed , focused , focus } Phoneme eDistance 1\n",
      "{middle , middl , newly } Phoneme eDistance 5\n",
      "{emphasize , emphasise , ideas } Phoneme eDistance 5\n",
      "{write , right , right } Phoneme eDistance 0\n",
      "{fourth , forth , formed } Phoneme eDistance 2\n",
      "{whether , weather , overhead } Phoneme eDistance 6\n",
      "{division , devision , efficient } Phoneme eDistance 4\n",
      "{misses , missus , missed } Phoneme eDistance 2\n",
      "{channel , channelll , channe } Phoneme eDistance 2\n",
      "{phils , fills , chance } Phoneme eDistance 4\n",
      "{freek , freak , click } Phoneme eDistance 3\n",
      "{tellies , tellys , times } Phoneme eDistance 3\n",
      "{standardised , standardized , standards } Phoneme eDistance 2\n",
      "{molds , moulds , models } Phoneme eDistance 4\n",
      "{wheres , wears , arrears } Phoneme eDistance 2\n",
      "{harry , hairy , thirty } Phoneme eDistance 3\n",
      "{spongey , spongy , snowmanshape } Phoneme eDistance 6\n",
      "{martin , maarten , might } Phoneme eDistance 4\n",
      "{summarize , summarise , superficial } Phoneme eDistance 7\n",
      "{wears , wheres , whereas } Phoneme eDistance 1\n",
      "{their , there , there } Phoneme eDistance 0\n",
      "{forest , forrest , forced } Phoneme eDistance 1\n",
      "{batterys , batteries , batteries } Phoneme eDistance 0\n",
      "{minimalized , minimalised , realises } Phoneme eDistance 7\n",
      "{hmmhmm , hmhmm , learn } Phoneme eDistance 5\n",
      "{judgement , judgment , potentialmeter } Phoneme eDistance 9\n",
      "{principle , principal , consult } Phoneme eDistance 6\n",
      "{tudum , tudumm , touch } Phoneme eDistance 3\n",
      "{whoohoo , woohoo , rework } Phoneme eDistance 5\n",
      "{fourteenthd , fourteenth , specimen } Phoneme eDistance 8\n",
      "{analyzer , analyser , hours } Phoneme eDistance 6\n",
      "{visualise , visualize , especialised } Phoneme eDistance 6\n",
      "{weather , whether , whether } Phoneme eDistance 0\n",
      "{playdo , playdoh , taking } Phoneme eDistance 5\n",
      "{batteries , batterys , laughters } Phoneme eDistance 3\n",
      "{ionising , ionizing , ninety } Phoneme eDistance 5\n",
      "{schutz , shuts , objects } Phoneme eDistance 5\n",
      "{customising , customizing , distinguishing } Phoneme eDistance 8\n",
      "{writes , rights , class } Phoneme eDistance 3\n",
      "{adapter , adaptor , nokia } Phoneme eDistance 6\n",
      "{mouldable , moldable , wonderful } Phoneme eDistance 5\n",
      "{common , comman , cochrane } Phoneme eDistance 2\n",
      "{common , commen , coming } Phoneme eDistance 3\n",
      "{memorys , memories , minimise } Phoneme eDistance 5\n",
      "{focussing , focusing , functon } Phoneme eDistance 6\n",
      "{christa , krista , christmas } Phoneme eDistance 2\n",
      "{corrine , corinne , committed } Phoneme eDistance 6\n",
      "{customised , customized , optimises } Phoneme eDistance 5\n",
      "{program , programme , programme } Phoneme eDistance 0\n",
      "0.11023622047244094\n"
     ]
    }
   ],
   "source": [
    "a = homophone_task(homophones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 38,\n",
       "         5: 38,\n",
       "         1: 18,\n",
       "         3: 29,\n",
       "         6: 28,\n",
       "         7: 25,\n",
       "         2: 25,\n",
       "         0: 29,\n",
       "         10: 4,\n",
       "         8: 16,\n",
       "         9: 5,\n",
       "         11: 3,\n",
       "         12: 2})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoney = BigPhoney()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_alphabets(string):\n",
    "\treturn ''.join(e for e in string if (e.isalpha() or e.isspace()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(wordpairs_df[\"word_1\"].to_list()).union(set(wordpairs_df[\"word_2\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9974\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Calculating Phonemic Expansion\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_phoneme_dict = {}\n",
    "\n",
    "#Calculate the word phonemes\n",
    "for word in words:\n",
    "    phonemes = phoney.phonize(word)\n",
    "    word_phoneme_dict[word] = filter_alphabets(phonemes)\n",
    "\n",
    "print('Finished Calculating Phonemic Expansion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel': 'P IH K S AH L',\n",
       " 'finds': 'F AY N D Z',\n",
       " 'echelons': 'EH SH AH L AA N Z',\n",
       " 'suppo': 'S UW P OW',\n",
       " 'brick': 'B R IH K',\n",
       " 'smallish': 'S M AO L IH SH',\n",
       " 'execute': 'EH K S AH K Y UW T',\n",
       " 'ignore': 'IH G N AO R',\n",
       " 'doesnt': 'D OW S AH N T',\n",
       " 'injury': 'IH N JH ER IY',\n",
       " 'lowering': 'L OW ER IH NG',\n",
       " 'innova': 'IH N OW V AH',\n",
       " 'horses': 'HH AO R S AH Z',\n",
       " 'atmosphere': 'AE T M AH S F IH R',\n",
       " 'kangaroo': 'K AE NG G ER UW',\n",
       " 'preparing': 'P R IY P EH R IH NG',\n",
       " 'refining': 'R AH F AY N IH NG',\n",
       " 'hypothetically': 'HH AY P AH TH EH T IH K L IY',\n",
       " 'beams': 'B IY M Z',\n",
       " 'sucking': 'S AH K IH NG',\n",
       " 'arbitrarily': 'AA R B IH T R EH R AH L IY',\n",
       " 'teambuilding': 'T IY M B IH L D IH NG',\n",
       " 'semisuperv': 'S EH M IY S UW P ER V',\n",
       " 'lefthander': 'L EH F T HH AE N D ER',\n",
       " 'assignments': 'AH S AY N M AH N T S',\n",
       " 'script': 'S K R IH P T',\n",
       " 'decision': 'D IH S IH ZH AH N',\n",
       " 'candidates': 'K AE N D AH D EY T S',\n",
       " 'fruitful': 'F R UW T F AH L',\n",
       " 'redundancy': 'R IH D AH N D AH N S IY',\n",
       " 'conformable': 'K AH N F AO R M AH B AH L',\n",
       " 'scroll': 'S K R OW L',\n",
       " 'menudriven': 'M EH N AH D R AY V AH N',\n",
       " 'pronounce': 'P R AH N AW N S',\n",
       " 'siemens': 'S IY M AH N Z',\n",
       " 'seasons': 'S IY Z AH N Z',\n",
       " 'synched': 'S IH N CH T',\n",
       " 'bloody': 'B L AH D IY',\n",
       " 'manufactured': 'M AE N Y AH F AE K CH ER D',\n",
       " 'tough': 'T AH F',\n",
       " 'identity': 'AY D EH N T AH T IY',\n",
       " 'adhere': 'AH D HH IH R',\n",
       " 'strangely': 'S T R EY N JH L IY',\n",
       " 'jeanneoui': 'ZH AA N IY UW IY',\n",
       " 'transistors': 'T R AE N Z IH S T ER Z',\n",
       " 'overheads': 'OW V ER HH EH D Z',\n",
       " 'deformed': 'D IH F AO R M D',\n",
       " 'allows': 'AH L AW Z',\n",
       " 'barbara': 'B AA R B ER AH',\n",
       " 'putting': 'P AH T IH NG',\n",
       " 'microstructure': 'M AY K R OW S T R AH K CH ER',\n",
       " 'interact': 'IH N T ER AE K T',\n",
       " 'slimmer': 'S L IH M ER',\n",
       " 'rotation': 'R OW T EY SH AH N',\n",
       " 'strips': 'S T R IH P S',\n",
       " 'cancel': 'K AE N S AH L',\n",
       " 'pared': 'P EH R D',\n",
       " 'leftdoubleclick': 'L EH F T D UW B L EH K L IH K',\n",
       " 'inspired': 'IH N S P AY ER D',\n",
       " 'spring': 'S P R IH NG',\n",
       " 'menus': 'M EH N Y UW Z',\n",
       " 'streamed': 'S T R IY M D',\n",
       " 'swimming': 'S W IH M IH NG',\n",
       " 'jordan': 'JH AO R D AH N',\n",
       " 'level': 'L EH V AH L',\n",
       " 'scripts': 'S K R IH P T S',\n",
       " 'raising': 'R EY Z IH NG',\n",
       " 'flops': 'F L AA P S',\n",
       " 'technologys': 'T EH K N AA L AH JH IY Z',\n",
       " 'amend': 'AH M EH N D',\n",
       " 'wheel': 'W IY L',\n",
       " 'switches': 'S W IH CH AH Z',\n",
       " 'relations': 'R IY L EY SH AH N Z',\n",
       " 'summarising': 'S AH M ER AY Z IH NG',\n",
       " 'congratulate': 'K AH N G R AE CH AH L EY T',\n",
       " 'stoplist': 'S T AA P L IH S T',\n",
       " 'prefilter': 'P R IY F IH L T ER',\n",
       " 'enhance': 'EH N HH AE N S',\n",
       " 'myself': 'M AY S EH L F',\n",
       " 'personalising': 'P ER S AA N AH L AY Z IH NG',\n",
       " 'niches': 'N IH SH IH Z',\n",
       " 'endurable': 'IH N D UH R AH B AH L',\n",
       " 'musthaves': 'M AH S T AH V Z',\n",
       " 'structural': 'S T R AH K CH ER AH L',\n",
       " 'relative': 'R EH L AH T IH V',\n",
       " 'clearcut': 'K L IH R K AH T',\n",
       " 'spider': 'S P AY D ER',\n",
       " 'brilliance': 'B R IH L Y AH N S',\n",
       " 'intent': 'IH N T EH N T',\n",
       " 'loyal': 'L OY AH L',\n",
       " 'previous': 'P R IY V IY AH S',\n",
       " 'brownie': 'B R AW N IY',\n",
       " 'leopard': 'L EH P ER D',\n",
       " 'martin': 'M AA R T AH N',\n",
       " 'contr': 'K AA N T ER',\n",
       " 'prevented': 'P R IH V EH N T IH D',\n",
       " 'badger': 'B AE JH ER',\n",
       " 'emitting': 'IH M IH T IH NG',\n",
       " 'forming': 'F AO R M IH NG',\n",
       " 'demanding': 'D IH M AE N D IH NG',\n",
       " 'realign': 'R IY AH L AY N',\n",
       " 'thats': 'TH AE T S',\n",
       " 'pairs': 'P EH R Z',\n",
       " 'navigate': 'N AE V AH G EY T',\n",
       " 'ionization': 'AY AH N AH Z EY SH AH N',\n",
       " 'outer': 'AW T ER',\n",
       " 'floating': 'F L OW T IH NG',\n",
       " 'urine': 'Y ER AH N',\n",
       " 'dedicate': 'D EH D AH K EY T',\n",
       " 'collapse': 'K AH L AE P S',\n",
       " 'events': 'IH V EH N T S',\n",
       " 'smart': 'S M AA R T',\n",
       " 'explained': 'IH K S P L EY N D',\n",
       " 'undercut': 'AH N D ER K AH T',\n",
       " 'presented': 'P R IY Z EH N T AH D',\n",
       " 'stored': 'S T AO R D',\n",
       " 'pictures': 'P IH K CH ER Z',\n",
       " 'keyword': 'K IY W ER D',\n",
       " 'priorities': 'P R AY AO R AH T IY Z',\n",
       " 'volumedown': 'V AH L UW M D AW N',\n",
       " 'vendors': 'V EH N D ER Z',\n",
       " 'reads': 'R IY D Z',\n",
       " 'conversation': 'K AA N V ER S EY SH AH N',\n",
       " 'connects': 'K AH N EH K T S',\n",
       " 'pasting': 'P EY S T IH NG',\n",
       " 'purdy': 'P ER D IY',\n",
       " 'democratically': 'D EH M AH K R AE T IH K L IY',\n",
       " 'press': 'P R EH S',\n",
       " 'booming': 'B UW M IH NG',\n",
       " 'massive': 'M AE S IH V',\n",
       " 'kates': 'K EY T S',\n",
       " 'corrine': 'K ER IY N',\n",
       " 'passionate': 'P AE SH AH N AH T',\n",
       " 'totally': 'T OW T AH L IY',\n",
       " 'timestamped': 'T AY M S T AE M P T',\n",
       " 'referring': 'R IH F ER IH NG',\n",
       " 'nonrecta': 'N AA N R EH K T AH',\n",
       " 'engines': 'EH N JH AH N Z',\n",
       " 'fixable': 'F IH K S AH B AH L',\n",
       " 'widely': 'W AY D L IY',\n",
       " 'happen': 'HH AE P AH N',\n",
       " 'antigoing': 'AE N T IY G OW IH NG',\n",
       " 'discover': 'D IH S K AH V ER',\n",
       " 'deconstructed': 'D IY K AH N S T R AH K T IH D',\n",
       " 'leeway': 'L IY W EY',\n",
       " 'kitchenware': 'K IH CH AH N W EH R',\n",
       " 'object': 'AA B JH EH K T',\n",
       " 'selfintersections': 'S EH L F IH N T ER S EH K SH AH N Z',\n",
       " 'juice': 'JH UW S',\n",
       " 'twohanded': 'T UW AH HH AE N D IH D',\n",
       " 'coordination': 'K OW AO R D AH N EY SH AH N',\n",
       " 'motto': 'M AA T OW',\n",
       " 'climb': 'K L AY M',\n",
       " 'interme': 'IH N T ER M',\n",
       " 'focusing': 'F OW K AH S IH NG',\n",
       " 'holiday': 'HH AA L AH D EY',\n",
       " 'finance': 'F AH N AE N S',\n",
       " 'remarkably': 'R IH M AA R K AH B L IY',\n",
       " 'monotonous': 'M AH N AA T AH N AH S',\n",
       " 'discs': 'D IH S K S',\n",
       " 'versa': 'V ER S AH',\n",
       " 'fluffy': 'F L AH F IY',\n",
       " 'attaching': 'AH T AE CH IH NG',\n",
       " 'unless': 'AH N L EH S',\n",
       " 'blurbs': 'B L ER B Z',\n",
       " 'temporary': 'T EH M P ER EH R IY',\n",
       " 'instinctual': 'IH N S T IH NG K CH UW AH L',\n",
       " 'bulge': 'B AH L JH',\n",
       " 'statue': 'S T AE CH UW',\n",
       " 'feasibility': 'F IY Z AH B IH L AH T IY',\n",
       " 'fighting': 'F AY T IH NG',\n",
       " 'wadadadadada': 'W AA D AH D AH D AA D AH D AH',\n",
       " 'arise': 'ER AY Z',\n",
       " 'buttonsll': 'B AH T AH N S AH L',\n",
       " 'righto': 'R AY T OW',\n",
       " 'useable': 'Y UW Z AH B AH L',\n",
       " 'telephone': 'T EH L AH F OW N',\n",
       " 'avoiding': 'AH V OY D IH NG',\n",
       " 'pixels': 'P IH K S AH L Z',\n",
       " 'veronica': 'V ER AA N IH K AH',\n",
       " 'elans': 'EH L AH N Z',\n",
       " 'royal': 'R OY AH L',\n",
       " 'trendiest': 'T R EH N D IY S T',\n",
       " 'symmet': 'S IH M AH T',\n",
       " 'slickness': 'S L IH K N AH S',\n",
       " 'locators': 'L OW K EY T ER Z',\n",
       " 'toolbar': 'T UW L B AA R',\n",
       " 'thousands': 'TH AW Z AH N D Z',\n",
       " 'dicier': 'D AY S IY ER',\n",
       " 'attacking': 'AH T AE K IH NG',\n",
       " 'submitting': 'S AH B M IH T IH NG',\n",
       " 'funkylooking': 'F AH NG K IY L UH K IH NG',\n",
       " 'grass': 'G R AE S',\n",
       " 'control': 'K AH N T R OW L',\n",
       " 'curvatures': 'K ER V AH CH ER Z',\n",
       " 'subgroup': 'S AH B G R UW P',\n",
       " 'unclear': 'AH N K L IH R',\n",
       " 'becomes': 'B IH K AH M Z',\n",
       " 'ending': 'EH N D IH NG',\n",
       " 'searching': 'S ER CH IH NG',\n",
       " 'embossing': 'EH M B AO S IH NG',\n",
       " 'reparable': 'R IY P EH R AH B AH L',\n",
       " 'repeated': 'R IH P IY T IH D',\n",
       " 'infor': 'IH N F ER',\n",
       " 'investigation': 'IH N V EH S T AH G EY SH AH N',\n",
       " 'higher': 'HH AY ER',\n",
       " 'progressed': 'P R AH G R EH S T',\n",
       " 'definition': 'D EH F AH N IH SH AH N',\n",
       " 'normal': 'N AO R M AH L',\n",
       " 'jokes': 'JH OW K S',\n",
       " 'hopefu': 'HH OW P IY F UW',\n",
       " 'appara': 'AA P AA R AH',\n",
       " 'evolve': 'IH V AA L V',\n",
       " 'refocus': 'R IY F OW K AH S',\n",
       " 'rever': 'R EH V ER',\n",
       " 'inventing': 'IH N V EH N T IH NG',\n",
       " 'access': 'AE K S EH S',\n",
       " 'fastforward': 'F AE S T F AO R W ER D',\n",
       " 'varying': 'V EH R IY IH NG',\n",
       " 'documentations': 'D AA K Y AH M EH N T EY SH AH N Z',\n",
       " 'silver': 'S IH L V ER',\n",
       " 'fruit': 'F R UW T',\n",
       " 'guillermo': 'G W IH L Y EH R M OW',\n",
       " 'lossiness': 'L AO S IY N AH S',\n",
       " 'emphasis': 'EH M F AH S IH S',\n",
       " 'somewhere': 'S AH M W EH R',\n",
       " 'radiowaves': 'R EY D IY OW W EY V Z',\n",
       " 'unaware': 'AH N AH W EH R',\n",
       " 'conceptual': 'K AH N S EH P CH UW AH L',\n",
       " 'reasonable': 'R IY Z AH N AH B AH L',\n",
       " 'rubble': 'R AH B AH L',\n",
       " 'nuclear': 'N UW K L IY ER',\n",
       " 'stutter': 'S T AH T ER',\n",
       " 'cushions': 'K UH SH AH N Z',\n",
       " 'changers': 'CH EY N JH ER Z',\n",
       " 'beefy': 'B IY F IY',\n",
       " 'focus': 'F OW K AH S',\n",
       " 'generat': 'JH EH N ER AH T',\n",
       " 'feasibly': 'F IY Z AH B L IY',\n",
       " 'homework': 'HH OW M W ER K',\n",
       " 'profiting': 'P R AA F AH T IH NG',\n",
       " 'manipulation': 'M AH N IH P Y AH L EY SH AH N',\n",
       " 'adult': 'AH D AH L T',\n",
       " 'judgment': 'JH AH JH M AH N T',\n",
       " 'inconsistency': 'IH N K AH N S IH S T AH N S IY',\n",
       " 'feedback': 'F IY D B AE K',\n",
       " 'handles': 'HH AE N D AH L Z',\n",
       " 'greyblack': 'G R EY B L AE K',\n",
       " 'finalising': 'F AY N AH L AY Z IH NG',\n",
       " 'additionally': 'AH D IH SH AH N AH L IY',\n",
       " 'public': 'P AH B L IH K',\n",
       " 'dissatisfy': 'D IH S AE T AH S F AY',\n",
       " 'spammed': 'S P AE M D',\n",
       " 'drape': 'D R EY P',\n",
       " 'clare': 'K L EH R',\n",
       " 'popularity': 'P AA P Y AH L EH R AH T IY',\n",
       " 'address': 'AE D R EH S',\n",
       " 'rubberness': 'R AH B ER N AH S',\n",
       " 'morning': 'M AO R N IH NG',\n",
       " 'omission': 'OW M IH SH AH N',\n",
       " 'array': 'ER EY',\n",
       " 'customized': 'K AH S T AH M AY Z D',\n",
       " 'epinions': 'IH P IH N Y AH N Z',\n",
       " 'spread': 'S P R EH D',\n",
       " 'thirteen': 'TH ER T IY N',\n",
       " 'consistent': 'K AH N S IH S T AH N T',\n",
       " 'respond': 'R IH S P AA N D',\n",
       " 'distress': 'D IH S T R EH S',\n",
       " 'compliments': 'K AA M P L AH M EH N T S',\n",
       " 'oscillating': 'AA S AH L EY T IH NG',\n",
       " 'hangup': 'HH AE NG G AH P',\n",
       " 'depression': 'D IH P R EH SH AH N',\n",
       " 'pullout': 'P UH L AW T',\n",
       " 'amplifier': 'AE M P L AH F AY ER',\n",
       " 'sector': 'S EH K T ER',\n",
       " 'selects': 'S AH L EH K T S',\n",
       " 'storyboard': 'S T AO R IY B AO R D',\n",
       " 'write': 'R AY T',\n",
       " 'discu': 'D IH S K Y UW',\n",
       " 'powerwise': 'P AW ER W AY Z',\n",
       " 'animals': 'AE N AH M AH L Z',\n",
       " 'augment': 'AO G M EH N T',\n",
       " 'receptor': 'R IY S EH P T ER',\n",
       " 'efficiency': 'IH F IH SH AH N S IY',\n",
       " 'store': 'S T AO R',\n",
       " 'wheat': 'W IY T',\n",
       " 'remultific': 'R IY M AH L T IH F IH K',\n",
       " 'summary': 'S AH M ER IY',\n",
       " 'microphone': 'M AY K R AH F OW N',\n",
       " 'whens': 'W IH N Z',\n",
       " 'cordless': 'K AO R D L AH S',\n",
       " 'encouraged': 'EH N K ER IH JH D',\n",
       " 'fitted': 'F IH T AH D',\n",
       " 'inverse': 'IH N V ER S',\n",
       " 'pricing': 'P R AY S IH NG',\n",
       " 'mediumlow': 'M IY D IY AH M L OW',\n",
       " 'leaderships': 'L IY D ER SH IH P S',\n",
       " 'screwed': 'S K R UW D',\n",
       " 'checking': 'CH EH K IH NG',\n",
       " 'blunt': 'B L AH N T',\n",
       " 'algorithmic': 'AE L G ER IH TH M IH K',\n",
       " 'phrases': 'F R EY Z AH Z',\n",
       " 'dryread': 'D R AY R EH D',\n",
       " 'prefer': 'P R AH F ER',\n",
       " 'bucciantini': 'B UW CH AH N T IY N IY',\n",
       " 'regret': 'R AH G R EH T',\n",
       " 'memory': 'M EH M ER IY',\n",
       " 'appliances': 'AH P L AY AH N S IH Z',\n",
       " 'disfluenc': 'D IH S F L UW AH NG K',\n",
       " 'concatenating': 'K AH N K AE T AH N EY T IH NG',\n",
       " 'hinge': 'HH IH N JH',\n",
       " 'blank': 'B L AE NG K',\n",
       " 'basics': 'B EY S IH K S',\n",
       " 'galactic': 'G AH L AE K T IH K',\n",
       " 'freedom': 'F R IY D AH M',\n",
       " 'piece': 'P IY S',\n",
       " 'complementary': 'K AA M P L AH M EH N T R IY',\n",
       " 'disappeared': 'D IH S AH P IH R D',\n",
       " 'smiles': 'S M AY L Z',\n",
       " 'extraction': 'EH K S T R AE K SH AH N',\n",
       " 'following': 'F AA L OW IH NG',\n",
       " 'subcentrally': 'S AH B S EH N T R AH L IY',\n",
       " 'inspect': 'IH N S P EH K T',\n",
       " 'pinpoint': 'P IH N P OY N T',\n",
       " 'directories': 'D AY R EH K T ER IY Z',\n",
       " 'lives': 'L IH V Z',\n",
       " 'smash': 'S M AE SH',\n",
       " 'companys': 'K AH M P AH N IY Z',\n",
       " 'prese': 'P R IY Z',\n",
       " 'verbal': 'V ER B AH L',\n",
       " 'yourself': 'Y ER S EH L F',\n",
       " 'rotsym': 'R AA T S IH M',\n",
       " 'calculation': 'K AE L K Y AH L EY SH AH N',\n",
       " 'applications': 'AE P L AH K EY SH AH N Z',\n",
       " 'television': 'T EH L AH V IH ZH AH N',\n",
       " 'ugliness': 'AH G L IY N AH S',\n",
       " 'vertical': 'V ER T IH K AH L',\n",
       " 'offer': 'AO F ER',\n",
       " 'grief': 'G R IY F',\n",
       " 'unmanageable': 'AH N M AE N IH JH AH B AH L',\n",
       " 'actors': 'AE K T ER Z',\n",
       " 'smallville': 'S M AO L V IH L',\n",
       " 'industrati': 'IH N D UW S T R AA T IY',\n",
       " 'saturation': 'S AE CH ER EY SH AH N',\n",
       " 'ontologies': 'AA N T AA L AH JH IY Z',\n",
       " 'corner': 'K AO R N ER',\n",
       " 'accentuate': 'AE K S EH N CH UW EY T',\n",
       " 'exactly': 'IH G Z AE K T L IY',\n",
       " 'maarika': 'M AA ER IY K AH',\n",
       " 'challenged': 'CH AE L AH N JH D',\n",
       " 'midjanuary': 'M IH D JH AE N Y UW EH R IY',\n",
       " 'signal': 'S IH G N AH L',\n",
       " 'diversity': 'D IH V ER S IH T IY',\n",
       " 'boxes': 'B AA K S AH Z',\n",
       " 'seminar': 'S EH M AH N AA R',\n",
       " 'motivation': 'M OW T AH V EY SH AH N',\n",
       " 'flaunt': 'F L AO N T',\n",
       " 'ionizing': 'AY AH N AY Z IH NG',\n",
       " 'durable': 'D UH R AH B AH L',\n",
       " 'incorporates': 'IH N K AO R P ER EY T S',\n",
       " 'plays': 'P L EY Z',\n",
       " 'initiation': 'IH N IH SH IY EY SH AH N',\n",
       " 'tones': 'T OW N Z',\n",
       " 'harvard': 'HH AA R V ER D',\n",
       " 'necess': 'N IY S EH S',\n",
       " 'release': 'R IY L IY S',\n",
       " 'michigan': 'M IH SH IH G AH N',\n",
       " 'listen': 'L IH S AH N',\n",
       " 'bizarre': 'B AH Z AA R',\n",
       " 'receiv': 'R IH S IY V',\n",
       " 'criterion': 'K R AY T IH R IY AH N',\n",
       " 'vertically': 'V ER T IH K L IY',\n",
       " 'rectangular': 'R EH K T AE NG G Y AH L ER',\n",
       " 'changes': 'CH EY N JH AH Z',\n",
       " 'preferably': 'P R EH F ER AH B L IY',\n",
       " 'forever': 'F ER EH V ER',\n",
       " 'utterance': 'AH T ER AH N S',\n",
       " 'emphasising': 'EH M F AH S AY Z IH NG',\n",
       " 'provin': 'P R OW V IH N',\n",
       " 'distributor': 'D IH S T R IH B Y AH T ER',\n",
       " 'inverted': 'IH N V ER T IH D',\n",
       " 'documents': 'D AA K Y AH M AH N T S',\n",
       " 'thicker': 'TH IH K ER',\n",
       " 'plugs': 'P L AH G Z',\n",
       " 'falafel': 'F AH L AA F AH L',\n",
       " 'negation': 'N AH G EY SH AH N',\n",
       " 'passage': 'P AE S AH JH',\n",
       " 'working': 'W ER K IH NG',\n",
       " 'eastpack': 'IY S T P AE K',\n",
       " 'fetching': 'F EH CH IH NG',\n",
       " 'elongated': 'IH L AO NG G EY T AH D',\n",
       " 'engineered': 'EH N JH AH N IY R D',\n",
       " 'selected': 'S AH L EH K T AH D',\n",
       " 'successfully': 'S AH K S EH S F AH L IY',\n",
       " 'lightings': 'L AY T IH NG Z',\n",
       " 'leaner': 'L IY N ER',\n",
       " 'rounder': 'R AW N D ER',\n",
       " 'frightening': 'F R AY T AH N IH NG',\n",
       " 'clearer': 'K L IH R ER',\n",
       " 'correctly': 'K ER EH K T L IY',\n",
       " 'translates': 'T R AE N Z L EY T S',\n",
       " 'niels': 'N IY L Z',\n",
       " 'literature': 'L IH T ER AH CH ER',\n",
       " 'righthand': 'R AY T HH AE N D',\n",
       " 'amuse': 'AH M Y UW Z',\n",
       " 'singlecurve': 'S IH NG G AH L K ER V',\n",
       " 'clobber': 'K L AA B ER',\n",
       " 'cleans': 'K L IY N Z',\n",
       " 'warned': 'W AO R N D',\n",
       " 'ideals': 'AY D IY L Z',\n",
       " 'offerings': 'AO F ER IH NG Z',\n",
       " 'minima': 'M IH N IY M AH',\n",
       " 'findings': 'F AY N D IH NG Z',\n",
       " 'whatev': 'W AH T EH V',\n",
       " 'prohibitive': 'P R OW HH IH B AH T IH V',\n",
       " 'physiotherapy': 'F IH Z IY OW TH EH R AH P IY',\n",
       " 'conception': 'K AH N S EH P SH AH N',\n",
       " 'directed': 'D ER EH K T AH D',\n",
       " 'representational': 'R EH P R AH Z AH N T EY SH AH N AH L',\n",
       " 'gaudy': 'G AO D IY',\n",
       " 'onedimensional': 'AA N AH D IH M EH N SH AH N AH L',\n",
       " 'weaker': 'W IY K ER',\n",
       " 'cycling': 'S AY K AH L IH NG',\n",
       " 'driven': 'D R IH V AH N',\n",
       " 'beautifully': 'B Y UW T AH F L IY',\n",
       " 'discommunicate': 'D IH S K AH M Y UW N AH K EY T',\n",
       " 'onehanded': 'AA N IH HH AE N D IH D',\n",
       " 'chitchat': 'CH IH T CH AE T',\n",
       " 'lithium': 'L IH TH IY AH M',\n",
       " 'mightve': 'M AY T V',\n",
       " 'desires': 'D IH Z AY ER Z',\n",
       " 'drunk': 'D R AH NG K',\n",
       " 'dynamic': 'D AY N AE M IH K',\n",
       " 'concrete': 'K AH N K R IY T',\n",
       " 'returns': 'R IH T ER N Z',\n",
       " 'superficial': 'S UW P ER F IH SH AH L',\n",
       " 'whisper': 'W IH S P ER',\n",
       " 'capitals': 'K AE P AH T AH L Z',\n",
       " 'selfintersection': 'S EH L F IH N T ER S EH K SH AH N',\n",
       " 'gimmicks': 'G IH M IH K S',\n",
       " 'million': 'M IH L Y AH N',\n",
       " 'compare': 'K AH M P EH R',\n",
       " 'graph': 'G R AE F',\n",
       " 'drawing': 'D R AO IH NG',\n",
       " 'expect': 'IH K S P EH K T',\n",
       " 'bathgate': 'B AE TH G EY T',\n",
       " 'recon': 'R IY K AO N',\n",
       " 'straightout': 'S T R EY T AW T',\n",
       " 'distraction': 'D IH S T R AE K SH AH N',\n",
       " 'carrier': 'K AE R IY ER',\n",
       " 'densities': 'D EH N S AH T IY Z',\n",
       " 'geometrical': 'JH IY AH M EH T R IH K AH L',\n",
       " 'touchscreen': 'T AH CH S K R IY N',\n",
       " 'dampers': 'D AE M P ER Z',\n",
       " 'telcon': 'T EH L K AH N',\n",
       " 'giraffe': 'JH ER AE F',\n",
       " 'demographic': 'D EH M AH G R AE F IH K',\n",
       " 'soccer': 'S AA K ER',\n",
       " 'childs': 'CH AY L D Z',\n",
       " 'comparatively': 'K AH M P AE R AH T IH V L IY',\n",
       " 'zebras': 'Z IY B R AH Z',\n",
       " 'availa': 'AH V EY L AH',\n",
       " 'sheet': 'SH IY T',\n",
       " 'attent': 'AH T EH N T',\n",
       " 'stiffness': 'S T IH F N AH S',\n",
       " 'buyers': 'B AY ER Z',\n",
       " 'further': 'F ER DH ER',\n",
       " 'contexts': 'K AA N T EH K S T S',\n",
       " 'matches': 'M AE CH AH Z',\n",
       " 'sketchboard': 'S K EH CH B AO R D',\n",
       " 'neglected': 'N AH G L EH K T AH D',\n",
       " 'washing': 'W AA SH IH NG',\n",
       " 'italy': 'IH T AH L IY',\n",
       " 'sabrina': 'S AH B R IY N AH',\n",
       " 'conceive': 'K AH N S IY V',\n",
       " 'trainable': 'T R EY N AH B AH L',\n",
       " 'plastics': 'P L AE S T IH K S',\n",
       " 'florescent': 'F L AO R EH S AH N T',\n",
       " 'enjoy': 'EH N JH OY',\n",
       " 'elaborated': 'IH L AE B ER EY T AH D',\n",
       " 'surfaced': 'S ER F IH S T',\n",
       " 'colourised': 'K AH L ER AY Z D',\n",
       " 'aspects': 'AE S P EH K T S',\n",
       " 'extractor': 'IH K S T R AE K T ER',\n",
       " 'clients': 'K L AY AH N T S',\n",
       " 'japanamation': 'JH AH P AE N AH M EY SH AH N',\n",
       " 'flipphone': 'F L IH P F OW N',\n",
       " 'shuffle': 'SH AH F AH L',\n",
       " 'userfriendly': 'Y UW Z ER F R EH N D L IY',\n",
       " 'cafeteria': 'K AE F AH T IH R IY AH',\n",
       " 'francine': 'F R AE N S IY N',\n",
       " 'compone': 'K AH M P OW N',\n",
       " 'arrive': 'ER AY V',\n",
       " 'perspectives': 'P ER S P EH K T IH V Z',\n",
       " 'suggests': 'S AH JH EH S T S',\n",
       " 'colourfuls': 'K AH L ER F AH L Z',\n",
       " 'diverting': 'D AY V ER T IH NG',\n",
       " 'trained': 'T R EY N D',\n",
       " 'wolfs': 'W UH L F S',\n",
       " 'operating': 'AA P ER EY T IH NG',\n",
       " 'overdone': 'OW V ER D AH N',\n",
       " 'earth': 'ER TH',\n",
       " 'penlight': 'P EH N L AY T',\n",
       " 'purposes': 'P ER P AH S AH Z',\n",
       " 'represented': 'R EH P R AH Z EH N T AH D',\n",
       " 'bearing': 'B EH R IH NG',\n",
       " 'somewhe': 'S AH M Y UW',\n",
       " 'revealed': 'R IH V IY L D',\n",
       " 'germany': 'JH ER M AH N IY',\n",
       " 'integrally': 'IH N T AH G R AH L IY',\n",
       " 'pointy': 'P OY N T IY',\n",
       " 'treble': 'T R EH B AH L',\n",
       " 'maintained': 'M EY N T EY N D',\n",
       " 'obsolete': 'AA B S AH L IY T',\n",
       " 'carrot': 'K AE R AH T',\n",
       " 'nijmegen': 'N IH JH M IH G AH N',\n",
       " 'experimented': 'IH K S P EH R AH M AH N T AH D',\n",
       " 'converts': 'K AA N V ER T S',\n",
       " 'pikachu': 'P IH K AA CH UW',\n",
       " 'detail': 'D IH T EY L',\n",
       " 'liquid': 'L IH K W AH D',\n",
       " 'retailers': 'R IY T EY L ER Z',\n",
       " 'revisiting': 'R IY V IH Z IH T IH NG',\n",
       " 'future': 'F Y UW CH ER',\n",
       " 'functioning': 'F AH NG K SH AH N IH NG',\n",
       " 'thoughtful': 'TH AO T F AH L',\n",
       " 'whistle': 'W IH S AH L',\n",
       " 'revolutionised': 'R EH V AH L UW SH AH N AY Z D',\n",
       " 'diodes': 'D AY OW D Z',\n",
       " 'cheesed': 'CH IY Z D',\n",
       " 'whats': 'W AH T S',\n",
       " 'frequencies': 'F R IY K W AH N S IY Z',\n",
       " 'sedentary': 'S EH D AH N T EH R IY',\n",
       " 'timewarp': 'T AY M W AO R P',\n",
       " 'heres': 'HH IH R Z',\n",
       " 'regardless': 'R AH G AA R D L AH S',\n",
       " 'reliability': 'R IY L AY AH B IH L AH T IY',\n",
       " 'receiving': 'R AH S IY V IH NG',\n",
       " 'constructs': 'K AH N S T R AH K T S',\n",
       " 'approx': 'AH P R AA K S',\n",
       " 'shellshaped': 'SH EH L SH EY P T',\n",
       " 'subliminal': 'S AH B L IH M IH N AH L',\n",
       " 'grease': 'G R IY S',\n",
       " 'researched': 'R IY S ER CH T',\n",
       " 'quadrants': 'K W AA D R AH N T S',\n",
       " 'interchan': 'IH N T ER CH AE N',\n",
       " 'naive': 'N AY IY V',\n",
       " 'sounder': 'S AW N D ER',\n",
       " 'cleanable': 'K L IY N AH B AH L',\n",
       " 'comes': 'K AH M Z',\n",
       " 'invested': 'IH N V EH S T AH D',\n",
       " 'context': 'K AA N T EH K S T',\n",
       " 'impending': 'IH M P EH N D IH NG',\n",
       " 'stupid': 'S T UW P AH D',\n",
       " 'hardcoded': 'HH AA R D K OW D IH D',\n",
       " 'power': 'P AW ER',\n",
       " 'attended': 'AH T EH N D AH D',\n",
       " 'effec': 'EH F EH K',\n",
       " 'magnetic': 'M AE G N EH T IH K',\n",
       " 'steadily': 'S T EH D AH L IY',\n",
       " 'repeating': 'R IH P IY T IH NG',\n",
       " 'amusing': 'AH M Y UW Z IH NG',\n",
       " 'topranked': 'T AH P R AE NG K T',\n",
       " 'tutorings': 'T UW T ER IH NG Z',\n",
       " 'seems': 'S IY M Z',\n",
       " 'awkward': 'AO K W ER D',\n",
       " 'doublecurve': 'D AH B AH L K ER V',\n",
       " 'maximising': 'M AE K S AH M AY Z IH NG',\n",
       " 'populated': 'P AA P Y AH L EY T AH D',\n",
       " 'bounce': 'B AW N S',\n",
       " 'juergen': 'Y ER G AH N',\n",
       " 'changeable': 'CH EY N JH AH B AH L',\n",
       " 'detailed': 'D IH T EY L D',\n",
       " 'compromise': 'K AA M P R AH M AY Z',\n",
       " 'pinging': 'P IH NG IH NG',\n",
       " 'encounter': 'IH N K AW N T ER',\n",
       " 'demokraphi': 'D IH M OW K R AA F IY',\n",
       " 'aggressive': 'AH G R EH S IH V',\n",
       " 'therell': 'TH EH R AH L',\n",
       " 'grade': 'G R EY D',\n",
       " 'success': 'S AH K S EH S',\n",
       " 'cognitive': 'K AA G N IH T IH V',\n",
       " 'highend': 'HH AY AH N D',\n",
       " 'evolved': 'IH V AA L V D',\n",
       " 'sixes': 'S IH K S IH Z',\n",
       " 'discriminate': 'D IH S K R IH M AH N EY T',\n",
       " 'ruled': 'R UW L D',\n",
       " 'versatile': 'V ER S AH T AH L',\n",
       " 'closest': 'K L OW S AH S T',\n",
       " 'detachable': 'D IH T AE CH AH B AH L',\n",
       " 'debate': 'D AH B EY T',\n",
       " 'attempting': 'AH T EH M P T IH NG',\n",
       " 'respet': 'R IH S P EH T',\n",
       " 'cancer': 'K AE N S ER',\n",
       " 'expression': 'IH K S P R EH SH AH N',\n",
       " 'breaking': 'B R EY K IH NG',\n",
       " 'rationale': 'R AE SH AH N AE L',\n",
       " 'october': 'AA K T OW B ER',\n",
       " 'relating': 'R IH L EY T IH NG',\n",
       " 'earli': 'ER L IY',\n",
       " 'joining': 'JH OY N IH NG',\n",
       " 'respondents': 'R IH S P AA N D AH N T S',\n",
       " 'carved': 'K AA R V D',\n",
       " 'alkaline': 'AE L K AH L AY N',\n",
       " 'disturbed': 'D IH S T ER B D',\n",
       " 'unstructured': 'AH N S T R AH K SH ER D',\n",
       " 'analyzer': 'AE N AH L AY Z ER',\n",
       " 'sleeping': 'S L IY P IH NG',\n",
       " 'evalua': 'EH V AA L Y UW AH',\n",
       " 'robins': 'R AA B AH N Z',\n",
       " 'version': 'V ER ZH AH N',\n",
       " 'concretely': 'K AA N K R IY T L IY',\n",
       " 'handi': 'HH AE N D IY',\n",
       " 'channelhopping': 'CH AE N AH L HH AA P IH NG',\n",
       " 'prioritized': 'P R AY AO R AH T AY Z D',\n",
       " 'tendency': 'T EH N D AH N S IY',\n",
       " 'sheep': 'SH IY P',\n",
       " 'laugh': 'L AE F',\n",
       " 'rapidly': 'R AE P AH D L IY',\n",
       " 'datas': 'D EY T AH Z',\n",
       " 'electrici': 'IH L EH K T R IY CH IY',\n",
       " 'requiremen': 'R IY K W AY R M AH N',\n",
       " 'recognisable': 'R EH K AH G N AY Z AH B AH L',\n",
       " 'whether': 'W EH DH ER',\n",
       " 'twiddle': 'T W IH D AH L',\n",
       " 'useful': 'Y UW S F AH L',\n",
       " 'funtionability': 'F AH N SH AH N AH B IH L IH T IY',\n",
       " 'uniquely': 'Y UW N IY K L IY',\n",
       " 'lately': 'L EY T L IY',\n",
       " 'rarity': 'R EH R AH T IY',\n",
       " 'increase': 'IH N K R IY S',\n",
       " 'implement': 'IH M P L AH M AH N T',\n",
       " 'weeny': 'W IY N IY',\n",
       " 'reminder': 'R IY M AY N D ER',\n",
       " 'limits': 'L IH M AH T S',\n",
       " 'disaster': 'D IH Z AE S T ER',\n",
       " 'scented': 'S EH N T IH D',\n",
       " 'liking': 'L AY K IH NG',\n",
       " 'gambling': 'G AE M B AH L IH NG',\n",
       " 'walking': 'W AO K IH NG',\n",
       " 'kittycat': 'K IH T IY K AE T',\n",
       " 'intro': 'IH N T R OW',\n",
       " 'catchy': 'K AE CH IY',\n",
       " 'joysticks': 'JH OY S T IH K S',\n",
       " 'informa': 'IH N F AO R M AH',\n",
       " 'reconsider': 'R IY K AH N S IH D ER',\n",
       " 'wonderful': 'W AH N D ER F AH L',\n",
       " 'wraps': 'R AE P S',\n",
       " 'agreeing': 'AH G R IY IH NG',\n",
       " 'prioritising': 'P R AY AO R AH T AY Z IH NG',\n",
       " 'burritos': 'B ER IY T OW S',\n",
       " 'multiformat': 'M AH L T IY F AO R M AH T',\n",
       " 'winner': 'W IH N ER',\n",
       " 'schedules': 'S K EH JH UH L Z',\n",
       " 'irritate': 'IH R IH T EY T',\n",
       " 'acupressure': 'AE K Y AH P R EH SH ER',\n",
       " 'books': 'B UH K S',\n",
       " 'mistake': 'M IH S T EY K',\n",
       " 'spats': 'S P AE T S',\n",
       " 'industry': 'IH N D AH S T R IY',\n",
       " 'loosen': 'L UW S AH N',\n",
       " 'decent': 'D IY S AH N T',\n",
       " 'clunker': 'K L AH NG K ER',\n",
       " 'financi': 'F IH N AE N CH IY',\n",
       " 'record': 'R AH K AO R D',\n",
       " 'blockbooked': 'B L AA K B UH K T',\n",
       " 'vulnerab': 'V AH L N ER AE B',\n",
       " 'sincerely': 'S IH N S IH R L IY',\n",
       " 'together': 'T AH G EH DH ER',\n",
       " 'betty': 'B EH T IY',\n",
       " 'kingdom': 'K IH NG D AH M',\n",
       " 'fruitiness': 'F R UW T IY N AH S',\n",
       " 'imagi': 'IH M AA G IY',\n",
       " 'traversal': 'T R AH V ER S AH L',\n",
       " 'emailing': 'IY M EY L IH NG',\n",
       " 'extraordinary': 'EH K S T R AH AO R D AH N EH R IY',\n",
       " 'fingernail': 'F IH NG G ER N EY L',\n",
       " 'experimental': 'IH K S P EH R IH M EH N T AH L',\n",
       " 'champagne': 'SH AE M P EY N',\n",
       " 'corporation': 'K AO R P ER EY SH AH N',\n",
       " 'ineffective': 'IH N IH F EH K T IH V',\n",
       " 'fascias': 'F AE SH AH S',\n",
       " 'still': 'S T IH L',\n",
       " 'magnitude': 'M AE G N AH T UW D',\n",
       " 'grandkids': 'G R AE N D K IH D Z',\n",
       " 'repeats': 'R IH P IY T S',\n",
       " 'pains': 'P EY N Z',\n",
       " 'nonremovable': 'N AA N R IH M UW V AH B AH L',\n",
       " 'discursive': 'D IH S K ER S IH V',\n",
       " 'congratulations': 'K AH N G R AE CH AH L EY SH AH N Z',\n",
       " 'weights': 'W EY T S',\n",
       " 'published': 'P AH B L IH SH T',\n",
       " 'short': 'SH AO R T',\n",
       " 'nokia': 'N OW K IY AH',\n",
       " 'snout': 'S N AW T',\n",
       " 'monochrome': 'M AA N AH K R OW M',\n",
       " 'upgraded': 'AH P G R EY D AH D',\n",
       " 'manipulating': 'M AH N IH P Y AH L EY T IH NG',\n",
       " 'gravity': 'G R AE V AH T IY',\n",
       " 'yourselves': 'Y UH R S EH L V Z',\n",
       " 'desktop': 'D EH S K T AA P',\n",
       " 'silvers': 'S IH L V ER Z',\n",
       " 'heard': 'HH ER D',\n",
       " 'voiceover': 'V OY S OW V ER',\n",
       " 'experiments': 'IH K S P EH R AH M AH N T S',\n",
       " 'explanatory': 'IH K S P L AE N AH T AO R IY',\n",
       " 'ought': 'AO T',\n",
       " 'gaurav': 'G AO R AA V',\n",
       " 'coaches': 'K OW CH IH Z',\n",
       " 'idiom': 'IH D IY AH M',\n",
       " 'continuity': 'K AA N T AH N UW AH T IY',\n",
       " 'revisit': 'R IY V IH Z IH T',\n",
       " 'platform': 'P L AE T F AO R M',\n",
       " 'misgroup': 'M IH S G R UW P',\n",
       " 'suede': 'S W EY D',\n",
       " 'megafile': 'M EH G AH F AY L',\n",
       " 'inwards': 'IH N W ER D Z',\n",
       " 'contrary': 'K AA N T R EH R IY',\n",
       " 'gonna': 'G AA N AH',\n",
       " 'table': 'T EY B AH L',\n",
       " 'proba': 'P R OW B AA',\n",
       " 'comfier': 'K AH M F IY ER',\n",
       " 'primitive': 'P R IH M AH T IH V',\n",
       " 'browser': 'B R AW Z ER',\n",
       " 'scarily': 'S K EH R AH L IY',\n",
       " 'drawable': 'D R AA W AH B AH L',\n",
       " 'minut': 'M IH N AH T',\n",
       " 'creature': 'K R IY CH ER',\n",
       " 'timer': 'T AY M ER',\n",
       " 'tendonditis': 'T EH N D AA N D AH T IH S',\n",
       " 'visit': 'V IH Z IH T',\n",
       " 'doubleclick': 'D AH B L IH K L IH K',\n",
       " 'plain': 'P L EY N',\n",
       " 'smooth': 'S M UW DH',\n",
       " 'simulates': 'S IH M Y AH L EY T S',\n",
       " 'adapted': 'AH D AE P T IH D',\n",
       " 'particular': 'P ER T IH K Y AH L ER',\n",
       " 'succe': 'S AH K S',\n",
       " 'dropdown': 'D R AA P D AW N',\n",
       " 'coding': 'K OW D IH NG',\n",
       " 'committing': 'K AH M IH T IH NG',\n",
       " 'swords': 'S AO R D Z',\n",
       " 'behave': 'B IH HH EY V',\n",
       " 'vertex': 'V ER T EH K S',\n",
       " 'vector': 'V EH K T ER',\n",
       " 'actively': 'AE K T IH V L IY',\n",
       " 'stylist': 'S T AY L IH S T',\n",
       " 'gesture': 'JH EH S CH ER',\n",
       " 'inquire': 'IH N K W AY R',\n",
       " 'younger': 'Y AH NG G ER',\n",
       " 'poking': 'P OW K IH NG',\n",
       " 'existence': 'EH G Z IH S T AH N S',\n",
       " 'characteris': 'K EH R AH K T EH R IH S',\n",
       " 'jirun': 'JH IH R AH N',\n",
       " 'cambodia': 'K AE M B OW D IY AH',\n",
       " 'turned': 'T ER N D',\n",
       " 'screams': 'S K R IY M Z',\n",
       " 'viewing': 'V Y UW IH NG',\n",
       " 'devices': 'D IH V AY S AH Z',\n",
       " 'leftclick': 'L EH F T K L IH K',\n",
       " 'coffee': 'K AA F IY',\n",
       " 'controllers': 'K AH N T R OW L ER Z',\n",
       " 'impulse': 'IH M P AH L S',\n",
       " 'newsflash': 'N UW S F L AE SH',\n",
       " 'texture': 'T EH K S CH ER',\n",
       " 'tshaped': 'CH EY P T',\n",
       " 'caramel': 'K EH R AH M AH L',\n",
       " 'spellcheck': 'S P EH L CH EH K',\n",
       " 'acknowledged': 'AE K N AA L IH JH D',\n",
       " 'especialised': 'IH S P EH SH AH L AY Z D',\n",
       " 'crotch': 'K R AA CH',\n",
       " 'horrendously': 'HH AO R EH N D AH S L IY',\n",
       " 'follows': 'F AA L OW Z',\n",
       " 'reduces': 'R IH D UW S IH Z',\n",
       " 'reporting': 'R IY P AO R T IH NG',\n",
       " 'temps': 'T EH M P S',\n",
       " 'stage': 'S T EY JH',\n",
       " 'timeframe': 'T AY M F R EY M',\n",
       " 'smarter': 'S M AA R T ER',\n",
       " 'olivier': 'OW L IH V IY EY',\n",
       " 'price': 'P R AY S',\n",
       " 'selfdestructible': 'S EH L F D IH S T R AH K T IH B AH L',\n",
       " 'energy': 'EH N ER JH IY',\n",
       " 'abilities': 'AH B IH L AH T IY Z',\n",
       " 'unsupportive': 'AH N S AH P AO R T IH V',\n",
       " 'cease': 'S IY S',\n",
       " 'midair': 'M IH D EH R',\n",
       " 'specify': 'S P EH S AH F AY',\n",
       " 'goodlooking': 'G UH D L UH K IH NG',\n",
       " 'nonissue': 'N AA N IH S UW',\n",
       " 'hands': 'HH AE N D Z',\n",
       " 'childishly': 'CH AY L D IH SH L IY',\n",
       " 'appreciated': 'AH P R IY SH IY EY T IH D',\n",
       " 'functionally': 'F AH NG K SH AH N AH L IY',\n",
       " 'extension': 'IH K S T EH N SH AH N',\n",
       " 'practicalit': 'P R AE K T IH K AE L AH T',\n",
       " 'increa': 'IH N K R IY AH',\n",
       " 'investi': 'IH N V EH S T IY',\n",
       " 'aubergines': 'AW B ER JH IY N Z',\n",
       " 'thumbs': 'TH AH M Z',\n",
       " 'taiwan': 'T AY W AA N',\n",
       " 'duplicating': 'D UW P L IH K EY T IH NG',\n",
       " 'restriction': 'R IY S T R IH K SH AH N',\n",
       " 'sponginess': 'S P AH N JH IY N AH S',\n",
       " 'vasilis': 'V AH S IH L AH S',\n",
       " 'frustrates': 'F R AH S T R EY T S',\n",
       " 'reload': 'R IY L OW D',\n",
       " 'intuitively': 'IH N T UW IH T IH V L IY',\n",
       " 'where': 'W EH R',\n",
       " 'preclude': 'P R IH K L UW D',\n",
       " 'audible': 'AA D AH B AH L',\n",
       " 'hills': 'HH IH L Z',\n",
       " 'plasticky': 'P L AH S T IH K IY',\n",
       " 'telewest': 'T EH L AH W EH S T',\n",
       " 'software': 'S AO F T W EH R',\n",
       " 'grand': 'G R AE N D',\n",
       " 'snapshot': 'S N AE P SH AA T',\n",
       " 'living': 'L IH V IH NG',\n",
       " 'thosell': 'TH AA S AH L',\n",
       " 'incorporated': 'IH N K AO R P ER EY T IH D',\n",
       " 'beast': 'B IY S T',\n",
       " 'burnt': 'B ER N T',\n",
       " 'unmarked': 'AH N M AA R K T',\n",
       " 'facials': 'F EY SH AH L Z',\n",
       " 'optimist': 'AA P T IH M IH S T',\n",
       " 'unders': 'AH N D ER Z',\n",
       " 'participate': 'P AA R T IH S AH P EY T',\n",
       " 'calculate': 'K AE L K Y AH L EY T',\n",
       " 'pseudoface': 'S UW D OW F EY S',\n",
       " 'learning': 'L ER N IH NG',\n",
       " 'consu': 'K AA N S UW',\n",
       " 'updates': 'AH P D EY T S',\n",
       " 'socket': 'S AA K AH T',\n",
       " 'thousand': 'TH AW Z AH N D',\n",
       " 'handle': 'HH AE N D AH L',\n",
       " 'strawberrys': 'S T R AO B EH R IY Z',\n",
       " 'bebacks': 'B IY B AE K S',\n",
       " 'fiddle': 'F IH D AH L',\n",
       " 'frankenstein': 'F R AE NG K AH N S T AY N',\n",
       " 'microphoneesque': 'M AY K R OW F OW N IY S K',\n",
       " 'perhaps': 'P ER HH AE P S',\n",
       " 'contacts': 'K AA N T AE K T S',\n",
       " 'lunchtime': 'L AH N CH T AY M',\n",
       " 'generates': 'JH EH N ER EY T S',\n",
       " 'adaptive': 'AH D AE P T IH V',\n",
       " 'naming': 'N EY M IH NG',\n",
       " 'automated': 'AO T AH M EY T IH D',\n",
       " 'ceilings': 'S IY L IH NG Z',\n",
       " 'whiteboard': 'W AY T B AO R D',\n",
       " 'coach': 'K OW CH',\n",
       " 'wanker': 'W AA NG K ER',\n",
       " 'multifunction': 'M AH L T IY F AH NG K SH AH N',\n",
       " 'ceefax': 'S IY F AE K S',\n",
       " 'floods': 'F L AH D Z',\n",
       " 'financial': 'F AH N AE N SH AH L',\n",
       " 'nineteenth': 'N AY N T IY N TH',\n",
       " 'grain': 'G R EY N',\n",
       " 'sides': 'S AY D Z',\n",
       " 'getting': 'G EH T IH NG',\n",
       " 'envisaged': 'EH N V IH Z IH JH D',\n",
       " 'dinner': 'D IH N ER',\n",
       " 'exposition': 'EH K S P AH Z IH SH AH N',\n",
       " 'capacitor': 'K AH P AE S AH T ER',\n",
       " 'bound': 'B AW N D',\n",
       " 'pineapple': 'P AY N AE P AH L',\n",
       " 'agrees': 'AH G R IY Z',\n",
       " 'surfing': 'S ER F IH NG',\n",
       " 'sketch': 'S K EH CH',\n",
       " 'amounts': 'AH M AW N T S',\n",
       " 'surprised': 'S ER P R AY Z D',\n",
       " 'reall': 'R IY L',\n",
       " 'played': 'P L EY D',\n",
       " 'rightclicking': 'R AY T K L IH K IH NG',\n",
       " 'highly': 'HH AY L IY',\n",
       " 'benchmarking': 'B EH N CH M AA R K IH NG',\n",
       " 'eeyore': 'IY Y AO R',\n",
       " 'indiv': 'IH N D IH V',\n",
       " 'hellbent': 'HH EH L B AH N T',\n",
       " 'vegetabley': 'V EH JH AH T AH B L IY',\n",
       " 'teaser': 'T IY Z ER',\n",
       " 'housing': 'HH AW Z IH NG',\n",
       " 'cloak': 'K L OW K',\n",
       " 'perfectly': 'P ER F AH K T L IY',\n",
       " 'fight': 'F AY T',\n",
       " 'overhead': 'OW V ER HH EH D',\n",
       " 'swiftly': 'S W IH F T L IY',\n",
       " 'foundation': 'F AW N D EY SH AH N',\n",
       " 'respective': 'R IH S P EH K T IH V',\n",
       " 'numbering': 'N AH M B ER IH NG',\n",
       " 'offended': 'AH F EH N D AH D',\n",
       " 'louder': 'L AW D ER',\n",
       " 'dumps': 'D AH M P S',\n",
       " 'specifies': 'S P EH S AH F AY Z',\n",
       " 'wearing': 'W EH R IH NG',\n",
       " 'survey': 'S ER V EY',\n",
       " 'industr': 'IH N D AH S T ER',\n",
       " 'thinking': 'TH IH NG K IH NG',\n",
       " 'leaning': 'L IY N IH NG',\n",
       " 'everyone': 'EH V R IY W AH N',\n",
       " 'indepen': 'IH N D IH P AH N',\n",
       " 'phonetically': 'F AH N EH T IH K L IY',\n",
       " 'thinks': 'TH IH NG K S',\n",
       " 'cellphone': 'S EH L F OW N',\n",
       " 'graphs': 'G R AE F S',\n",
       " 'design': 'D IH Z AY N',\n",
       " 'hmhmm': 'HH AH M EH M',\n",
       " 'dialects': 'D AY AH L EH K T S',\n",
       " 'beaver': 'B IY V ER',\n",
       " 'circuitry': 'S ER K AH T R IY',\n",
       " 'imagine': 'IH M AE JH AH N',\n",
       " 'consists': 'K AH N S IH S T S',\n",
       " 'weigh': 'W EY',\n",
       " 'trains': 'T R EY N Z',\n",
       " 'monotonic': 'M AA N AH T AA N IH K',\n",
       " 'accomplish': 'AH K AA M P L IH SH',\n",
       " 'rasta': 'R AE S T AH',\n",
       " 'flaws': 'F L AO Z',\n",
       " 'contrast': 'K AA N T R AE S T',\n",
       " 'eaten': 'IY T AH N',\n",
       " 'prolong': 'P R AH L AO NG',\n",
       " 'belgium': 'B EH L JH AH M',\n",
       " 'worker': 'W ER K ER',\n",
       " 'twenty': 'T W EH N T IY',\n",
       " 'frequent': 'F R IY K W AH N T',\n",
       " 'remem': 'R EH M AH M',\n",
       " 'remarking': 'R IH M AA R K IH NG',\n",
       " 'major': 'M EY JH ER',\n",
       " 'softer': 'S AA F T ER',\n",
       " 'pushable': 'P UH SH AH B AH L',\n",
       " 'illumination': 'IH L UW M AH N EY SH AH N',\n",
       " 'costly': 'K AA S T L IY',\n",
       " 'correlation': 'K AO R AH L EY SH AH N',\n",
       " 'tinkering': 'T IH NG K ER IH NG',\n",
       " 'queries': 'K W IH R IY Z',\n",
       " 'coordinated': 'K OW AO R D AH N EY T IH D',\n",
       " 'semester': 'S AH M EH S T ER',\n",
       " 'jogdial': 'JH AA G D IY AH L',\n",
       " 'refresh': 'R IH F R EH SH',\n",
       " 'synchs': 'S IH N CH IH Z',\n",
       " 'quantify': 'K W AA N T IH F AY',\n",
       " 'tiered': 'T IY R D',\n",
       " 'timetables': 'T AY M T EY B AH L Z',\n",
       " 'summarisers': 'S AH M ER AY Z ER Z',\n",
       " 'pensioners': 'P EH N SH AH N ER Z',\n",
       " 'hilarious': 'HH IH L EH R IY AH S',\n",
       " 'babas': 'B AA B AH Z',\n",
       " 'light': 'L AY T',\n",
       " 'comparable': 'K AA M P R AH B AH L',\n",
       " 'versus': 'V ER S AH S',\n",
       " 'teenager': 'T IY N EY JH ER',\n",
       " 'neater': 'N IY T ER',\n",
       " 'reset': 'R IY S EH T',\n",
       " 'tightly': 'T AY T L IY',\n",
       " 'clouds': 'K L AW D Z',\n",
       " 'alrigh': 'AE L R AY',\n",
       " 'softwares': 'S AO F W EH R Z',\n",
       " 'engineers': 'EH N JH AH N IH R Z',\n",
       " 'photocopy': 'F OW T OW K AA P IY',\n",
       " 'macro': 'M AE K R OW',\n",
       " 'suppl': 'S AH P AH L',\n",
       " 'sideways': 'S AY D W EY Z',\n",
       " 'worldwide': 'W ER L D W AY D',\n",
       " 'untitled': 'AH N T AY T AH L D',\n",
       " 'bumblebeeish': 'B AH M B AH L B IY SH',\n",
       " 'reckoning': 'R EH K AH N IH NG',\n",
       " 'functionalitywise': 'F AH NG K SH AH N AH L IH T AH T IH W IH Z',\n",
       " 'carryon': 'K AE R IY AH N',\n",
       " 'advantageous': 'AE D V AH N T EY JH AH S',\n",
       " 'directions': 'D ER EH K SH AH N Z',\n",
       " 'bulges': 'B AH L JH IH Z',\n",
       " 'sizable': 'S AY Z AH B AH L',\n",
       " 'interfacing': 'IH N T ER F EY S IH NG',\n",
       " 'japan': 'JH AH P AE N',\n",
       " 'seeds': 'S IY D Z',\n",
       " 'saver': 'S EY V ER',\n",
       " 'pennies': 'P EH N IY Z',\n",
       " 'split': 'S P L IH T',\n",
       " 'cooler': 'K UW L ER',\n",
       " 'eighty': 'EY T IY',\n",
       " 'infro': 'IH N F R OW',\n",
       " 'states': 'S T EY T S',\n",
       " 'reintegrate': 'R IY IH N T AH G R EY T',\n",
       " 'novel': 'N AA V AH L',\n",
       " 'grossly': 'G R OW S L IY',\n",
       " 'bullet': 'B UH L AH T',\n",
       " 'mysterious': 'M IH S T IH R IY AH S',\n",
       " 'docking': 'D AA K IH NG',\n",
       " 'surrounding': 'S ER AW N D IH NG',\n",
       " 'introduce': 'IH N T R AH D UW S',\n",
       " 'location': 'L OW K EY SH AH N',\n",
       " 'whoever': 'HH UW EH V ER',\n",
       " 'interject': 'IH N T ER JH EH K T',\n",
       " 'colin': 'K OW L IH N',\n",
       " 'dormant': 'D AO R M AH N T',\n",
       " 'surprisingly': 'S ER P R AY Z IH NG L IY',\n",
       " 'websites': 'W EH B S AY T S',\n",
       " 'quasar': 'K W EY Z AA R',\n",
       " 'sturdy': 'S T ER D IY',\n",
       " 'wouldve': 'W UH L D V',\n",
       " 'association': 'AH S OW S IY EY SH AH N',\n",
       " ...}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_phoneme_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/word_phoneme_dict.npy\",word_phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"Data/word_phoneme_dict.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P IH K S AH L'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.item().get(\"pixel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_score(ser):\n",
    "\treturn 10 - min(10,ser/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_ser = pd.read_csv('Data/wordpairs_test_with_ser.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_embedding_similarity = pd.read_csv('Data/wordpairs_test_embedding_similarity.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>orthographic_edit_distance</th>\n",
       "      <th>phonetic_edit_distance</th>\n",
       "      <th>word_1_phonetic_ser</th>\n",
       "      <th>word_2_phonetic_ser</th>\n",
       "      <th>word_1_orthographic_ser</th>\n",
       "      <th>word_2_orthographic_ser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>share</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complexity</td>\n",
       "      <td>simple</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chance</td>\n",
       "      <td>probably</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covered</td>\n",
       "      <td>sheet</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shifting</td>\n",
       "      <td>topics</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_1    word_2  orthographic_edit_distance  phonetic_edit_distance  \\\n",
       "0   marketing     share                           7                       7   \n",
       "1  complexity    simple                           6                       8   \n",
       "2      chance  probably                           7                       8   \n",
       "3     covered     sheet                           5                       5   \n",
       "4    shifting    topics                           7                       6   \n",
       "\n",
       "   word_1_phonetic_ser  word_2_phonetic_ser  word_1_orthographic_ser  \\\n",
       "0             0.777778             1.400000                 0.777778   \n",
       "1             0.800000             1.333333                 0.600000   \n",
       "2             1.333333             1.000000                 1.166667   \n",
       "3             0.714286             1.000000                 0.714286   \n",
       "4             0.750000             1.000000                 0.875000   \n",
       "\n",
       "   word_2_orthographic_ser  \n",
       "0                 1.400000  \n",
       "1                 1.000000  \n",
       "2                 0.875000  \n",
       "3                 1.000000  \n",
       "4                 1.166667  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_ser.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_1_phonetic_ser</th>\n",
       "      <th>word_2_phonetic_ser</th>\n",
       "      <th>word_1_orthographic_ser</th>\n",
       "      <th>word_2_orthographic_ser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>share</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complexity</td>\n",
       "      <td>simple</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chance</td>\n",
       "      <td>probably</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covered</td>\n",
       "      <td>sheet</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shifting</td>\n",
       "      <td>topics</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547915</th>\n",
       "      <td>pilots</td>\n",
       "      <td>producing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547916</th>\n",
       "      <td>devices</td>\n",
       "      <td>forcing</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547917</th>\n",
       "      <td>group</td>\n",
       "      <td>smartboards</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547918</th>\n",
       "      <td>agreed</td>\n",
       "      <td>compress</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547919</th>\n",
       "      <td>display</td>\n",
       "      <td>handy</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1547920 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_1       word_2  word_1_phonetic_ser  word_2_phonetic_ser  \\\n",
       "0         marketing        share             0.777778             1.400000   \n",
       "1        complexity       simple             0.800000             1.333333   \n",
       "2            chance     probably             1.333333             1.000000   \n",
       "3           covered        sheet             0.714286             1.000000   \n",
       "4          shifting       topics             0.750000             1.000000   \n",
       "...             ...          ...                  ...                  ...   \n",
       "1547915      pilots    producing             1.000000             0.666667   \n",
       "1547916     devices      forcing             0.857143             0.857143   \n",
       "1547917       group  smartboards             1.800000             0.818182   \n",
       "1547918      agreed     compress             1.000000             0.750000   \n",
       "1547919     display        handy             0.857143             1.200000   \n",
       "\n",
       "         word_1_orthographic_ser  word_2_orthographic_ser  \n",
       "0                       0.777778                 1.400000  \n",
       "1                       0.600000                 1.000000  \n",
       "2                       1.166667                 0.875000  \n",
       "3                       0.714286                 1.000000  \n",
       "4                       0.875000                 1.166667  \n",
       "...                          ...                      ...  \n",
       "1547915                 1.333333                 0.888889  \n",
       "1547916                 1.000000                 1.000000  \n",
       "1547917                 1.800000                 0.818182  \n",
       "1547918                 1.000000                 0.750000  \n",
       "1547919                 0.857143                 1.200000  \n",
       "\n",
       "[1547920 rows x 6 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del wordpairs_ser[\"orthographic_edit_distance\"]\n",
    "del wordpairs_ser[\"phonetic_edit_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_ser.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>share</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complexity</td>\n",
       "      <td>simple</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chance</td>\n",
       "      <td>probably</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covered</td>\n",
       "      <td>sheet</td>\n",
       "      <td>0.005748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shifting</td>\n",
       "      <td>topics</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547915</th>\n",
       "      <td>pilots</td>\n",
       "      <td>producing</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547916</th>\n",
       "      <td>devices</td>\n",
       "      <td>forcing</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547917</th>\n",
       "      <td>group</td>\n",
       "      <td>smartboards</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547918</th>\n",
       "      <td>agreed</td>\n",
       "      <td>compress</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547919</th>\n",
       "      <td>display</td>\n",
       "      <td>handy</td>\n",
       "      <td>0.002876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1547920 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_1       word_2  cosine_similarity\n",
       "0         marketing        share           0.000090\n",
       "1        complexity       simple           0.000142\n",
       "2            chance     probably           0.000292\n",
       "3           covered        sheet           0.005748\n",
       "4          shifting       topics           0.000115\n",
       "...             ...          ...                ...\n",
       "1547915      pilots    producing           0.000000\n",
       "1547916     devices      forcing           0.000000\n",
       "1547917       group  smartboards           0.001058\n",
       "1547918      agreed     compress           0.001191\n",
       "1547919     display        handy           0.002876\n",
       "\n",
       "[1547920 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_embedding_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_distance = pd.merge(wordpairs_ser,wordpairs_embedding_similarity,on = [\"word_1\",\"word_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_1_phonetic_ser</th>\n",
       "      <th>word_2_phonetic_ser</th>\n",
       "      <th>word_1_orthographic_ser</th>\n",
       "      <th>word_2_orthographic_ser</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>share</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complexity</td>\n",
       "      <td>simple</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chance</td>\n",
       "      <td>probably</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covered</td>\n",
       "      <td>sheet</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shifting</td>\n",
       "      <td>topics</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547915</th>\n",
       "      <td>pilots</td>\n",
       "      <td>producing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547916</th>\n",
       "      <td>devices</td>\n",
       "      <td>forcing</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547917</th>\n",
       "      <td>group</td>\n",
       "      <td>smartboards</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547918</th>\n",
       "      <td>agreed</td>\n",
       "      <td>compress</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547919</th>\n",
       "      <td>display</td>\n",
       "      <td>handy</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.002876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1547920 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_1       word_2  word_1_phonetic_ser  word_2_phonetic_ser  \\\n",
       "0         marketing        share             0.777778             1.400000   \n",
       "1        complexity       simple             0.800000             1.333333   \n",
       "2            chance     probably             1.333333             1.000000   \n",
       "3           covered        sheet             0.714286             1.000000   \n",
       "4          shifting       topics             0.750000             1.000000   \n",
       "...             ...          ...                  ...                  ...   \n",
       "1547915      pilots    producing             1.000000             0.666667   \n",
       "1547916     devices      forcing             0.857143             0.857143   \n",
       "1547917       group  smartboards             1.800000             0.818182   \n",
       "1547918      agreed     compress             1.000000             0.750000   \n",
       "1547919     display        handy             0.857143             1.200000   \n",
       "\n",
       "         word_1_orthographic_ser  word_2_orthographic_ser  cosine_similarity  \n",
       "0                       0.777778                 1.400000           0.000090  \n",
       "1                       0.600000                 1.000000           0.000142  \n",
       "2                       1.166667                 0.875000           0.000292  \n",
       "3                       0.714286                 1.000000           0.005748  \n",
       "4                       0.875000                 1.166667           0.000115  \n",
       "...                          ...                      ...                ...  \n",
       "1547915                 1.333333                 0.888889           0.000000  \n",
       "1547916                 1.000000                 1.000000           0.000000  \n",
       "1547917                 1.800000                 0.818182           0.001058  \n",
       "1547918                 1.000000                 0.750000           0.001191  \n",
       "1547919                 0.857143                 1.200000           0.002876  \n",
       "\n",
       "[1547920 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2222222222222214,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 1.1111111111111107,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.9090909090909083,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 2.8571428571428568,\n",
       " 2.8571428571428568,\n",
       " 1.666666666666666,\n",
       " 2.7272727272727266,\n",
       " 2.8571428571428568,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 4.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 4.444444444444445,\n",
       " 1.1111111111111107,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.3076923076923066,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 1.666666666666666,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 1.5384615384615383,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.2857142857142865,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.4285714285714288,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 0.9090909090909083,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 4.2857142857142865,\n",
       " 3.333333333333334,\n",
       " 1.4285714285714288,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.25,\n",
       " 1.4285714285714288,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.3333333333333321,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 1.1111111111111107,\n",
       " 3.75,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 1.666666666666666,\n",
       " 1.4285714285714288,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 3.333333333333334,\n",
       " 1.25,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.8333333333333339,\n",
       " 1.1111111111111107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 1.4285714285714288,\n",
       " 4.2857142857142865,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.8181818181818166,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 4.545454545454546,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.7272727272727266,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 1.8181818181818166,\n",
       " 1.666666666666666,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 4.444444444444445,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 3.6363636363636367,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.444444444444445,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.8181818181818166,\n",
       " 1.4285714285714288,\n",
       " 1.0,\n",
       " 1.25,\n",
       " 3.333333333333334,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 2.3076923076923066,\n",
       " 0.9090909090909083,\n",
       " 1.666666666666666,\n",
       " 2.5,\n",
       " 1.4285714285714288,\n",
       " 1.25,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 5.555555555555555,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 1.666666666666666,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 2.2222222222222214,\n",
       " 2.2222222222222214,\n",
       " 2.7272727272727266,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 2.1428571428571432,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.444444444444445,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 3.0769230769230775,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 2.8571428571428568,\n",
       " 1.8181818181818166,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 2.2222222222222214,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.25,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 2.5,\n",
       " 1.666666666666666,\n",
       " 2.2222222222222214,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.1111111111111107,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 4.2857142857142865,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 3.333333333333334,\n",
       " 1.666666666666666,\n",
       " 3.333333333333334,\n",
       " 2.5,\n",
       " 1.8181818181818166,\n",
       " 0.0,\n",
       " 6.25,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 2.5,\n",
       " 3.333333333333334,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 1.3333333333333321,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.666666666666666,\n",
       " 2.0,\n",
       " 2.2222222222222214,\n",
       " 1.25,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.5,\n",
       " 1.25,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 2.8571428571428568,\n",
       " 1.0,\n",
       " 2.5,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 4.2857142857142865,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 1.8181818181818166,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 1.4285714285714288,\n",
       " 3.0769230769230775,\n",
       " 3.333333333333334,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.6363636363636367,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 1.25,\n",
       " 2.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 3.6363636363636367,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 2.2222222222222214,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 4.2857142857142865,\n",
       " 1.4285714285714288,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 3.0769230769230775,\n",
       " 1.666666666666666,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.2222222222222214,\n",
       " 2.0,\n",
       " 3.333333333333334,\n",
       " 1.25,\n",
       " 1.4285714285714288,\n",
       " 1.666666666666666,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 1.8181818181818166,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.4285714285714288,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 2.8571428571428568,\n",
       " 3.333333333333334,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.0,\n",
       " 2.8571428571428568,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 3.6363636363636367,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 4.2857142857142865,\n",
       " 1.8181818181818166,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 4.444444444444445,\n",
       " 0.0,\n",
       " 0.9090909090909083,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 1.1111111111111107,\n",
       " 2.3076923076923066,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 2.5,\n",
       " 4.444444444444445,\n",
       " 5.714285714285714,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 5.714285714285714,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 2.0,\n",
       " 3.333333333333334,\n",
       " 5.0,\n",
       " 1.666666666666666,\n",
       " 0.9090909090909083,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 3.333333333333334,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 1.1111111111111107,\n",
       " 2.7272727272727266,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 2.5,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 2.2222222222222214,\n",
       " 1.0,\n",
       " 1.666666666666666,\n",
       " 2.8571428571428568,\n",
       " 4.444444444444445,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.4285714285714288,\n",
       " 1.25,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.8181818181818166,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.75,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 1.4285714285714288,\n",
       " 2.8571428571428568,\n",
       " 2.0,\n",
       " 4.2857142857142865,\n",
       " 1.1111111111111107,\n",
       " 2.5,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 1.1111111111111107,\n",
       " 1.25,\n",
       " 1.666666666666666,\n",
       " 0.9090909090909083,\n",
       " 2.0,\n",
       " 4.545454545454546,\n",
       " 1.25,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 3.0769230769230775,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 2.2222222222222214,\n",
       " 1.666666666666666,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.2857142857142865,\n",
       " 1.1111111111111107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 3.75,\n",
       " 4.545454545454546,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.0,\n",
       " 1.4285714285714288,\n",
       " 4.444444444444445,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 1.4285714285714288,\n",
       " 2.5,\n",
       " 6.363636363636363,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 2.8571428571428568,\n",
       " 3.75,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.666666666666666,\n",
       " 4.0,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 3.75,\n",
       " 2.0,\n",
       " 1.8181818181818166,\n",
       " 0.0,\n",
       " 1.8181818181818166,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 2.5,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 3.0769230769230775,\n",
       " 2.5,\n",
       " 3.333333333333334,\n",
       " 3.333333333333334,\n",
       " 3.75,\n",
       " 2.2222222222222214,\n",
       " 1.666666666666666,\n",
       " 3.333333333333334,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 4.2857142857142865,\n",
       " 4.444444444444445,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 2.2222222222222214,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 3.75,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.333333333333334,\n",
       " 2.0,\n",
       " 2.8571428571428568,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.333333333333334,\n",
       " 3.75,\n",
       " 1.4285714285714288,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 1.25,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.8181818181818166,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 3.333333333333334,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.9090909090909083,\n",
       " 3.333333333333334,\n",
       " 2.0,\n",
       " 1.3333333333333321,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.545454545454546,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.2222222222222214,\n",
       " 2.8571428571428568,\n",
       " 1.4285714285714288,\n",
       " 6.666666666666667,\n",
       " 2.5,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.8571428571428568,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.1111111111111107,\n",
       " 1.666666666666666,\n",
       " 3.333333333333334,\n",
       " 1.4285714285714288,\n",
       " 4.2857142857142865,\n",
       " 1.4285714285714288,\n",
       " 1.666666666666666,\n",
       " 1.666666666666666,\n",
       " 2.0,\n",
       " 2.2222222222222214,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 2.0,\n",
       " 3.333333333333334,\n",
       " 2.2222222222222214,\n",
       " 1.1111111111111107,\n",
       " 1.25,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 2.2222222222222214,\n",
       " 2.8571428571428568,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.5384615384615383,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 4.444444444444445,\n",
       " 1.0,\n",
       " 0.9090909090909083,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.1111111111111107,\n",
       " 3.333333333333334,\n",
       " 3.333333333333334,\n",
       " 1.1111111111111107,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 2.7272727272727266,\n",
       " 6.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 2.8571428571428568,\n",
       " 0.9090909090909083,\n",
       " 3.333333333333334,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 1.8181818181818166,\n",
       " 3.333333333333334,\n",
       " 1.4285714285714288,\n",
       " 4.0,\n",
       " 2.8571428571428568,\n",
       " 1.25,\n",
       " 2.2222222222222214,\n",
       " 2.5,\n",
       " 3.0769230769230775,\n",
       " 5.0,\n",
       " 1.666666666666666,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 2.8571428571428568,\n",
       " 2.8571428571428568,\n",
       " 3.333333333333334,\n",
       " 3.333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.666666666666666,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.4285714285714288,\n",
       " 1.4285714285714288,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.5,\n",
       " 1.666666666666666,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 1.666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.7272727272727266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.8181818181818166,\n",
       " 2.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_distance[\"word_1_phonetic_ser\"].apply(lambda x: sim_score(x*100)).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.15835975897834867, pvalue=0.0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(wordpairs_distance[\"word_1_phonetic_ser\"].to_list(),wordpairs_distance[\"cosine_similarity\"].apply(lambda x: 1-x).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
